{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from quote_prediction.data import ExplicitQuoteCorpus, Novel\n",
    "import numpy as np \n",
    "import tqdm \n",
    "import torch\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.cm as mplcm\n",
    "import matplotlib.colors as colors\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize\n",
    "import re \n",
    "import spacy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util \n",
    "import torch.nn.functional as F \n",
    "from transformers import AutoTokenizer\n",
    "from transformers import RobertaForCausalLM, AutoConfig, AutoModel, RobertaModel, BertModel, AutoModelForSequenceClassification\n",
    "from transformers.trainer import Trainer\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-08 12:06:26,981 - Starting experiment ALL with query size >= 5\n",
      "2024-05-08 12:06:26,981 - Loading data...\n",
      "2024-05-08 12:06:32,951 - Loaded quotes of 28 novels\n",
      "2024-05-08 12:06:32,951 - Starting sampling .....\n",
      "NOVEL: AHandfulOfDust, ID 0 - # of queries: 25 - avg # of targets: 16.0 - avg # of quote targets: 1388.0 - avg query length: 61.8 - # of speakers in novel 17 - Percent Active Speakers 0.588\n",
      "NOVEL: APassageToIndia, ID 1 - # of queries: 88 - avg # of targets: 18.0 - avg # of quote targets: 1904.2 - avg query length: 21.5 - # of speakers in novel 18 - Percent Active Speakers 1.000\n",
      "NOVEL: ARoomWithAView, ID 2 - # of queries: 67 - avg # of targets: 10.0 - avg # of quote targets: 1474.4 - avg query length: 22.5 - # of speakers in novel 10 - Percent Active Speakers 1.000\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 - # of queries: 29 - avg # of targets: 11.0 - avg # of quote targets: 541.5 - avg query length: 19.0 - # of speakers in novel 11 - Percent Active Speakers 0.909\n",
      "NOVEL: AnneOfGreenGables, ID 4 - # of queries: 75 - avg # of targets: 5.0 - avg # of quote targets: 1064.4 - avg query length: 13.1 - # of speakers in novel 5 - Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 - # of queries: 10 - avg # of targets: 5.5 - avg # of quote targets: 263.0 - avg query length: 48.8 - # of speakers in novel 6 - Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 - # of queries: 103 - avg # of targets: 13.0 - avg # of quote targets: 1548.4 - avg query length: 13.2 - # of speakers in novel 13 - Percent Active Speakers 1.000\n",
      "NOVEL: HardTimes, ID 7 - # of queries: 98 - avg # of targets: 14.0 - avg # of quote targets: 1515.7 - avg query length: 14.7 - # of speakers in novel 14 - Percent Active Speakers 0.929\n",
      "NOVEL: HowardsEnd, ID 8 - # of queries: 103 - avg # of targets: 11.0 - avg # of quote targets: 2391.0 - avg query length: 23.0 - # of speakers in novel 11 - Percent Active Speakers 1.000\n",
      "NOVEL: MansfieldPark, ID 9 - # of queries: 70 - avg # of targets: 12.0 - avg # of quote targets: 1047.3 - avg query length: 11.1 - # of speakers in novel 12 - Percent Active Speakers 1.000\n",
      "NOVEL: NightAndDay, ID 10 - # of queries: 93 - avg # of targets: 12.9 - avg # of quote targets: 1938.9 - avg query length: 20.4 - # of speakers in novel 13 - Percent Active Speakers 0.846\n",
      "NOVEL: NorthangerAbbey, ID 11 - # of queries: 51 - avg # of targets: 6.0 - avg # of quote targets: 728.6 - avg query length: 13.4 - # of speakers in novel 6 - Percent Active Speakers 1.000\n",
      "NOVEL: OliverTwist, ID 12 - # of queries: 127 - avg # of targets: 28.0 - avg # of quote targets: 2280.5 - avg query length: 15.9 - # of speakers in novel 28 - Percent Active Speakers 0.893\n",
      "NOVEL: Persuasion, ID 13 - # of queries: 25 - avg # of targets: 9.0 - avg # of quote targets: 372.0 - avg query length: 11.8 - # of speakers in novel 9 - Percent Active Speakers 1.000\n",
      "NOVEL: PrideAndPrejudice, ID 14 - # of queries: 84 - avg # of targets: 14.0 - avg # of quote targets: 1171.9 - avg query length: 11.0 - # of speakers in novel 14 - Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 - # of queries: 63 - avg # of targets: 12.0 - avg # of quote targets: 983.3 - avg query length: 12.3 - # of speakers in novel 12 - Percent Active Speakers 1.000\n",
      "NOVEL: TheAgeOfInnocence, ID 16 - # of queries: 65 - avg # of targets: 12.0 - avg # of quote targets: 1225.6 - avg query length: 17.6 - # of speakers in novel 12 - Percent Active Speakers 0.917\n",
      "NOVEL: TheAwakening, ID 17 - # of queries: 40 - avg # of targets: 7.0 - avg # of quote targets: 520.4 - avg query length: 11.7 - # of speakers in novel 7 - Percent Active Speakers 1.000\n",
      "NOVEL: TheGambler, ID 18 - # of queries: 34 - avg # of targets: 7.0 - avg # of quote targets: 681.2 - avg query length: 19.1 - # of speakers in novel 7 - Percent Active Speakers 1.000\n",
      "NOVEL: TheInvisibleMan, ID 19 - # of queries: 41 - avg # of targets: 10.0 - avg # of quote targets: 789.2 - avg query length: 17.9 - # of speakers in novel 10 - Percent Active Speakers 0.900\n",
      "NOVEL: TheManWhoWasThursday, ID 20 - # of queries: 35 - avg # of targets: 9.0 - avg # of quote targets: 669.5 - avg query length: 18.8 - # of speakers in novel 9 - Percent Active Speakers 0.889\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 - # of queries: 54 - avg # of targets: 12.8 - avg # of quote targets: 1590.2 - avg query length: 29.7 - # of speakers in novel 13 - Percent Active Speakers 0.846\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 - # of queries: 37 - avg # of targets: 6.9 - avg # of quote targets: 953.1 - avg query length: 26.4 - # of speakers in novel 7 - Percent Active Speakers 0.857\n",
      "NOVEL: TheSignOfTheFour, ID 23 - # of queries: 28 - avg # of targets: 6.0 - avg # of quote targets: 539.6 - avg query length: 19.8 - # of speakers in novel 6 - Percent Active Speakers 1.000\n",
      "NOVEL: TheSportOfTheGods, ID 24 - # of queries: 32 - avg # of targets: 11.0 - avg # of quote targets: 478.9 - avg query length: 14.2 - # of speakers in novel 11 - Percent Active Speakers 1.000\n",
      "NOVEL: TheSunAlsoRises, ID 25 - # of queries: 61 - avg # of targets: 12.8 - avg # of quote targets: 2311.4 - avg query length: 38.3 - # of speakers in novel 13 - Percent Active Speakers 0.692\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 - # of queries: 30 - avg # of targets: 6.9 - avg # of quote targets: 859.5 - avg query length: 30.9 - # of speakers in novel 7 - Percent Active Speakers 0.857\n",
      "NOVEL: WinnieThePooh, ID 27 - # of queries: 38 - avg # of targets: 9.0 - avg # of quote targets: 762.9 - avg query length: 21.7 - # of speakers in novel 9 - Percent Active Speakers 1.000\n",
      "# Novels 28 - Speaker Activity 0.93 +/- (0.10) - Total # queries 1606 - Avg # queries 57.36 +/- (29.34) - Avg query length 21.42 +/- (11.52) - Avg # targets/query 10.99 +/- (4.60) - Avg # quote targets/query 1142.66 +/- (599.69) - Avg # speaker in novel 11.07 +/- (4.63)\n",
      "NOVEL: AHandfulOfDust, ID 0 - # of queries: 14 - avg # of targets: 16.1 - avg # of quote targets: 1126.6 - avg query length: 13.3 - # of speakers in novel 17 - Percent Active Speakers 0.471\n",
      "NOVEL: APassageToIndia, ID 1 - # of queries: 14 - avg # of targets: 18.0 - avg # of quote targets: 1633.1 - avg query length: 6.7 - # of speakers in novel 18 - Percent Active Speakers 0.500\n",
      "NOVEL: ARoomWithAView, ID 2 - # of queries: 22 - avg # of targets: 10.0 - avg # of quote targets: 1137.4 - avg query length: 9.3 - # of speakers in novel 10 - Percent Active Speakers 0.900\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 - # of queries: 26 - avg # of targets: 9.8 - avg # of quote targets: 105.2 - avg query length: 16.0 - # of speakers in novel 11 - Percent Active Speakers 0.727\n",
      "NOVEL: AnneOfGreenGables, ID 4 - # of queries: 42 - avg # of targets: 5.0 - avg # of quote targets: 502.8 - avg query length: 9.1 - # of speakers in novel 5 - Percent Active Speakers 0.800\n",
      "NOVEL: DaisyMiller, ID 5 - # of queries: 10 - avg # of targets: 5.5 - avg # of quote targets: 149.0 - avg query length: 21.2 - # of speakers in novel 6 - Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 - # of queries: 11 - avg # of targets: 13.0 - avg # of quote targets: 1175.1 - avg query length: 7.3 - # of speakers in novel 13 - Percent Active Speakers 0.154\n",
      "NOVEL: HardTimes, ID 7 - # of queries: 51 - avg # of targets: 13.9 - avg # of quote targets: 805.8 - avg query length: 11.0 - # of speakers in novel 14 - Percent Active Speakers 0.857\n",
      "NOVEL: HowardsEnd, ID 8 - # of queries: 37 - avg # of targets: 11.0 - avg # of quote targets: 1870.5 - avg query length: 8.4 - # of speakers in novel 11 - Percent Active Speakers 0.545\n",
      "NOVEL: MansfieldPark, ID 9 - # of queries: 8 - avg # of targets: 12.0 - avg # of quote targets: 694.0 - avg query length: 6.5 - # of speakers in novel 12 - Percent Active Speakers 0.500\n",
      "NOVEL: NightAndDay, ID 10 - # of queries: 46 - avg # of targets: 12.9 - avg # of quote targets: 1282.5 - avg query length: 10.4 - # of speakers in novel 13 - Percent Active Speakers 0.769\n",
      "NOVEL: NorthangerAbbey, ID 11 - # of queries: 2 - avg # of targets: 6.0 - avg # of quote targets: 589.5 - avg query length: 6.0 - # of speakers in novel 6 - Percent Active Speakers 0.167\n",
      "NOVEL: OliverTwist, ID 12 - # of queries: 80 - avg # of targets: 28.0 - avg # of quote targets: 969.4 - avg query length: 12.9 - # of speakers in novel 28 - Percent Active Speakers 0.786\n",
      "NOVEL: Persuasion, ID 13 - # of queries: 6 - avg # of targets: 9.0 - avg # of quote targets: 207.8 - avg query length: 10.0 - # of speakers in novel 9 - Percent Active Speakers 0.556\n",
      "NOVEL: PrideAndPrejudice, ID 14 - # of queries: 5 - avg # of targets: 14.0 - avg # of quote targets: 869.2 - avg query length: 6.6 - # of speakers in novel 14 - Percent Active Speakers 0.214\n",
      "NOVEL: SenseAndSensibility, ID 15 - # of queries: 15 - avg # of targets: 12.0 - avg # of quote targets: 641.9 - avg query length: 8.1 - # of speakers in novel 12 - Percent Active Speakers 0.417\n",
      "NOVEL: TheAgeOfInnocence, ID 16 - # of queries: 9 - avg # of targets: 12.0 - avg # of quote targets: 1046.3 - avg query length: 7.8 - # of speakers in novel 12 - Percent Active Speakers 0.417\n",
      "NOVEL: TheAwakening, ID 17 - # of queries: 5 - avg # of targets: 7.0 - avg # of quote targets: 405.0 - avg query length: 7.6 - # of speakers in novel 7 - Percent Active Speakers 0.429\n",
      "NOVEL: TheGambler, ID 18 - Found no pairs - # of speakers in novel 7 - Percent Active Speakers 0.000\n",
      "NOVEL: TheInvisibleMan, ID 19 - # of queries: 22 - avg # of targets: 9.9 - avg # of quote targets: 458.5 - avg query length: 11.0 - # of speakers in novel 10 - Percent Active Speakers 0.900\n",
      "NOVEL: TheManWhoWasThursday, ID 20 - # of queries: 29 - avg # of targets: 9.0 - avg # of quote targets: 244.5 - avg query length: 13.8 - # of speakers in novel 9 - Percent Active Speakers 0.889\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 - # of queries: 9 - avg # of targets: 12.9 - avg # of quote targets: 1372.0 - avg query length: 15.0 - # of speakers in novel 13 - Percent Active Speakers 0.231\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 - # of queries: 17 - avg # of targets: 6.9 - avg # of quote targets: 742.1 - avg query length: 8.7 - # of speakers in novel 7 - Percent Active Speakers 0.429\n",
      "NOVEL: TheSignOfTheFour, ID 23 - # of queries: 4 - avg # of targets: 6.0 - avg # of quote targets: 476.8 - avg query length: 8.8 - # of speakers in novel 6 - Percent Active Speakers 0.167\n",
      "NOVEL: TheSportOfTheGods, ID 24 - Found no pairs - # of speakers in novel 11 - Percent Active Speakers 0.000\n",
      "NOVEL: TheSunAlsoRises, ID 25 - # of queries: 33 - avg # of targets: 12.8 - avg # of quote targets: 1790.5 - avg query length: 14.7 - # of speakers in novel 13 - Percent Active Speakers 0.385\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 - # of queries: 12 - avg # of targets: 6.9 - avg # of quote targets: 672.2 - avg query length: 10.8 - # of speakers in novel 7 - Percent Active Speakers 0.714\n",
      "NOVEL: WinnieThePooh, ID 27 - # of queries: 30 - avg # of targets: 9.0 - avg # of quote targets: 392.0 - avg query length: 12.8 - # of speakers in novel 9 - Percent Active Speakers 0.889\n",
      "# Novels 28 - Speaker Activity 0.52 +/- (0.28) - Total # queries 559 - Avg # queries 21.50 +/- (17.88) - Avg query length 10.53 +/- (3.53) - Avg # targets/query 11.10 +/- (4.72) - Avg # quote targets/query 821.52 +/- (485.26) - Avg # speaker in novel 11.07 +/- (4.63)\n",
      "NOVEL: AHandfulOfDust, ID 0 - # of queries: 14 - avg # of targets: 16.1 - avg # of quote targets: 1341.8 - avg query length: 13.3 - # of speakers in novel 17 - Percent Active Speakers 0.471\n",
      "NOVEL: APassageToIndia, ID 1 - # of queries: 14 - avg # of targets: 18.0 - avg # of quote targets: 1877.7 - avg query length: 6.7 - # of speakers in novel 18 - Percent Active Speakers 0.500\n",
      "NOVEL: ARoomWithAView, ID 2 - # of queries: 22 - avg # of targets: 10.0 - avg # of quote targets: 1469.5 - avg query length: 9.3 - # of speakers in novel 10 - Percent Active Speakers 0.900\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 - # of queries: 28 - avg # of targets: 11.0 - avg # of quote targets: 540.6 - avg query length: 15.8 - # of speakers in novel 11 - Percent Active Speakers 0.818\n",
      "NOVEL: AnneOfGreenGables, ID 4 - # of queries: 42 - avg # of targets: 5.0 - avg # of quote targets: 1060.9 - avg query length: 9.1 - # of speakers in novel 5 - Percent Active Speakers 0.800\n",
      "NOVEL: DaisyMiller, ID 5 - # of queries: 10 - avg # of targets: 5.5 - avg # of quote targets: 263.0 - avg query length: 21.2 - # of speakers in novel 6 - Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 - # of queries: 11 - avg # of targets: 13.0 - avg # of quote targets: 1549.6 - avg query length: 7.3 - # of speakers in novel 13 - Percent Active Speakers 0.154\n",
      "NOVEL: HardTimes, ID 7 - # of queries: 51 - avg # of targets: 13.9 - avg # of quote targets: 1512.5 - avg query length: 11.0 - # of speakers in novel 14 - Percent Active Speakers 0.857\n",
      "NOVEL: HowardsEnd, ID 8 - # of queries: 37 - avg # of targets: 11.0 - avg # of quote targets: 2375.0 - avg query length: 8.4 - # of speakers in novel 11 - Percent Active Speakers 0.545\n",
      "NOVEL: MansfieldPark, ID 9 - # of queries: 8 - avg # of targets: 12.0 - avg # of quote targets: 1039.2 - avg query length: 6.5 - # of speakers in novel 12 - Percent Active Speakers 0.500\n",
      "NOVEL: NightAndDay, ID 10 - # of queries: 46 - avg # of targets: 12.9 - avg # of quote targets: 1930.9 - avg query length: 10.4 - # of speakers in novel 13 - Percent Active Speakers 0.769\n",
      "NOVEL: NorthangerAbbey, ID 11 - # of queries: 2 - avg # of targets: 6.0 - avg # of quote targets: 725.5 - avg query length: 6.0 - # of speakers in novel 6 - Percent Active Speakers 0.167\n",
      "NOVEL: OliverTwist, ID 12 - # of queries: 80 - avg # of targets: 28.0 - avg # of quote targets: 2279.2 - avg query length: 12.9 - # of speakers in novel 28 - Percent Active Speakers 0.786\n",
      "NOVEL: Persuasion, ID 13 - # of queries: 6 - avg # of targets: 9.0 - avg # of quote targets: 356.5 - avg query length: 10.0 - # of speakers in novel 9 - Percent Active Speakers 0.556\n",
      "NOVEL: PrideAndPrejudice, ID 14 - # of queries: 5 - avg # of targets: 14.0 - avg # of quote targets: 1158.0 - avg query length: 6.6 - # of speakers in novel 14 - Percent Active Speakers 0.214\n",
      "NOVEL: SenseAndSensibility, ID 15 - # of queries: 15 - avg # of targets: 12.0 - avg # of quote targets: 974.6 - avg query length: 8.1 - # of speakers in novel 12 - Percent Active Speakers 0.417\n",
      "NOVEL: TheAgeOfInnocence, ID 16 - # of queries: 9 - avg # of targets: 12.0 - avg # of quote targets: 1225.8 - avg query length: 7.8 - # of speakers in novel 12 - Percent Active Speakers 0.417\n",
      "NOVEL: TheAwakening, ID 17 - # of queries: 5 - avg # of targets: 7.0 - avg # of quote targets: 514.8 - avg query length: 7.6 - # of speakers in novel 7 - Percent Active Speakers 0.429\n",
      "NOVEL: TheGambler, ID 18 - Found no pairs - # of speakers in novel 7 - Percent Active Speakers 0.000\n",
      "NOVEL: TheInvisibleMan, ID 19 - # of queries: 23 - avg # of targets: 10.0 - avg # of quote targets: 783.5 - avg query length: 10.8 - # of speakers in novel 10 - Percent Active Speakers 0.900\n",
      "NOVEL: TheManWhoWasThursday, ID 20 - # of queries: 29 - avg # of targets: 9.0 - avg # of quote targets: 667.3 - avg query length: 13.8 - # of speakers in novel 9 - Percent Active Speakers 0.889\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 - # of queries: 9 - avg # of targets: 12.9 - avg # of quote targets: 1564.7 - avg query length: 15.0 - # of speakers in novel 13 - Percent Active Speakers 0.231\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 - # of queries: 17 - avg # of targets: 6.9 - avg # of quote targets: 940.7 - avg query length: 8.7 - # of speakers in novel 7 - Percent Active Speakers 0.429\n",
      "NOVEL: TheSignOfTheFour, ID 23 - # of queries: 4 - avg # of targets: 6.0 - avg # of quote targets: 544.0 - avg query length: 8.8 - # of speakers in novel 6 - Percent Active Speakers 0.167\n",
      "NOVEL: TheSportOfTheGods, ID 24 - Found no pairs - # of speakers in novel 11 - Percent Active Speakers 0.000\n",
      "NOVEL: TheSunAlsoRises, ID 25 - # of queries: 33 - avg # of targets: 12.8 - avg # of quote targets: 2298.8 - avg query length: 14.7 - # of speakers in novel 13 - Percent Active Speakers 0.385\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 - # of queries: 12 - avg # of targets: 6.9 - avg # of quote targets: 830.8 - avg query length: 10.8 - # of speakers in novel 7 - Percent Active Speakers 0.714\n",
      "NOVEL: WinnieThePooh, ID 27 - # of queries: 30 - avg # of targets: 9.0 - avg # of quote targets: 760.1 - avg query length: 12.8 - # of speakers in novel 9 - Percent Active Speakers 0.889\n",
      "# Novels 28 - Speaker Activity 0.53 +/- (0.28) - Total # queries 562 - Avg # queries 21.62 +/- (17.90) - Avg query length 10.51 +/- (3.52) - Avg # targets/query 11.15 +/- (4.71) - Avg # quote targets/query 1176.34 +/- (597.41) - Avg # speaker in novel 11.07 +/- (4.63)\n",
      "********** PROCESSING # UTTERANCES 1**********\n",
      "NOVEL: AHandfulOfDust, ID 0 Percent Active Speakers 0.235\n",
      "NOVEL: APassageToIndia, ID 1 Percent Active Speakers 0.889\n",
      "NOVEL: ARoomWithAView, ID 2 Percent Active Speakers 0.800\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 Percent Active Speakers 0.364\n",
      "NOVEL: AnneOfGreenGables, ID 4 Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 Percent Active Speakers 0.846\n",
      "NOVEL: HardTimes, ID 7 Percent Active Speakers 0.857\n",
      "NOVEL: HowardsEnd, ID 8 Percent Active Speakers 0.818\n",
      "NOVEL: MansfieldPark, ID 9 Percent Active Speakers 0.667\n",
      "NOVEL: NightAndDay, ID 10 Percent Active Speakers 0.769\n",
      "NOVEL: NorthangerAbbey, ID 11 Percent Active Speakers 0.833\n",
      "NOVEL: OliverTwist, ID 12 Percent Active Speakers 0.607\n",
      "NOVEL: Persuasion, ID 13 Percent Active Speakers 0.889\n",
      "NOVEL: PrideAndPrejudice, ID 14 Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 Percent Active Speakers 0.917\n",
      "NOVEL: TheAgeOfInnocence, ID 16 Percent Active Speakers 0.833\n",
      "NOVEL: TheAwakening, ID 17 Percent Active Speakers 0.714\n",
      "NOVEL: TheGambler, ID 18 Percent Active Speakers 1.000\n",
      "NOVEL: TheInvisibleMan, ID 19 Percent Active Speakers 0.200\n",
      "NOVEL: TheManWhoWasThursday, ID 20 Percent Active Speakers 0.778\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 Percent Active Speakers 0.769\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 Percent Active Speakers 0.571\n",
      "NOVEL: TheSignOfTheFour, ID 23 Percent Active Speakers 0.667\n",
      "NOVEL: TheSportOfTheGods, ID 24 Percent Active Speakers 0.818\n",
      "NOVEL: TheSunAlsoRises, ID 25 Percent Active Speakers 0.462\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 Percent Active Speakers 0.571\n",
      "NOVEL: WinnieThePooh, ID 27 Percent Active Speakers 0.889\n",
      "# Novels 28, Speaker Activity 0.736\n",
      "********** PROCESSING # UTTERANCES 5**********\n",
      "NOVEL: AHandfulOfDust, ID 0 Percent Active Speakers 0.235\n",
      "NOVEL: APassageToIndia, ID 1 Percent Active Speakers 0.889\n",
      "NOVEL: ARoomWithAView, ID 2 Percent Active Speakers 0.800\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 Percent Active Speakers 0.364\n",
      "NOVEL: AnneOfGreenGables, ID 4 Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 Percent Active Speakers 0.846\n",
      "NOVEL: HardTimes, ID 7 Percent Active Speakers 0.857\n",
      "NOVEL: HowardsEnd, ID 8 Percent Active Speakers 0.818\n",
      "NOVEL: MansfieldPark, ID 9 Percent Active Speakers 0.667\n",
      "NOVEL: NightAndDay, ID 10 Percent Active Speakers 0.769\n",
      "NOVEL: NorthangerAbbey, ID 11 Percent Active Speakers 0.833\n",
      "NOVEL: OliverTwist, ID 12 Percent Active Speakers 0.607\n",
      "NOVEL: Persuasion, ID 13 Percent Active Speakers 0.889\n",
      "NOVEL: PrideAndPrejudice, ID 14 Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 Percent Active Speakers 0.917\n",
      "NOVEL: TheAgeOfInnocence, ID 16 Percent Active Speakers 0.833\n",
      "NOVEL: TheAwakening, ID 17 Percent Active Speakers 0.714\n",
      "NOVEL: TheGambler, ID 18 Percent Active Speakers 1.000\n",
      "NOVEL: TheInvisibleMan, ID 19 Percent Active Speakers 0.200\n",
      "NOVEL: TheManWhoWasThursday, ID 20 Percent Active Speakers 0.778\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 Percent Active Speakers 0.769\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 Percent Active Speakers 0.571\n",
      "NOVEL: TheSignOfTheFour, ID 23 Percent Active Speakers 0.667\n",
      "NOVEL: TheSportOfTheGods, ID 24 Percent Active Speakers 0.818\n",
      "NOVEL: TheSunAlsoRises, ID 25 Percent Active Speakers 0.462\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 Percent Active Speakers 0.571\n",
      "NOVEL: WinnieThePooh, ID 27 Percent Active Speakers 0.889\n",
      "# Novels 28, Speaker Activity 0.736\n",
      "********** PROCESSING # UTTERANCES 10**********\n",
      "NOVEL: AHandfulOfDust, ID 0 Percent Active Speakers 0.235\n",
      "NOVEL: APassageToIndia, ID 1 Percent Active Speakers 0.889\n",
      "NOVEL: ARoomWithAView, ID 2 Percent Active Speakers 0.800\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 Percent Active Speakers 0.364\n",
      "NOVEL: AnneOfGreenGables, ID 4 Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 Percent Active Speakers 0.846\n",
      "NOVEL: HardTimes, ID 7 Percent Active Speakers 0.857\n",
      "NOVEL: HowardsEnd, ID 8 Percent Active Speakers 0.818\n",
      "NOVEL: MansfieldPark, ID 9 Percent Active Speakers 0.667\n",
      "NOVEL: NightAndDay, ID 10 Percent Active Speakers 0.769\n",
      "NOVEL: NorthangerAbbey, ID 11 Percent Active Speakers 0.833\n",
      "NOVEL: OliverTwist, ID 12 Percent Active Speakers 0.607\n",
      "NOVEL: Persuasion, ID 13 Percent Active Speakers 0.889\n",
      "NOVEL: PrideAndPrejudice, ID 14 Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 Percent Active Speakers 0.917\n",
      "NOVEL: TheAgeOfInnocence, ID 16 Percent Active Speakers 0.833\n",
      "NOVEL: TheAwakening, ID 17 Percent Active Speakers 0.714\n",
      "NOVEL: TheGambler, ID 18 Percent Active Speakers 1.000\n",
      "NOVEL: TheInvisibleMan, ID 19 Percent Active Speakers 0.200\n",
      "NOVEL: TheManWhoWasThursday, ID 20 Percent Active Speakers 0.778\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 Percent Active Speakers 0.769\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 Percent Active Speakers 0.571\n",
      "NOVEL: TheSignOfTheFour, ID 23 Percent Active Speakers 0.667\n",
      "NOVEL: TheSportOfTheGods, ID 24 Percent Active Speakers 0.818\n",
      "NOVEL: TheSunAlsoRises, ID 25 Percent Active Speakers 0.462\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 Percent Active Speakers 0.571\n",
      "NOVEL: WinnieThePooh, ID 27 Percent Active Speakers 0.889\n",
      "# Novels 28, Speaker Activity 0.736\n",
      "********** PROCESSING # UTTERANCES 20**********\n",
      "NOVEL: AHandfulOfDust, ID 0 Percent Active Speakers 0.235\n",
      "NOVEL: APassageToIndia, ID 1 Percent Active Speakers 0.889\n",
      "NOVEL: ARoomWithAView, ID 2 Percent Active Speakers 0.800\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 Percent Active Speakers 0.364\n",
      "NOVEL: AnneOfGreenGables, ID 4 Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 Percent Active Speakers 0.846\n",
      "NOVEL: HardTimes, ID 7 Percent Active Speakers 0.857\n",
      "NOVEL: HowardsEnd, ID 8 Percent Active Speakers 0.818\n",
      "NOVEL: MansfieldPark, ID 9 Percent Active Speakers 0.667\n",
      "NOVEL: NightAndDay, ID 10 Percent Active Speakers 0.769\n",
      "NOVEL: NorthangerAbbey, ID 11 Percent Active Speakers 0.833\n",
      "NOVEL: OliverTwist, ID 12 Percent Active Speakers 0.607\n",
      "NOVEL: Persuasion, ID 13 Percent Active Speakers 0.889\n",
      "NOVEL: PrideAndPrejudice, ID 14 Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 Percent Active Speakers 0.917\n",
      "NOVEL: TheAgeOfInnocence, ID 16 Percent Active Speakers 0.833\n",
      "NOVEL: TheAwakening, ID 17 Percent Active Speakers 0.714\n",
      "NOVEL: TheGambler, ID 18 Percent Active Speakers 1.000\n",
      "NOVEL: TheInvisibleMan, ID 19 Percent Active Speakers 0.200\n",
      "NOVEL: TheManWhoWasThursday, ID 20 Percent Active Speakers 0.778\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 Percent Active Speakers 0.769\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 Percent Active Speakers 0.571\n",
      "NOVEL: TheSignOfTheFour, ID 23 Percent Active Speakers 0.667\n",
      "NOVEL: TheSportOfTheGods, ID 24 Percent Active Speakers 0.818\n",
      "NOVEL: TheSunAlsoRises, ID 25 Percent Active Speakers 0.462\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 Percent Active Speakers 0.571\n",
      "NOVEL: WinnieThePooh, ID 27 Percent Active Speakers 0.889\n",
      "# Novels 28, Speaker Activity 0.736\n",
      "********** PROCESSING # UTTERANCES 50**********\n",
      "NOVEL: AHandfulOfDust, ID 0 Percent Active Speakers 0.235\n",
      "NOVEL: APassageToIndia, ID 1 Percent Active Speakers 0.889\n",
      "NOVEL: ARoomWithAView, ID 2 Percent Active Speakers 0.800\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 Percent Active Speakers 0.364\n",
      "NOVEL: AnneOfGreenGables, ID 4 Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 Percent Active Speakers 0.846\n",
      "NOVEL: HardTimes, ID 7 Percent Active Speakers 0.857\n",
      "NOVEL: HowardsEnd, ID 8 Percent Active Speakers 0.818\n",
      "NOVEL: MansfieldPark, ID 9 Percent Active Speakers 0.667\n",
      "NOVEL: NightAndDay, ID 10 Percent Active Speakers 0.769\n",
      "NOVEL: NorthangerAbbey, ID 11 Percent Active Speakers 0.833\n",
      "NOVEL: OliverTwist, ID 12 Percent Active Speakers 0.607\n",
      "NOVEL: Persuasion, ID 13 Percent Active Speakers 0.889\n",
      "NOVEL: PrideAndPrejudice, ID 14 Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 Percent Active Speakers 0.917\n",
      "NOVEL: TheAgeOfInnocence, ID 16 Percent Active Speakers 0.833\n",
      "NOVEL: TheAwakening, ID 17 Percent Active Speakers 0.714\n",
      "NOVEL: TheGambler, ID 18 Percent Active Speakers 1.000\n",
      "NOVEL: TheInvisibleMan, ID 19 Percent Active Speakers 0.200\n",
      "NOVEL: TheManWhoWasThursday, ID 20 Percent Active Speakers 0.778\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 Percent Active Speakers 0.769\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 Percent Active Speakers 0.571\n",
      "NOVEL: TheSignOfTheFour, ID 23 Percent Active Speakers 0.667\n",
      "NOVEL: TheSportOfTheGods, ID 24 Percent Active Speakers 0.818\n",
      "NOVEL: TheSunAlsoRises, ID 25 Percent Active Speakers 0.462\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 Percent Active Speakers 0.571\n",
      "NOVEL: WinnieThePooh, ID 27 Percent Active Speakers 0.889\n",
      "# Novels 28, Speaker Activity 0.736\n",
      "********** PROCESSING # UTTERANCES 100**********\n",
      "NOVEL: AHandfulOfDust, ID 0 Percent Active Speakers 0.235\n",
      "NOVEL: APassageToIndia, ID 1 Percent Active Speakers 0.889\n",
      "NOVEL: ARoomWithAView, ID 2 Percent Active Speakers 0.800\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 Percent Active Speakers 0.364\n",
      "NOVEL: AnneOfGreenGables, ID 4 Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 Percent Active Speakers 0.846\n",
      "NOVEL: HardTimes, ID 7 Percent Active Speakers 0.857\n",
      "NOVEL: HowardsEnd, ID 8 Percent Active Speakers 0.818\n",
      "NOVEL: MansfieldPark, ID 9 Percent Active Speakers 0.667\n",
      "NOVEL: NightAndDay, ID 10 Percent Active Speakers 0.769\n",
      "NOVEL: NorthangerAbbey, ID 11 Percent Active Speakers 0.833\n",
      "NOVEL: OliverTwist, ID 12 Percent Active Speakers 0.607\n",
      "NOVEL: Persuasion, ID 13 Percent Active Speakers 0.889\n",
      "NOVEL: PrideAndPrejudice, ID 14 Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 Percent Active Speakers 0.917\n",
      "NOVEL: TheAgeOfInnocence, ID 16 Percent Active Speakers 0.833\n",
      "NOVEL: TheAwakening, ID 17 Percent Active Speakers 0.714\n",
      "NOVEL: TheGambler, ID 18 Percent Active Speakers 1.000\n",
      "NOVEL: TheInvisibleMan, ID 19 Percent Active Speakers 0.200\n",
      "NOVEL: TheManWhoWasThursday, ID 20 Percent Active Speakers 0.778\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 Percent Active Speakers 0.769\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 Percent Active Speakers 0.571\n",
      "NOVEL: TheSignOfTheFour, ID 23 Percent Active Speakers 0.667\n",
      "NOVEL: TheSportOfTheGods, ID 24 Percent Active Speakers 0.818\n",
      "NOVEL: TheSunAlsoRises, ID 25 Percent Active Speakers 0.462\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 Percent Active Speakers 0.571\n",
      "NOVEL: WinnieThePooh, ID 27 Percent Active Speakers 0.889\n",
      "# Novels 28, Speaker Activity 0.736\n",
      "/usr/local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-05-08 12:08:56,969 - \n",
      "2024-05-08 12:08:56,969 - ----------MODEL: DRAMA_LUAR | EXPERIMENT: CHAPTERWISE----------\n",
      "2024-05-08 12:08:56,969 - \n",
      "2024-05-08 12:08:56,969 - \n",
      "2024-05-08 12:08:56,969 - CHARACTER - CHARACTER\n",
      "2024-05-08 12:10:12,315 - \t\tNOVEL 0 ---- MRR 0.678 | Recall@1 0.560 | Recall@3 0.760 | Recall@8 0.880 | AUC 0.866 | Major AUC 0.980 | Intermediate AUC 0.741\n",
      "2024-05-08 12:16:10,661 - \t\tNOVEL 1 ---- MRR 0.571 | Recall@1 0.420 | Recall@3 0.625 | Recall@8 0.864 | AUC 0.832 | Major AUC 0.928 | Intermediate AUC 0.657\n",
      "2024-05-08 12:19:41,935 - \t\tNOVEL 2 ---- MRR 0.846 | Recall@1 0.761 | Recall@3 0.910 | Recall@8 1.000 | AUC 0.942 | Major AUC 0.983 | Intermediate AUC 0.885\n",
      "2024-05-08 12:20:17,337 - \t\tNOVEL 3 ---- MRR 0.653 | Recall@1 0.517 | Recall@3 0.724 | Recall@8 0.897 | AUC 0.810 | Major AUC 0.983 | Intermediate AUC 0.688\n",
      "2024-05-08 12:23:09,919 - \t\tNOVEL 4 ---- MRR 0.944 | Recall@1 0.893 | Recall@3 1.000 | Recall@8 1.000 | AUC 0.970 | Major AUC 0.970 | Intermediate AUC 0.969\n",
      "2024-05-08 12:23:16,660 - \t\tNOVEL 5 ---- MRR 0.933 | Recall@1 0.900 | Recall@3 1.000 | Recall@8 1.000 | AUC 0.960 | Major AUC 1.000 | Intermediate AUC 0.933\n",
      "2024-05-08 12:29:01,200 - \t\tNOVEL 6 ---- MRR 0.812 | Recall@1 0.699 | Recall@3 0.932 | Recall@8 0.981 | AUC 0.941 | Major AUC 0.945 | Intermediate AUC 0.933\n",
      "2024-05-08 12:34:21,354 - \t\tNOVEL 7 ---- MRR 0.749 | Recall@1 0.592 | Recall@3 0.918 | Recall@8 0.969 | AUC 0.921 | Major AUC 0.947 | Intermediate AUC 0.779\n"
     ]
    }
   ],
   "source": [
    "!python main.py --model drama_luar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016837d84c9f4625aa0c2e11e42c06a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73644ee3f42e4f799f700b32fea3d1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac8c7f9b24a4878b6d2c755bba9f02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a072e9fca9f447989fc80480db5a5a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb27fe14eab41eeaac5ae32a5760350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9d9504afde4abca51c0d5f3cab6786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/335 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7ed2a9ba49454dacff1aa92d416a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.py:   0%|          | 0.00/535 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/rrivera1849/LUAR-MUD:\n",
      "- config.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6835ca65b7c947c9b8fc6123b66f1807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.py:   0%|          | 0.00/8.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/rrivera1849/LUAR-MUD:\n",
      "- model.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4da62d7d29c44118f6db922de38b2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3347e61cf8b84e1b9a7e109a66183613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc8f21a82f640bc81462c61d26ac538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from metrics import *\n",
    "from utils import luar_tokenize, get_model\n",
    "model, tokenizer = get_model(\"luar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, tokenizer = get_model(\"drama_luar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (transformer): MPNetModel(\n",
       "    (embeddings): MPNetEmbeddings(\n",
       "      (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): MPNetEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (relative_attention_bias): Embedding(32, 12)\n",
       "    )\n",
       "    (pooler): MPNetPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (attn_fn): SelfAttention()\n",
       "  (linear): Linear(in_features=768, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ExplicitQuoteCorpus(\"/data/datasets/project-dialogism-novel-corpus/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = []\n",
    "count_no_exp = 0\n",
    "no_exp_major = 0 \n",
    "total_count = 0\n",
    "\n",
    "for n in corpus : \n",
    "    d = {}\n",
    "    exp = np.asarray(n[\"is_explicit\"])\n",
    "    sid =  np.asarray(n[\"speaker_id\"])\n",
    "    for id in np.unique(n[\"speaker_id\"]) : \n",
    "        quotes = np.where(sid == id)[0]\n",
    "        \n",
    "        if sum(exp[quotes]) >= 5 : \n",
    "            d[id] = sum(exp[quotes]) / len(quotes) \n",
    "        else : \n",
    "            d[id] = 0 \n",
    "            count_no_exp += 1\n",
    "            if n[\"is_major\"][id] == 1 : \n",
    "                no_exp_major += 1\n",
    "    percentage.append(np.mean(list(d.values())))\n",
    "\n",
    "        \n",
    "    total_count += len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17142857142857143"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_exp_major / count_no_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11290322580645161"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_no_exp / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOVEL: AHandfulOfDust, ID 0 - # of queries: 30 - avg # of targets: 15.8 - avg # of quote targets: 1419.1 - avg query length: 52.0 - # of speakers in novel 17 - Percent Active Speakers 0.588\n",
      "NOVEL: APassageToIndia, ID 1 - # of queries: 115 - avg # of targets: 18.0 - avg # of quote targets: 1910.8 - avg query length: 17.2 - # of speakers in novel 18 - Percent Active Speakers 1.000\n",
      "NOVEL: ARoomWithAView, ID 2 - # of queries: 84 - avg # of targets: 10.0 - avg # of quote targets: 1475.4 - avg query length: 18.5 - # of speakers in novel 10 - Percent Active Speakers 1.000\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 - # of queries: 36 - avg # of targets: 11.0 - avg # of quote targets: 543.6 - avg query length: 16.1 - # of speakers in novel 11 - Percent Active Speakers 0.909\n",
      "NOVEL: AnneOfGreenGables, ID 4 - # of queries: 106 - avg # of targets: 5.0 - avg # of quote targets: 1066.0 - avg query length: 10.2 - # of speakers in novel 5 - Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 - # of queries: 10 - avg # of targets: 5.5 - avg # of quote targets: 263.0 - avg query length: 48.8 - # of speakers in novel 6 - Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 - # of queries: 163 - avg # of targets: 13.0 - avg # of quote targets: 1550.2 - avg query length: 9.6 - # of speakers in novel 13 - Percent Active Speakers 1.000\n",
      "NOVEL: HardTimes, ID 7 - # of queries: 124 - avg # of targets: 14.0 - avg # of quote targets: 1516.7 - avg query length: 12.4 - # of speakers in novel 14 - Percent Active Speakers 0.929\n",
      "NOVEL: HowardsEnd, ID 8 - # of queries: 126 - avg # of targets: 11.0 - avg # of quote targets: 2393.6 - avg query length: 19.4 - # of speakers in novel 11 - Percent Active Speakers 1.000\n",
      "NOVEL: MansfieldPark, ID 9 - # of queries: 153 - avg # of targets: 12.0 - avg # of quote targets: 1053.4 - avg query length: 6.9 - # of speakers in novel 12 - Percent Active Speakers 1.000\n",
      "NOVEL: NightAndDay, ID 10 - # of queries: 108 - avg # of targets: 13.0 - avg # of quote targets: 1938.2 - avg query length: 18.0 - # of speakers in novel 13 - Percent Active Speakers 0.846\n",
      "NOVEL: NorthangerAbbey, ID 11 - # of queries: 73 - avg # of targets: 6.0 - avg # of quote targets: 732.3 - avg query length: 10.4 - # of speakers in novel 6 - Percent Active Speakers 1.000\n",
      "NOVEL: OliverTwist, ID 12 - # of queries: 192 - avg # of targets: 28.0 - avg # of quote targets: 2283.5 - avg query length: 11.7 - # of speakers in novel 28 - Percent Active Speakers 0.929\n",
      "NOVEL: Persuasion, ID 13 - # of queries: 56 - avg # of targets: 9.0 - avg # of quote targets: 379.2 - avg query length: 7.0 - # of speakers in novel 9 - Percent Active Speakers 1.000\n",
      "NOVEL: PrideAndPrejudice, ID 14 - # of queries: 158 - avg # of targets: 14.0 - avg # of quote targets: 1175.5 - avg query length: 7.4 - # of speakers in novel 14 - Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 - # of queries: 130 - avg # of targets: 12.0 - avg # of quote targets: 990.9 - avg query length: 7.6 - # of speakers in novel 12 - Percent Active Speakers 1.000\n",
      "NOVEL: TheAgeOfInnocence, ID 16 - # of queries: 93 - avg # of targets: 12.0 - avg # of quote targets: 1229.2 - avg query length: 13.2 - # of speakers in novel 12 - Percent Active Speakers 0.917\n",
      "NOVEL: TheAwakening, ID 17 - # of queries: 58 - avg # of targets: 7.0 - avg # of quote targets: 524.2 - avg query length: 9.2 - # of speakers in novel 7 - Percent Active Speakers 1.000\n",
      "NOVEL: TheGambler, ID 18 - # of queries: 56 - avg # of targets: 7.0 - avg # of quote targets: 683.7 - avg query length: 13.0 - # of speakers in novel 7 - Percent Active Speakers 1.000\n",
      "NOVEL: TheInvisibleMan, ID 19 - # of queries: 57 - avg # of targets: 10.0 - avg # of quote targets: 792.9 - avg query length: 13.8 - # of speakers in novel 10 - Percent Active Speakers 0.900\n",
      "NOVEL: TheManWhoWasThursday, ID 20 - # of queries: 51 - avg # of targets: 9.0 - avg # of quote targets: 680.7 - avg query length: 13.9 - # of speakers in novel 9 - Percent Active Speakers 0.889\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 - # of queries: 76 - avg # of targets: 12.8 - avg # of quote targets: 1601.6 - avg query length: 22.0 - # of speakers in novel 13 - Percent Active Speakers 0.846\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 - # of queries: 41 - avg # of targets: 7.0 - avg # of quote targets: 956.5 - avg query length: 24.1 - # of speakers in novel 7 - Percent Active Speakers 0.857\n",
      "NOVEL: TheSignOfTheFour, ID 23 - # of queries: 38 - avg # of targets: 6.0 - avg # of quote targets: 543.9 - avg query length: 15.6 - # of speakers in novel 6 - Percent Active Speakers 1.000\n",
      "NOVEL: TheSportOfTheGods, ID 24 - # of queries: 47 - avg # of targets: 11.0 - avg # of quote targets: 483.1 - avg query length: 10.9 - # of speakers in novel 11 - Percent Active Speakers 1.000\n",
      "NOVEL: TheSunAlsoRises, ID 25 - # of queries: 68 - avg # of targets: 12.8 - avg # of quote targets: 2314.3 - avg query length: 34.7 - # of speakers in novel 13 - Percent Active Speakers 0.692\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 - # of queries: 32 - avg # of targets: 6.9 - avg # of quote targets: 855.9 - avg query length: 29.2 - # of speakers in novel 7 - Percent Active Speakers 0.857\n",
      "NOVEL: WinnieThePooh, ID 27 - # of queries: 46 - avg # of targets: 9.0 - avg # of quote targets: 762.1 - avg query length: 18.6 - # of speakers in novel 9 - Percent Active Speakers 1.000\n",
      "# Novels 28 - Speaker Activity 0.93 +/- (0.10) - Total # queries 2327 - Avg # queries 83.11 +/- (46.44) - Avg query length 17.55 +/- (11.20) - Avg # targets/query 10.99 +/- (4.59) - Avg # quote targets/query 1147.12 +/- (599.97) - Avg # speaker in novel 11.07 +/- (4.63)\n",
      "NOVEL: AHandfulOfDust, ID 0 - # of queries: 26 - avg # of targets: 15.9 - avg # of quote targets: 1170.7 - avg query length: 8.6 - # of speakers in novel 17 - Percent Active Speakers 0.588\n",
      "NOVEL: APassageToIndia, ID 1 - # of queries: 58 - avg # of targets: 18.0 - avg # of quote targets: 1642.0 - avg query length: 4.0 - # of speakers in novel 18 - Percent Active Speakers 0.833\n",
      "NOVEL: ARoomWithAView, ID 2 - # of queries: 63 - avg # of targets: 10.0 - avg # of quote targets: 1137.8 - avg query length: 5.5 - # of speakers in novel 10 - Percent Active Speakers 1.000\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 - # of queries: 32 - avg # of targets: 9.8 - avg # of quote targets: 105.1 - avg query length: 13.7 - # of speakers in novel 11 - Percent Active Speakers 0.818\n",
      "NOVEL: AnneOfGreenGables, ID 4 - # of queries: 96 - avg # of targets: 5.0 - avg # of quote targets: 505.4 - avg query length: 5.9 - # of speakers in novel 5 - Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 - # of queries: 10 - avg # of targets: 5.5 - avg # of quote targets: 149.0 - avg query length: 21.2 - # of speakers in novel 6 - Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 - # of queries: 96 - avg # of targets: 13.0 - avg # of quote targets: 1171.6 - avg query length: 3.5 - # of speakers in novel 13 - Percent Active Speakers 1.000\n",
      "NOVEL: HardTimes, ID 7 - # of queries: 95 - avg # of targets: 14.0 - avg # of quote targets: 806.0 - avg query length: 7.4 - # of speakers in novel 14 - Percent Active Speakers 0.929\n",
      "NOVEL: HowardsEnd, ID 8 - # of queries: 92 - avg # of targets: 11.0 - avg # of quote targets: 1878.3 - avg query length: 5.4 - # of speakers in novel 11 - Percent Active Speakers 1.000\n",
      "NOVEL: MansfieldPark, ID 9 - # of queries: 89 - avg # of targets: 12.0 - avg # of quote targets: 699.5 - avg query length: 3.4 - # of speakers in novel 12 - Percent Active Speakers 1.000\n",
      "NOVEL: NightAndDay, ID 10 - # of queries: 91 - avg # of targets: 12.9 - avg # of quote targets: 1282.7 - avg query length: 7.0 - # of speakers in novel 13 - Percent Active Speakers 0.846\n",
      "NOVEL: NorthangerAbbey, ID 11 - # of queries: 39 - avg # of targets: 6.0 - avg # of quote targets: 588.7 - avg query length: 3.1 - # of speakers in novel 6 - Percent Active Speakers 1.000\n",
      "NOVEL: OliverTwist, ID 12 - # of queries: 150 - avg # of targets: 28.0 - avg # of quote targets: 968.3 - avg query length: 8.4 - # of speakers in novel 28 - Percent Active Speakers 0.857\n",
      "NOVEL: Persuasion, ID 13 - # of queries: 32 - avg # of targets: 9.0 - avg # of quote targets: 217.7 - avg query length: 4.7 - # of speakers in novel 9 - Percent Active Speakers 1.000\n",
      "NOVEL: PrideAndPrejudice, ID 14 - # of queries: 76 - avg # of targets: 14.0 - avg # of quote targets: 877.1 - avg query length: 3.1 - # of speakers in novel 14 - Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 - # of queries: 76 - avg # of targets: 12.0 - avg # of quote targets: 649.5 - avg query length: 4.0 - # of speakers in novel 12 - Percent Active Speakers 0.917\n",
      "NOVEL: TheAgeOfInnocence, ID 16 - # of queries: 45 - avg # of targets: 12.0 - avg # of quote targets: 1039.0 - avg query length: 3.6 - # of speakers in novel 12 - Percent Active Speakers 0.833\n",
      "NOVEL: TheAwakening, ID 17 - # of queries: 30 - avg # of targets: 7.0 - avg # of quote targets: 405.1 - avg query length: 3.6 - # of speakers in novel 7 - Percent Active Speakers 1.000\n",
      "NOVEL: TheGambler, ID 18 - # of queries: 20 - avg # of targets: 7.0 - avg # of quote targets: 613.5 - avg query length: 3.2 - # of speakers in novel 7 - Percent Active Speakers 0.714\n",
      "NOVEL: TheInvisibleMan, ID 19 - # of queries: 44 - avg # of targets: 9.9 - avg # of quote targets: 459.5 - avg query length: 7.0 - # of speakers in novel 10 - Percent Active Speakers 0.900\n",
      "NOVEL: TheManWhoWasThursday, ID 20 - # of queries: 46 - avg # of targets: 9.0 - avg # of quote targets: 248.0 - avg query length: 9.8 - # of speakers in novel 9 - Percent Active Speakers 0.889\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 - # of queries: 29 - avg # of targets: 12.9 - avg # of quote targets: 1392.9 - avg query length: 6.7 - # of speakers in novel 13 - Percent Active Speakers 0.692\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 - # of queries: 34 - avg # of targets: 6.9 - avg # of quote targets: 751.0 - avg query length: 6.2 - # of speakers in novel 7 - Percent Active Speakers 0.714\n",
      "NOVEL: TheSignOfTheFour, ID 23 - # of queries: 15 - avg # of targets: 6.0 - avg # of quote targets: 464.9 - avg query length: 4.6 - # of speakers in novel 6 - Percent Active Speakers 0.500\n",
      "NOVEL: TheSportOfTheGods, ID 24 - # of queries: 18 - avg # of targets: 11.0 - avg # of quote targets: 413.6 - avg query length: 2.8 - # of speakers in novel 11 - Percent Active Speakers 1.000\n",
      "NOVEL: TheSunAlsoRises, ID 25 - # of queries: 43 - avg # of targets: 12.7 - avg # of quote targets: 1791.4 - avg query length: 12.0 - # of speakers in novel 13 - Percent Active Speakers 0.538\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 - # of queries: 27 - avg # of targets: 6.9 - avg # of quote targets: 688.8 - avg query length: 6.6 - # of speakers in novel 7 - Percent Active Speakers 0.857\n",
      "NOVEL: WinnieThePooh, ID 27 - # of queries: 41 - avg # of targets: 9.0 - avg # of quote targets: 392.8 - avg query length: 10.1 - # of speakers in novel 9 - Percent Active Speakers 0.889\n",
      "# Novels 28 - Speaker Activity 0.86 +/- (0.14) - Total # queries 1513 - Avg # queries 54.04 +/- (32.88) - Avg query length 6.61 +/- (3.95) - Avg # targets/query 10.94 +/- (4.60) - Avg # quote targets/query 803.92 +/- (477.47) - Avg # speaker in novel 11.07 +/- (4.63)\n",
      "NOVEL: AHandfulOfDust, ID 0 - # of queries: 26 - avg # of targets: 15.9 - avg # of quote targets: 1390.5 - avg query length: 8.6 - # of speakers in novel 17 - Percent Active Speakers 0.588\n",
      "NOVEL: APassageToIndia, ID 1 - # of queries: 58 - avg # of targets: 18.0 - avg # of quote targets: 1892.9 - avg query length: 4.0 - # of speakers in novel 18 - Percent Active Speakers 0.833\n",
      "NOVEL: ARoomWithAView, ID 2 - # of queries: 63 - avg # of targets: 10.0 - avg # of quote targets: 1471.1 - avg query length: 5.5 - # of speakers in novel 10 - Percent Active Speakers 1.000\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 - # of queries: 35 - avg # of targets: 11.0 - avg # of quote targets: 542.4 - avg query length: 13.3 - # of speakers in novel 11 - Percent Active Speakers 0.909\n",
      "NOVEL: AnneOfGreenGables, ID 4 - # of queries: 96 - avg # of targets: 5.0 - avg # of quote targets: 1066.2 - avg query length: 5.9 - # of speakers in novel 5 - Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 - # of queries: 10 - avg # of targets: 5.5 - avg # of quote targets: 263.0 - avg query length: 21.2 - # of speakers in novel 6 - Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 - # of queries: 96 - avg # of targets: 13.0 - avg # of quote targets: 1549.6 - avg query length: 3.5 - # of speakers in novel 13 - Percent Active Speakers 1.000\n",
      "NOVEL: HardTimes, ID 7 - # of queries: 95 - avg # of targets: 14.0 - avg # of quote targets: 1516.1 - avg query length: 7.4 - # of speakers in novel 14 - Percent Active Speakers 0.929\n",
      "NOVEL: HowardsEnd, ID 8 - # of queries: 92 - avg # of targets: 11.0 - avg # of quote targets: 2388.1 - avg query length: 5.4 - # of speakers in novel 11 - Percent Active Speakers 1.000\n",
      "NOVEL: MansfieldPark, ID 9 - # of queries: 89 - avg # of targets: 12.0 - avg # of quote targets: 1047.7 - avg query length: 3.4 - # of speakers in novel 12 - Percent Active Speakers 1.000\n",
      "NOVEL: NightAndDay, ID 10 - # of queries: 91 - avg # of targets: 12.9 - avg # of quote targets: 1937.9 - avg query length: 7.0 - # of speakers in novel 13 - Percent Active Speakers 0.846\n",
      "NOVEL: NorthangerAbbey, ID 11 - # of queries: 39 - avg # of targets: 6.0 - avg # of quote targets: 727.7 - avg query length: 3.1 - # of speakers in novel 6 - Percent Active Speakers 1.000\n",
      "NOVEL: OliverTwist, ID 12 - # of queries: 150 - avg # of targets: 28.0 - avg # of quote targets: 2281.8 - avg query length: 8.4 - # of speakers in novel 28 - Percent Active Speakers 0.857\n",
      "NOVEL: Persuasion, ID 13 - # of queries: 32 - avg # of targets: 9.0 - avg # of quote targets: 374.7 - avg query length: 4.7 - # of speakers in novel 9 - Percent Active Speakers 1.000\n",
      "NOVEL: PrideAndPrejudice, ID 14 - # of queries: 76 - avg # of targets: 14.0 - avg # of quote targets: 1170.1 - avg query length: 3.1 - # of speakers in novel 14 - Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 - # of queries: 76 - avg # of targets: 12.0 - avg # of quote targets: 988.2 - avg query length: 4.0 - # of speakers in novel 12 - Percent Active Speakers 0.917\n",
      "NOVEL: TheAgeOfInnocence, ID 16 - # of queries: 45 - avg # of targets: 12.0 - avg # of quote targets: 1225.8 - avg query length: 3.6 - # of speakers in novel 12 - Percent Active Speakers 0.833\n",
      "NOVEL: TheAwakening, ID 17 - # of queries: 30 - avg # of targets: 7.0 - avg # of quote targets: 521.4 - avg query length: 3.6 - # of speakers in novel 7 - Percent Active Speakers 1.000\n",
      "NOVEL: TheGambler, ID 18 - # of queries: 20 - avg # of targets: 7.0 - avg # of quote targets: 675.6 - avg query length: 3.2 - # of speakers in novel 7 - Percent Active Speakers 0.714\n",
      "NOVEL: TheInvisibleMan, ID 19 - # of queries: 45 - avg # of targets: 10.0 - avg # of quote targets: 790.1 - avg query length: 7.0 - # of speakers in novel 10 - Percent Active Speakers 0.900\n",
      "NOVEL: TheManWhoWasThursday, ID 20 - # of queries: 46 - avg # of targets: 9.0 - avg # of quote targets: 677.1 - avg query length: 9.8 - # of speakers in novel 9 - Percent Active Speakers 0.889\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 - # of queries: 29 - avg # of targets: 12.9 - avg # of quote targets: 1588.6 - avg query length: 6.7 - # of speakers in novel 13 - Percent Active Speakers 0.692\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 - # of queries: 34 - avg # of targets: 6.9 - avg # of quote targets: 954.6 - avg query length: 6.2 - # of speakers in novel 7 - Percent Active Speakers 0.714\n",
      "NOVEL: TheSignOfTheFour, ID 23 - # of queries: 15 - avg # of targets: 6.0 - avg # of quote targets: 534.7 - avg query length: 4.6 - # of speakers in novel 6 - Percent Active Speakers 0.500\n",
      "NOVEL: TheSportOfTheGods, ID 24 - # of queries: 18 - avg # of targets: 11.0 - avg # of quote targets: 470.8 - avg query length: 2.8 - # of speakers in novel 11 - Percent Active Speakers 1.000\n",
      "NOVEL: TheSunAlsoRises, ID 25 - # of queries: 43 - avg # of targets: 12.7 - avg # of quote targets: 2301.5 - avg query length: 12.0 - # of speakers in novel 13 - Percent Active Speakers 0.538\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 - # of queries: 27 - avg # of targets: 6.9 - avg # of quote targets: 852.7 - avg query length: 6.6 - # of speakers in novel 7 - Percent Active Speakers 0.857\n",
      "NOVEL: WinnieThePooh, ID 27 - # of queries: 41 - avg # of targets: 9.0 - avg # of quote targets: 762.1 - avg query length: 10.1 - # of speakers in novel 9 - Percent Active Speakers 0.889\n",
      "# Novels 28 - Speaker Activity 0.87 +/- (0.14) - Total # queries 1517 - Avg # queries 54.18 +/- (32.81) - Avg query length 6.59 +/- (3.93) - Avg # targets/query 10.99 +/- (4.60) - Avg # quote targets/query 1141.54 +/- (598.60) - Avg # speaker in novel 11.07 +/- (4.63)\n",
      "********** PROCESSING # UTTERANCES 1**********\n",
      "NOVEL: AHandfulOfDust, ID 0 Percent Active Speakers 0.235\n",
      "NOVEL: APassageToIndia, ID 1 Percent Active Speakers 0.889\n",
      "NOVEL: ARoomWithAView, ID 2 Percent Active Speakers 0.800\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 Percent Active Speakers 0.364\n",
      "NOVEL: AnneOfGreenGables, ID 4 Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 Percent Active Speakers 0.846\n",
      "NOVEL: HardTimes, ID 7 Percent Active Speakers 0.857\n",
      "NOVEL: HowardsEnd, ID 8 Percent Active Speakers 0.818\n",
      "NOVEL: MansfieldPark, ID 9 Percent Active Speakers 0.667\n",
      "NOVEL: NightAndDay, ID 10 Percent Active Speakers 0.769\n",
      "NOVEL: NorthangerAbbey, ID 11 Percent Active Speakers 0.833\n",
      "NOVEL: OliverTwist, ID 12 Percent Active Speakers 0.607\n",
      "NOVEL: Persuasion, ID 13 Percent Active Speakers 0.889\n",
      "NOVEL: PrideAndPrejudice, ID 14 Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 Percent Active Speakers 0.917\n",
      "NOVEL: TheAgeOfInnocence, ID 16 Percent Active Speakers 0.833\n",
      "NOVEL: TheAwakening, ID 17 Percent Active Speakers 0.714\n",
      "NOVEL: TheGambler, ID 18 Percent Active Speakers 1.000\n",
      "NOVEL: TheInvisibleMan, ID 19 Percent Active Speakers 0.200\n",
      "NOVEL: TheManWhoWasThursday, ID 20 Percent Active Speakers 0.778\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 Percent Active Speakers 0.769\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 Percent Active Speakers 0.571\n",
      "NOVEL: TheSignOfTheFour, ID 23 Percent Active Speakers 0.667\n",
      "NOVEL: TheSportOfTheGods, ID 24 Percent Active Speakers 0.818\n",
      "NOVEL: TheSunAlsoRises, ID 25 Percent Active Speakers 0.462\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 Percent Active Speakers 0.571\n",
      "NOVEL: WinnieThePooh, ID 27 Percent Active Speakers 0.889\n",
      "# Novels 28, Speaker Activity 0.736\n",
      "********** PROCESSING # UTTERANCES 5**********\n",
      "NOVEL: AHandfulOfDust, ID 0 Percent Active Speakers 0.235\n",
      "NOVEL: APassageToIndia, ID 1 Percent Active Speakers 0.889\n",
      "NOVEL: ARoomWithAView, ID 2 Percent Active Speakers 0.800\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 Percent Active Speakers 0.364\n",
      "NOVEL: AnneOfGreenGables, ID 4 Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 Percent Active Speakers 0.846\n",
      "NOVEL: HardTimes, ID 7 Percent Active Speakers 0.857\n",
      "NOVEL: HowardsEnd, ID 8 Percent Active Speakers 0.818\n",
      "NOVEL: MansfieldPark, ID 9 Percent Active Speakers 0.667\n",
      "NOVEL: NightAndDay, ID 10 Percent Active Speakers 0.769\n",
      "NOVEL: NorthangerAbbey, ID 11 Percent Active Speakers 0.833\n",
      "NOVEL: OliverTwist, ID 12 Percent Active Speakers 0.607\n",
      "NOVEL: Persuasion, ID 13 Percent Active Speakers 0.889\n",
      "NOVEL: PrideAndPrejudice, ID 14 Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 Percent Active Speakers 0.917\n",
      "NOVEL: TheAgeOfInnocence, ID 16 Percent Active Speakers 0.833\n",
      "NOVEL: TheAwakening, ID 17 Percent Active Speakers 0.714\n",
      "NOVEL: TheGambler, ID 18 Percent Active Speakers 1.000\n",
      "NOVEL: TheInvisibleMan, ID 19 Percent Active Speakers 0.200\n",
      "NOVEL: TheManWhoWasThursday, ID 20 Percent Active Speakers 0.778\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 Percent Active Speakers 0.769\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 Percent Active Speakers 0.571\n",
      "NOVEL: TheSignOfTheFour, ID 23 Percent Active Speakers 0.667\n",
      "NOVEL: TheSportOfTheGods, ID 24 Percent Active Speakers 0.818\n",
      "NOVEL: TheSunAlsoRises, ID 25 Percent Active Speakers 0.462\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 Percent Active Speakers 0.571\n",
      "NOVEL: WinnieThePooh, ID 27 Percent Active Speakers 0.889\n",
      "# Novels 28, Speaker Activity 0.736\n",
      "********** PROCESSING # UTTERANCES 10**********\n",
      "NOVEL: AHandfulOfDust, ID 0 Percent Active Speakers 0.235\n",
      "NOVEL: APassageToIndia, ID 1 Percent Active Speakers 0.889\n",
      "NOVEL: ARoomWithAView, ID 2 Percent Active Speakers 0.800\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 Percent Active Speakers 0.364\n",
      "NOVEL: AnneOfGreenGables, ID 4 Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 Percent Active Speakers 0.846\n",
      "NOVEL: HardTimes, ID 7 Percent Active Speakers 0.857\n",
      "NOVEL: HowardsEnd, ID 8 Percent Active Speakers 0.818\n",
      "NOVEL: MansfieldPark, ID 9 Percent Active Speakers 0.667\n",
      "NOVEL: NightAndDay, ID 10 Percent Active Speakers 0.769\n",
      "NOVEL: NorthangerAbbey, ID 11 Percent Active Speakers 0.833\n",
      "NOVEL: OliverTwist, ID 12 Percent Active Speakers 0.607\n",
      "NOVEL: Persuasion, ID 13 Percent Active Speakers 0.889\n",
      "NOVEL: PrideAndPrejudice, ID 14 Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 Percent Active Speakers 0.917\n",
      "NOVEL: TheAgeOfInnocence, ID 16 Percent Active Speakers 0.833\n",
      "NOVEL: TheAwakening, ID 17 Percent Active Speakers 0.714\n",
      "NOVEL: TheGambler, ID 18 Percent Active Speakers 1.000\n",
      "NOVEL: TheInvisibleMan, ID 19 Percent Active Speakers 0.200\n",
      "NOVEL: TheManWhoWasThursday, ID 20 Percent Active Speakers 0.778\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 Percent Active Speakers 0.769\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 Percent Active Speakers 0.571\n",
      "NOVEL: TheSignOfTheFour, ID 23 Percent Active Speakers 0.667\n",
      "NOVEL: TheSportOfTheGods, ID 24 Percent Active Speakers 0.818\n",
      "NOVEL: TheSunAlsoRises, ID 25 Percent Active Speakers 0.462\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 Percent Active Speakers 0.571\n",
      "NOVEL: WinnieThePooh, ID 27 Percent Active Speakers 0.889\n",
      "# Novels 28, Speaker Activity 0.736\n",
      "********** PROCESSING # UTTERANCES 20**********\n",
      "NOVEL: AHandfulOfDust, ID 0 Percent Active Speakers 0.235\n",
      "NOVEL: APassageToIndia, ID 1 Percent Active Speakers 0.889\n",
      "NOVEL: ARoomWithAView, ID 2 Percent Active Speakers 0.800\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 Percent Active Speakers 0.364\n",
      "NOVEL: AnneOfGreenGables, ID 4 Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 Percent Active Speakers 0.846\n",
      "NOVEL: HardTimes, ID 7 Percent Active Speakers 0.857\n",
      "NOVEL: HowardsEnd, ID 8 Percent Active Speakers 0.818\n",
      "NOVEL: MansfieldPark, ID 9 Percent Active Speakers 0.667\n",
      "NOVEL: NightAndDay, ID 10 Percent Active Speakers 0.769\n",
      "NOVEL: NorthangerAbbey, ID 11 Percent Active Speakers 0.833\n",
      "NOVEL: OliverTwist, ID 12 Percent Active Speakers 0.607\n",
      "NOVEL: Persuasion, ID 13 Percent Active Speakers 0.889\n",
      "NOVEL: PrideAndPrejudice, ID 14 Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 Percent Active Speakers 0.917\n",
      "NOVEL: TheAgeOfInnocence, ID 16 Percent Active Speakers 0.833\n",
      "NOVEL: TheAwakening, ID 17 Percent Active Speakers 0.714\n",
      "NOVEL: TheGambler, ID 18 Percent Active Speakers 1.000\n",
      "NOVEL: TheInvisibleMan, ID 19 Percent Active Speakers 0.200\n",
      "NOVEL: TheManWhoWasThursday, ID 20 Percent Active Speakers 0.778\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 Percent Active Speakers 0.769\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 Percent Active Speakers 0.571\n",
      "NOVEL: TheSignOfTheFour, ID 23 Percent Active Speakers 0.667\n",
      "NOVEL: TheSportOfTheGods, ID 24 Percent Active Speakers 0.818\n",
      "NOVEL: TheSunAlsoRises, ID 25 Percent Active Speakers 0.462\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 Percent Active Speakers 0.571\n",
      "NOVEL: WinnieThePooh, ID 27 Percent Active Speakers 0.889\n",
      "# Novels 28, Speaker Activity 0.736\n",
      "********** PROCESSING # UTTERANCES 50**********\n",
      "NOVEL: AHandfulOfDust, ID 0 Percent Active Speakers 0.235\n",
      "NOVEL: APassageToIndia, ID 1 Percent Active Speakers 0.889\n",
      "NOVEL: ARoomWithAView, ID 2 Percent Active Speakers 0.800\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 Percent Active Speakers 0.364\n",
      "NOVEL: AnneOfGreenGables, ID 4 Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 Percent Active Speakers 0.846\n",
      "NOVEL: HardTimes, ID 7 Percent Active Speakers 0.857\n",
      "NOVEL: HowardsEnd, ID 8 Percent Active Speakers 0.818\n",
      "NOVEL: MansfieldPark, ID 9 Percent Active Speakers 0.667\n",
      "NOVEL: NightAndDay, ID 10 Percent Active Speakers 0.769\n",
      "NOVEL: NorthangerAbbey, ID 11 Percent Active Speakers 0.833\n",
      "NOVEL: OliverTwist, ID 12 Percent Active Speakers 0.607\n",
      "NOVEL: Persuasion, ID 13 Percent Active Speakers 0.889\n",
      "NOVEL: PrideAndPrejudice, ID 14 Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 Percent Active Speakers 0.917\n",
      "NOVEL: TheAgeOfInnocence, ID 16 Percent Active Speakers 0.833\n",
      "NOVEL: TheAwakening, ID 17 Percent Active Speakers 0.714\n",
      "NOVEL: TheGambler, ID 18 Percent Active Speakers 1.000\n",
      "NOVEL: TheInvisibleMan, ID 19 Percent Active Speakers 0.200\n",
      "NOVEL: TheManWhoWasThursday, ID 20 Percent Active Speakers 0.778\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 Percent Active Speakers 0.769\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 Percent Active Speakers 0.571\n",
      "NOVEL: TheSignOfTheFour, ID 23 Percent Active Speakers 0.667\n",
      "NOVEL: TheSportOfTheGods, ID 24 Percent Active Speakers 0.818\n",
      "NOVEL: TheSunAlsoRises, ID 25 Percent Active Speakers 0.462\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 Percent Active Speakers 0.571\n",
      "NOVEL: WinnieThePooh, ID 27 Percent Active Speakers 0.889\n",
      "# Novels 28, Speaker Activity 0.736\n",
      "********** PROCESSING # UTTERANCES 100**********\n",
      "NOVEL: AHandfulOfDust, ID 0 Percent Active Speakers 0.235\n",
      "NOVEL: APassageToIndia, ID 1 Percent Active Speakers 0.889\n",
      "NOVEL: ARoomWithAView, ID 2 Percent Active Speakers 0.800\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 Percent Active Speakers 0.364\n",
      "NOVEL: AnneOfGreenGables, ID 4 Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 Percent Active Speakers 0.846\n",
      "NOVEL: HardTimes, ID 7 Percent Active Speakers 0.857\n",
      "NOVEL: HowardsEnd, ID 8 Percent Active Speakers 0.818\n",
      "NOVEL: MansfieldPark, ID 9 Percent Active Speakers 0.667\n",
      "NOVEL: NightAndDay, ID 10 Percent Active Speakers 0.769\n",
      "NOVEL: NorthangerAbbey, ID 11 Percent Active Speakers 0.833\n",
      "NOVEL: OliverTwist, ID 12 Percent Active Speakers 0.607\n",
      "NOVEL: Persuasion, ID 13 Percent Active Speakers 0.889\n",
      "NOVEL: PrideAndPrejudice, ID 14 Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 Percent Active Speakers 0.917\n",
      "NOVEL: TheAgeOfInnocence, ID 16 Percent Active Speakers 0.833\n",
      "NOVEL: TheAwakening, ID 17 Percent Active Speakers 0.714\n",
      "NOVEL: TheGambler, ID 18 Percent Active Speakers 1.000\n",
      "NOVEL: TheInvisibleMan, ID 19 Percent Active Speakers 0.200\n",
      "NOVEL: TheManWhoWasThursday, ID 20 Percent Active Speakers 0.778\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 Percent Active Speakers 0.769\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 Percent Active Speakers 0.571\n",
      "NOVEL: TheSignOfTheFour, ID 23 Percent Active Speakers 0.667\n",
      "NOVEL: TheSportOfTheGods, ID 24 Percent Active Speakers 0.818\n",
      "NOVEL: TheSunAlsoRises, ID 25 Percent Active Speakers 0.462\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 Percent Active Speakers 0.571\n",
      "NOVEL: WinnieThePooh, ID 27 Percent Active Speakers 0.889\n",
      "# Novels 28, Speaker Activity 0.736\n"
     ]
    }
   ],
   "source": [
    "min_utterances_for_query = 1\n",
    "iterator = [\n",
    "    (\n",
    "        corpus.chapterwise_AV_samples(\n",
    "            min_utterances_for_anchor=min_utterances_for_query\n",
    "        ),\n",
    "        \"chapterwise\",\n",
    "    ),\n",
    "    (\n",
    "        corpus.chapterwise_AV_samples(\n",
    "            train_with_explicit=True,\n",
    "            test_without_explicit=True,\n",
    "            min_utterances_for_anchor=min_utterances_for_query,\n",
    "        ),\n",
    "        \"explicit_to_other\",\n",
    "    ),\n",
    "    (\n",
    "        corpus.chapterwise_AV_samples(\n",
    "            train_with_explicit=True,\n",
    "            test_without_explicit=False,\n",
    "            min_utterances_for_anchor=min_utterances_for_query,\n",
    "        ),\n",
    "        \"explicit_to_all\",\n",
    "    ),\n",
    "]\n",
    "ns = [1, 5, 10, 20, 50, 100]\n",
    "pairs = []\n",
    "for n in ns:\n",
    "    print(\"*\" * 10 + f\" PROCESSING # UTTERANCES {n }\" + \"*\" * 10)\n",
    "    p = corpus.utterances_AV_samples(n_utterances=n, test_percentage=0.5)\n",
    "    pairs.append(p)\n",
    "iterator.append((pairs, \"reading_order\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (681 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "sizes = []\n",
    "for quote in corpus[\"quotes\"] : \n",
    "    for q in quote : \n",
    "        v = tokenizer(\n",
    "        q,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "        \n",
    "        sizes.append(v.input_ids.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.657857205227465"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sizes) / len(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1561, 1, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[\"input_ids\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Evelyn Waugh', 'AHandfulOfDust', 'literary', 3.0),\n",
       " (1, 'Edward Morgan Forster', 'APassageToIndia', 'literary', 3.0),\n",
       " (2, 'Edward Morgan Forster', 'ARoomWithAView', 'literary', 3.0),\n",
       " (3, 'Lewis Carroll', 'AlicesAdventuresInWonderland', 'children', 3.0),\n",
       " (4, 'Lucy Maud Montgomery', 'AnneOfGreenGables', 'children', 3.0),\n",
       " (5, 'Henry James', 'DaisyMiller', 'literary', 1.0),\n",
       " (6, 'Jane Austen', 'Emma', 'literary', 3.0),\n",
       " (7, 'Charles Dickens', 'HardTimes', 'literary', 3.0),\n",
       " (8, 'Edward Morgan Forster', 'HowardsEnd', 'literary', 3.0),\n",
       " (9, 'Jane Austen', 'MansfieldPark', 'literary', 3.0),\n",
       " (10, 'Virginia Woolf', 'NightAndDay', 'literary', 3.0),\n",
       " (11, 'Jane Austen', 'NorthangerAbbey', 'literary', 3.0),\n",
       " (12, 'Charles Dickens', 'OliverTwist', 'literary', 3.0),\n",
       " (13, 'Jane Austen', 'Persuasion', 'literary', 3.0),\n",
       " (14, 'Jane Austen', 'PrideAndPrejudice', 'literary', 3.0),\n",
       " (15, 'Jane Austen', 'SenseAndSensibility', 'literary', 3.0),\n",
       " (16, 'Edith Wharton', 'TheAgeOfInnocence', 'literary', 3.0),\n",
       " (17, 'Kate Chopin', 'TheAwakening', 'literary', 3.0),\n",
       " (18, 'Fyodor Mikhailovich Dostoevsky', 'TheGambler', 'literary', 1.0),\n",
       " (19, 'Herbert George Wells', 'TheInvisibleMan', 'scifi', 3.0),\n",
       " (20, 'Gilbert Keith  Chesterton', 'TheManWhoWasThursday', 'literary', 3.0),\n",
       " (21, 'Agatha Christie', 'TheMysteriousAffairAtStyles', 'crime', 1.0),\n",
       " (22, 'Oscar Wilde', 'ThePictureOfDorianGray', 'literary', 3.0),\n",
       " (23, 'Arthur Conan Doyle', 'TheSignOfTheFour', 'crime', 1.0),\n",
       " (24, 'Paul Laurence Dunbar', 'TheSportOfTheGods', 'literary', 3.0),\n",
       " (25, 'Ernest Hemingway', 'TheSunAlsoRises', 'literary', 1.0),\n",
       " (26, 'Edward Morgan Forster', 'WhereAngelsFearToTread', 'literary', 1.0),\n",
       " (27, nan, 'WinnieThePooh', 'children', 1.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(n, i.author, i.name, i.genre, i.narrative_person) for n,i in enumerate(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['children', 'crime', 'literary', 'scifi'], dtype='<U8'),\n",
       " array([ 3,  2, 22,  1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = [i.genre for n,i in enumerate(corpus)]\n",
    "import numpy as np \n",
    "np.unique(authors, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(n, i.genre) for n,i in enumerate(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12859884836852206"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(corpus[24][\"is_explicit\"]) / len(corpus[24][\"is_explicit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote_id</th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quoteText</th>\n",
       "      <th>subQuotationList</th>\n",
       "      <th>quoteByteSpans</th>\n",
       "      <th>speaker</th>\n",
       "      <th>addressees</th>\n",
       "      <th>quoteType</th>\n",
       "      <th>referringExpression</th>\n",
       "      <th>mentionTextsList</th>\n",
       "      <th>mentionSpansList</th>\n",
       "      <th>mentionEntitiesList</th>\n",
       "      <th>is_explicit</th>\n",
       "      <th>Character ID</th>\n",
       "      <th>role</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>214</td>\n",
       "      <td>Q214</td>\n",
       "      <td>You 'd just as well tell Kitty now,\\nma, for s...</td>\n",
       "      <td>[You 'd just as well tell Kitty now,\\nma, for ...</td>\n",
       "      <td>[[60648, 60728]]</td>\n",
       "      <td>Joe Hamilton</td>\n",
       "      <td>[Kitty Hamilton, Fannie Hamilton]</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>Joe spoke</td>\n",
       "      <td>[[You, Kitty, ma, she]]</td>\n",
       "      <td>[[[60648, 60651], [60673, 60678], [60684, 6068...</td>\n",
       "      <td>[[[Fannie Hamilton], [Kitty Hamilton], [Fannie...</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>221</td>\n",
       "      <td>Q221</td>\n",
       "      <td>I 'd like to cut the heart out of a few of 'em,</td>\n",
       "      <td>[I 'd like to cut the heart out of a few of 'em,]</td>\n",
       "      <td>[[61831, 61878]]</td>\n",
       "      <td>Joe Hamilton</td>\n",
       "      <td>[Kitty Hamilton, Fannie Hamilton]</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>said Joe</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>231</td>\n",
       "      <td>Q231</td>\n",
       "      <td>Let 's go to New York,</td>\n",
       "      <td>[Let 's go to New York,]</td>\n",
       "      <td>[[63727, 63749]]</td>\n",
       "      <td>Joe Hamilton</td>\n",
       "      <td>[Kitty Hamilton, Fannie Hamilton]</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>said Joe</td>\n",
       "      <td>[[s]]</td>\n",
       "      <td>[[[63732, 63733]]]</td>\n",
       "      <td>[[[Fannie Hamilton, Joe Hamilton, Kitty Hamilt...</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>243</td>\n",
       "      <td>Q243</td>\n",
       "      <td>Great place,</td>\n",
       "      <td>[Great place,]</td>\n",
       "      <td>[[75048, 75060]]</td>\n",
       "      <td>Joe Hamilton</td>\n",
       "      <td>[Kitty Hamilton, Mr. Thomas, Mrs. Jones, Fanni...</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>said Joe</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>297</td>\n",
       "      <td>Q297</td>\n",
       "      <td>No, no, this is on me,</td>\n",
       "      <td>[No, no, this is on me,]</td>\n",
       "      <td>[[94040, 94062]]</td>\n",
       "      <td>Joe Hamilton</td>\n",
       "      <td>[Mr. Thomas]</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>cried Joe</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>299</td>\n",
       "      <td>Q299</td>\n",
       "      <td>Happy to know you, Mr.\\nWilliams.</td>\n",
       "      <td>[Happy to know you, Mr.\\nWilliams.]</td>\n",
       "      <td>[[94831, 94863]]</td>\n",
       "      <td>Joe Hamilton</td>\n",
       "      <td>[Mr. Thomas, Sadness, Mr. Turner]</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>Joes</td>\n",
       "      <td>[[you, Mr.\\nWilliams]]</td>\n",
       "      <td>[[[94845, 94848], [94850, 94862]]]</td>\n",
       "      <td>[[[Sadness], [Sadness]]]</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>304</td>\n",
       "      <td>Q304</td>\n",
       "      <td>I cert'n'y will,</td>\n",
       "      <td>[I cert'n'y will,]</td>\n",
       "      <td>[[95712, 95728]]</td>\n",
       "      <td>Joe Hamilton</td>\n",
       "      <td>[Mr. Thomas, Sadness, Mr. Turner]</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>was Joes opportune remark</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>319</td>\n",
       "      <td>Q319</td>\n",
       "      <td>Oh, she did n't ask for me,</td>\n",
       "      <td>[Oh, she did n't ask for me,]</td>\n",
       "      <td>[[106283, 106310]]</td>\n",
       "      <td>Joe Hamilton</td>\n",
       "      <td>[Mr. Thomas]</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>cried Joe</td>\n",
       "      <td>[[she]]</td>\n",
       "      <td>[[[106287, 106290]]]</td>\n",
       "      <td>[[[Hattie Sterling]]]</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>326</td>\n",
       "      <td>Q326</td>\n",
       "      <td>Goo'-night, I be a' ri'. Goo'-night.</td>\n",
       "      <td>[Goo'-night,, I be a' ri'. Goo'-night.]</td>\n",
       "      <td>[[107401, 107412], [107434, 107458]]</td>\n",
       "      <td>Joe Hamilton</td>\n",
       "      <td>[Mr. Thomas, Hattie Sterling]</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>said Joe</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>365</td>\n",
       "      <td>Q365</td>\n",
       "      <td>There, there!\\nsee what you 've done with your...</td>\n",
       "      <td>[There, there!\\nsee what you 've done with you...</td>\n",
       "      <td>[[118174, 118239]]</td>\n",
       "      <td>Joe Hamilton</td>\n",
       "      <td>[Kit Hamilton, Fannie Hamilton]</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>Joe burst out</td>\n",
       "      <td>[[you, your]]</td>\n",
       "      <td>[[[118197, 118200], [118215, 118219]]]</td>\n",
       "      <td>[[[Fannie Hamilton], [Fannie Hamilton]]]</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>376</td>\n",
       "      <td>Q376</td>\n",
       "      <td>Oh, I know you fellows now well\\nenough to kno...</td>\n",
       "      <td>[Oh, I know you fellows now well\\nenough to kn...</td>\n",
       "      <td>[[122632, 122721]]</td>\n",
       "      <td>Joe Hamilton</td>\n",
       "      <td>[Sadness]</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>Young Hamilton hastened to protest</td>\n",
       "      <td>[[you fellows]]</td>\n",
       "      <td>[[[122643, 122654]]]</td>\n",
       "      <td>[[[Sadness]]]</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>430</td>\n",
       "      <td>Q430</td>\n",
       "      <td>Kit is a good deal of a jay yet,</td>\n",
       "      <td>[Kit is a good deal of a jay yet,]</td>\n",
       "      <td>[[140582, 140614]]</td>\n",
       "      <td>Joe Hamilton</td>\n",
       "      <td>[Hattie Sterling]</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>Joe remarked</td>\n",
       "      <td>[[Kit]]</td>\n",
       "      <td>[[[140582, 140585]]]</td>\n",
       "      <td>[[[Kit Hamilton]]]</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     quote_id quoteID                                          quoteText  \\\n",
       "182       214    Q214  You 'd just as well tell Kitty now,\\nma, for s...   \n",
       "187       221    Q221    I 'd like to cut the heart out of a few of 'em,   \n",
       "195       231    Q231                             Let 's go to New York,   \n",
       "204       243    Q243                                       Great place,   \n",
       "245       297    Q297                             No, no, this is on me,   \n",
       "247       299    Q299                  Happy to know you, Mr.\\nWilliams.   \n",
       "252       304    Q304                                   I cert'n'y will,   \n",
       "265       319    Q319                        Oh, she did n't ask for me,   \n",
       "272       326    Q326               Goo'-night, I be a' ri'. Goo'-night.   \n",
       "299       365    Q365  There, there!\\nsee what you 've done with your...   \n",
       "310       376    Q376  Oh, I know you fellows now well\\nenough to kno...   \n",
       "356       430    Q430                   Kit is a good deal of a jay yet,   \n",
       "\n",
       "                                      subQuotationList  \\\n",
       "182  [You 'd just as well tell Kitty now,\\nma, for ...   \n",
       "187  [I 'd like to cut the heart out of a few of 'em,]   \n",
       "195                           [Let 's go to New York,]   \n",
       "204                                     [Great place,]   \n",
       "245                           [No, no, this is on me,]   \n",
       "247                [Happy to know you, Mr.\\nWilliams.]   \n",
       "252                                 [I cert'n'y will,]   \n",
       "265                      [Oh, she did n't ask for me,]   \n",
       "272            [Goo'-night,, I be a' ri'. Goo'-night.]   \n",
       "299  [There, there!\\nsee what you 've done with you...   \n",
       "310  [Oh, I know you fellows now well\\nenough to kn...   \n",
       "356                 [Kit is a good deal of a jay yet,]   \n",
       "\n",
       "                           quoteByteSpans       speaker  \\\n",
       "182                      [[60648, 60728]]  Joe Hamilton   \n",
       "187                      [[61831, 61878]]  Joe Hamilton   \n",
       "195                      [[63727, 63749]]  Joe Hamilton   \n",
       "204                      [[75048, 75060]]  Joe Hamilton   \n",
       "245                      [[94040, 94062]]  Joe Hamilton   \n",
       "247                      [[94831, 94863]]  Joe Hamilton   \n",
       "252                      [[95712, 95728]]  Joe Hamilton   \n",
       "265                    [[106283, 106310]]  Joe Hamilton   \n",
       "272  [[107401, 107412], [107434, 107458]]  Joe Hamilton   \n",
       "299                    [[118174, 118239]]  Joe Hamilton   \n",
       "310                    [[122632, 122721]]  Joe Hamilton   \n",
       "356                    [[140582, 140614]]  Joe Hamilton   \n",
       "\n",
       "                                            addressees quoteType  \\\n",
       "182                  [Kitty Hamilton, Fannie Hamilton]  Explicit   \n",
       "187                  [Kitty Hamilton, Fannie Hamilton]  Explicit   \n",
       "195                  [Kitty Hamilton, Fannie Hamilton]  Explicit   \n",
       "204  [Kitty Hamilton, Mr. Thomas, Mrs. Jones, Fanni...  Explicit   \n",
       "245                                       [Mr. Thomas]  Explicit   \n",
       "247                  [Mr. Thomas, Sadness, Mr. Turner]  Explicit   \n",
       "252                  [Mr. Thomas, Sadness, Mr. Turner]  Explicit   \n",
       "265                                       [Mr. Thomas]  Explicit   \n",
       "272                      [Mr. Thomas, Hattie Sterling]  Explicit   \n",
       "299                    [Kit Hamilton, Fannie Hamilton]  Explicit   \n",
       "310                                          [Sadness]  Explicit   \n",
       "356                                  [Hattie Sterling]  Explicit   \n",
       "\n",
       "                    referringExpression         mentionTextsList  \\\n",
       "182                           Joe spoke  [[You, Kitty, ma, she]]   \n",
       "187                            said Joe                     [[]]   \n",
       "195                            said Joe                    [[s]]   \n",
       "204                            said Joe                     [[]]   \n",
       "245                           cried Joe                     [[]]   \n",
       "247                                Joes   [[you, Mr.\\nWilliams]]   \n",
       "252           was Joes opportune remark                     [[]]   \n",
       "265                           cried Joe                  [[she]]   \n",
       "272                            said Joe                 [[], []]   \n",
       "299                       Joe burst out            [[you, your]]   \n",
       "310  Young Hamilton hastened to protest          [[you fellows]]   \n",
       "356                        Joe remarked                  [[Kit]]   \n",
       "\n",
       "                                      mentionSpansList  \\\n",
       "182  [[[60648, 60651], [60673, 60678], [60684, 6068...   \n",
       "187                                               [[]]   \n",
       "195                                 [[[63732, 63733]]]   \n",
       "204                                               [[]]   \n",
       "245                                               [[]]   \n",
       "247                 [[[94845, 94848], [94850, 94862]]]   \n",
       "252                                               [[]]   \n",
       "265                               [[[106287, 106290]]]   \n",
       "272                                           [[], []]   \n",
       "299             [[[118197, 118200], [118215, 118219]]]   \n",
       "310                               [[[122643, 122654]]]   \n",
       "356                               [[[140582, 140585]]]   \n",
       "\n",
       "                                   mentionEntitiesList  is_explicit  \\\n",
       "182  [[[Fannie Hamilton], [Kitty Hamilton], [Fannie...            1   \n",
       "187                                               [[]]            1   \n",
       "195  [[[Fannie Hamilton, Joe Hamilton, Kitty Hamilt...            1   \n",
       "204                                               [[]]            1   \n",
       "245                                               [[]]            1   \n",
       "247                           [[[Sadness], [Sadness]]]            1   \n",
       "252                                               [[]]            1   \n",
       "265                              [[[Hattie Sterling]]]            1   \n",
       "272                                           [[], []]            1   \n",
       "299           [[[Fannie Hamilton], [Fannie Hamilton]]]            1   \n",
       "310                                      [[[Sadness]]]            1   \n",
       "356                                 [[[Kit Hamilton]]]            1   \n",
       "\n",
       "     Character ID   role  \n",
       "182            15  major  \n",
       "187            15  major  \n",
       "195            15  major  \n",
       "204            15  major  \n",
       "245            15  major  \n",
       "247            15  major  \n",
       "252            15  major  \n",
       "265            15  major  \n",
       "272            15  major  \n",
       "299            15  major  \n",
       "310            15  major  \n",
       "356            15  major  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expl = corpus[24].quote_table[corpus[24].quote_table[\"is_explicit\"]==1]\n",
    "expl[expl[\"Character ID\"]==15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = corpus[0].chapterwise_AV_samples(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66, 77, 89, 104, 123, 194]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1884, 1885)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = sum([i.tolist() for i in corpus[0].quote_id_by_chapter],[])\n",
    "max(v), len(corpus[0][\"is_explicit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 [1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1625, 1627, 1629, 1631, 1633, 1635, 1637, 1656, 1657, 1658, 1659, 1713, 1715, 1717, 1719, 1721, 1723, 1725, 1727, 1729]\n",
      "7 [1574, 1575, 1576, 1577, 1578, 1624, 1626, 1628, 1630, 1632, 1634, 1636, 1638, 1640, 1642, 1644, 1646, 1711, 1712, 1714, 1716, 1718, 1720, 1722, 1724, 1726, 1728, 1730, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779]\n",
      "9 [1507, 1508, 1510, 1512, 1514, 1516, 1517, 1519, 1520, 1521, 1522, 1523, 1525, 1542, 1570, 1571, 1572, 1573, 1579, 1580, 1581, 1583, 1585, 1587, 1589, 1591, 1593, 1595, 1597, 1598, 1599, 1601, 1604, 1606, 1607, 1609, 1611, 1613, 1647, 1648, 1649, 1651, 1653, 1655, 1660, 1662, 1663, 1665, 1667, 1669, 1670, 1671, 1672, 1674, 1676, 1678, 1679, 1681, 1683, 1685, 1687, 1688, 1690, 1691, 1693, 1694, 1695, 1696, 1697, 1699, 1701, 1703, 1705, 1707, 1708, 1710, 1731, 1733, 1735, 1736, 1738, 1740, 1742, 1747, 1749, 1751, 1753, 1755, 1756, 1758, 1760, 1761, 1763, 1765, 1767, 1769]\n",
      "10 [1639, 1641, 1643, 1645]\n",
      "22 [1582, 1584, 1586, 1588, 1590, 1592, 1594, 1596, 1600, 1602, 1603, 1605, 1608, 1610, 1612, 1650, 1652, 1654, 1661, 1664, 1666, 1673, 1675, 1677, 1680, 1682, 1684, 1689, 1692, 1698, 1700, 1702, 1704]\n",
      "33 [1527, 1530, 1532, 1534, 1536, 1537, 1538, 1539, 1541, 1543, 1544, 1546, 1548, 1549, 1551, 1553, 1555, 1557, 1559, 1561, 1563, 1565, 1567, 1568, 1569]\n",
      "35 [1500, 1501, 1502, 1503, 1504, 1505, 1506, 1509, 1511, 1513, 1515, 1518, 1524, 1526, 1528, 1529, 1531, 1533, 1535, 1540, 1545, 1547, 1550, 1552, 1554, 1556, 1558, 1560, 1562, 1564, 1566, 1668, 1686, 1706, 1709, 1732, 1734, 1737, 1739, 1741, 1743, 1744, 1745, 1746, 1748, 1750, 1752, 1754, 1757, 1759, 1762, 1764, 1766, 1768, 1770, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1794, 1796, 1798, 1800, 1802, 1804, 1806, 1808, 1810, 1812, 1813, 1814, 1815, 1817, 1819, 1821, 1823, 1826, 1828, 1833, 1835, 1843, 1845, 1846, 1848, 1850, 1852, 1854, 1856, 1858, 1861, 1863, 1865, 1867, 1869, 1875, 1877, 1879, 1881, 1883]\n",
      "19 [1793, 1795, 1797, 1799, 1801, 1803, 1805, 1807, 1809, 1811, 1816, 1818, 1820, 1822, 1824, 1825, 1827, 1829, 1830, 1831, 1832, 1834, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1844, 1847, 1849, 1851, 1853, 1855, 1857, 1859, 1860, 1862, 1864, 1866, 1868, 1870, 1871, 1872, 1873, 1874, 1876, 1878, 1880, 1882, 1884]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 [1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1625, 1627, 1629, 1631, 1633, 1635, 1637, 1656, 1657, 1658, 1659, 1713, 1715, 1717, 1719, 1721, 1723, 1725, 1727, 1729]\n",
      "7 [1574, 1575, 1576, 1577, 1578, 1624, 1626, 1628, 1630, 1632, 1634, 1636, 1638, 1640, 1642, 1644, 1646, 1711, 1712, 1714, 1716, 1718, 1720, 1722, 1724, 1726, 1728, 1730, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779]\n",
      "9 [1507, 1508, 1510, 1512, 1514, 1516, 1517, 1519, 1520, 1521, 1522, 1523, 1525, 1542, 1570, 1571, 1572, 1573, 1579, 1580, 1581, 1583, 1585, 1587, 1589, 1591, 1593, 1595, 1597, 1598, 1599, 1601, 1604, 1606, 1607, 1609, 1611, 1613, 1647, 1648, 1649, 1651, 1653, 1655, 1660, 1662, 1663, 1665, 1667, 1669, 1670, 1671, 1672, 1674, 1676, 1678, 1679, 1681, 1683, 1685, 1687, 1688, 1690, 1691, 1693, 1694, 1695, 1696, 1697, 1699, 1701, 1703, 1705, 1707, 1708, 1710, 1731, 1733, 1735, 1736, 1738, 1740, 1742, 1747, 1749, 1751, 1753, 1755, 1756, 1758, 1760, 1761, 1763, 1765, 1767, 1769]\n",
      "10 [1639, 1641, 1643, 1645]\n",
      "22 [1582, 1584, 1586, 1588, 1590, 1592, 1594, 1596, 1600, 1602, 1603, 1605, 1608, 1610, 1612, 1650, 1652, 1654, 1661, 1664, 1666, 1673, 1675, 1677, 1680, 1682, 1684, 1689, 1692, 1698, 1700, 1702, 1704]\n",
      "33 [1527, 1530, 1532, 1534, 1536, 1537, 1538, 1539, 1541, 1543, 1544, 1546, 1548, 1549, 1551, 1553, 1555, 1557, 1559, 1561, 1563, 1565, 1567, 1568, 1569]\n",
      "35 [1500, 1501, 1502, 1503, 1504, 1505, 1506, 1509, 1511, 1513, 1515, 1518, 1524, 1526, 1528, 1529, 1531, 1533, 1535, 1540, 1545, 1547, 1550, 1552, 1554, 1556, 1558, 1560, 1562, 1564, 1566, 1668, 1686, 1706, 1709, 1732, 1734, 1737, 1739, 1741, 1743, 1744, 1745, 1746, 1748, 1750, 1752, 1754, 1757, 1759, 1762, 1764, 1766, 1768, 1770, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1794, 1796, 1798, 1800, 1802, 1804, 1806, 1808, 1810, 1812, 1813, 1814, 1815, 1817, 1819, 1821, 1823, 1826, 1828, 1833, 1835, 1843, 1845, 1846, 1848, 1850, 1852, 1854, 1856, 1858, 1861, 1863, 1865, 1867, 1869, 1875, 1877, 1879, 1881, 1883]\n",
      "19 [1793, 1795, 1797, 1799, 1801, 1803, 1805, 1807, 1809, 1811, 1816, 1818, 1820, 1822, 1824, 1825, 1827, 1829, 1830, 1831, 1832, 1834, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1844, 1847, 1849, 1851, 1853, 1855, 1857, 1859, 1860, 1862, 1864, 1866, 1868, 1870, 1871, 1872, 1873, 1874, 1876, 1878, 1880, 1882, 1884]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 [1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1625, 1627, 1629, 1631, 1633, 1635, 1637, 1656, 1657, 1658, 1659, 1713, 1715, 1717, 1719, 1721, 1723, 1725, 1727, 1729]\n",
      "7 [1574, 1575, 1576, 1577, 1578, 1624, 1626, 1628, 1630, 1632, 1634, 1636, 1638, 1640, 1642, 1644, 1646, 1711, 1712, 1714, 1716, 1718, 1720, 1722, 1724, 1726, 1728, 1730, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779]\n",
      "9 [1507, 1508, 1510, 1512, 1514, 1516, 1517, 1519, 1520, 1521, 1522, 1523, 1525, 1542, 1570, 1571, 1572, 1573, 1579, 1580, 1581, 1583, 1585, 1587, 1589, 1591, 1593, 1595, 1597, 1598, 1599, 1601, 1604, 1606, 1607, 1609, 1611, 1613, 1647, 1648, 1649, 1651, 1653, 1655, 1660, 1662, 1663, 1665, 1667, 1669, 1670, 1671, 1672, 1674, 1676, 1678, 1679, 1681, 1683, 1685, 1687, 1688, 1690, 1691, 1693, 1694, 1695, 1696, 1697, 1699, 1701, 1703, 1705, 1707, 1708, 1710, 1731, 1733, 1735, 1736, 1738, 1740, 1742, 1747, 1749, 1751, 1753, 1755, 1756, 1758, 1760, 1761, 1763, 1765, 1767, 1769]\n",
      "10 [1639, 1641, 1643, 1645]\n",
      "22 [1582, 1584, 1586, 1588, 1590, 1592, 1594, 1596, 1600, 1602, 1603, 1605, 1608, 1610, 1612, 1650, 1652, 1654, 1661, 1664, 1666, 1673, 1675, 1677, 1680, 1682, 1684, 1689, 1692, 1698, 1700, 1702, 1704]\n",
      "33 [1527, 1530, 1532, 1534, 1536, 1537, 1538, 1539, 1541, 1543, 1544, 1546, 1548, 1549, 1551, 1553, 1555, 1557, 1559, 1561, 1563, 1565, 1567, 1568, 1569]\n",
      "35 [1500, 1501, 1502, 1503, 1504, 1505, 1506, 1509, 1511, 1513, 1515, 1518, 1524, 1526, 1528, 1529, 1531, 1533, 1535, 1540, 1545, 1547, 1550, 1552, 1554, 1556, 1558, 1560, 1562, 1564, 1566, 1668, 1686, 1706, 1709, 1732, 1734, 1737, 1739, 1741, 1743, 1744, 1745, 1746, 1748, 1750, 1752, 1754, 1757, 1759, 1762, 1764, 1766, 1768, 1770, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1794, 1796, 1798, 1800, 1802, 1804, 1806, 1808, 1810, 1812, 1813, 1814, 1815, 1817, 1819, 1821, 1823, 1826, 1828, 1833, 1835, 1843, 1845, 1846, 1848, 1850, 1852, 1854, 1856, 1858, 1861, 1863, 1865, 1867, 1869, 1875, 1877, 1879, 1881, 1883]\n",
      "19 [1793, 1795, 1797, 1799, 1801, 1803, 1805, 1807, 1809, 1811, 1816, 1818, 1820, 1822, 1824, 1825, 1827, 1829, 1830, 1831, 1832, 1834, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1844, 1847, 1849, 1851, 1853, 1855, 1857, 1859, 1860, 1862, 1864, 1866, 1868, 1870, 1871, 1872, 1873, 1874, 1876, 1878, 1880, 1882, 1884]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 [1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1625, 1627, 1629, 1631, 1633, 1635, 1637, 1656, 1657, 1658, 1659, 1713, 1715, 1717, 1719, 1721, 1723, 1725, 1727, 1729]\n",
      "7 [1574, 1575, 1576, 1577, 1578, 1624, 1626, 1628, 1630, 1632, 1634, 1636, 1638, 1640, 1642, 1644, 1646, 1711, 1712, 1714, 1716, 1718, 1720, 1722, 1724, 1726, 1728, 1730, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779]\n",
      "9 [1507, 1508, 1510, 1512, 1514, 1516, 1517, 1519, 1520, 1521, 1522, 1523, 1525, 1542, 1570, 1571, 1572, 1573, 1579, 1580, 1581, 1583, 1585, 1587, 1589, 1591, 1593, 1595, 1597, 1598, 1599, 1601, 1604, 1606, 1607, 1609, 1611, 1613, 1647, 1648, 1649, 1651, 1653, 1655, 1660, 1662, 1663, 1665, 1667, 1669, 1670, 1671, 1672, 1674, 1676, 1678, 1679, 1681, 1683, 1685, 1687, 1688, 1690, 1691, 1693, 1694, 1695, 1696, 1697, 1699, 1701, 1703, 1705, 1707, 1708, 1710, 1731, 1733, 1735, 1736, 1738, 1740, 1742, 1747, 1749, 1751, 1753, 1755, 1756, 1758, 1760, 1761, 1763, 1765, 1767, 1769]\n",
      "10 [1639, 1641, 1643, 1645]\n",
      "22 [1582, 1584, 1586, 1588, 1590, 1592, 1594, 1596, 1600, 1602, 1603, 1605, 1608, 1610, 1612, 1650, 1652, 1654, 1661, 1664, 1666, 1673, 1675, 1677, 1680, 1682, 1684, 1689, 1692, 1698, 1700, 1702, 1704]\n",
      "33 [1527, 1530, 1532, 1534, 1536, 1537, 1538, 1539, 1541, 1543, 1544, 1546, 1548, 1549, 1551, 1553, 1555, 1557, 1559, 1561, 1563, 1565, 1567, 1568, 1569]\n",
      "35 [1500, 1501, 1502, 1503, 1504, 1505, 1506, 1509, 1511, 1513, 1515, 1518, 1524, 1526, 1528, 1529, 1531, 1533, 1535, 1540, 1545, 1547, 1550, 1552, 1554, 1556, 1558, 1560, 1562, 1564, 1566, 1668, 1686, 1706, 1709, 1732, 1734, 1737, 1739, 1741, 1743, 1744, 1745, 1746, 1748, 1750, 1752, 1754, 1757, 1759, 1762, 1764, 1766, 1768, 1770, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1794, 1796, 1798, 1800, 1802, 1804, 1806, 1808, 1810, 1812, 1813, 1814, 1815, 1817, 1819, 1821, 1823, 1826, 1828, 1833, 1835, 1843, 1845, 1846, 1848, 1850, 1852, 1854, 1856, 1858, 1861, 1863, 1865, 1867, 1869, 1875, 1877, 1879, 1881, 1883]\n",
      "19 [1793, 1795, 1797, 1799, 1801, 1803, 1805, 1807, 1809, 1811, 1816, 1818, 1820, 1822, 1824, 1825, 1827, 1829, 1830, 1831, 1832, 1834, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1844, 1847, 1849, 1851, 1853, 1855, 1857, 1859, 1860, 1862, 1864, 1866, 1868, 1870, 1871, 1872, 1873, 1874, 1876, 1878, 1880, 1882, 1884]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 [1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1625, 1627, 1629, 1631, 1633, 1635, 1637, 1656, 1657, 1658, 1659, 1713, 1715, 1717, 1719, 1721, 1723, 1725, 1727, 1729]\n",
      "7 [1574, 1575, 1576, 1577, 1578, 1624, 1626, 1628, 1630, 1632, 1634, 1636, 1638, 1640, 1642, 1644, 1646, 1711, 1712, 1714, 1716, 1718, 1720, 1722, 1724, 1726, 1728, 1730, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779]\n",
      "9 [1507, 1508, 1510, 1512, 1514, 1516, 1517, 1519, 1520, 1521, 1522, 1523, 1525, 1542, 1570, 1571, 1572, 1573, 1579, 1580, 1581, 1583, 1585, 1587, 1589, 1591, 1593, 1595, 1597, 1598, 1599, 1601, 1604, 1606, 1607, 1609, 1611, 1613, 1647, 1648, 1649, 1651, 1653, 1655, 1660, 1662, 1663, 1665, 1667, 1669, 1670, 1671, 1672, 1674, 1676, 1678, 1679, 1681, 1683, 1685, 1687, 1688, 1690, 1691, 1693, 1694, 1695, 1696, 1697, 1699, 1701, 1703, 1705, 1707, 1708, 1710, 1731, 1733, 1735, 1736, 1738, 1740, 1742, 1747, 1749, 1751, 1753, 1755, 1756, 1758, 1760, 1761, 1763, 1765, 1767, 1769]\n",
      "10 [1639, 1641, 1643, 1645]\n",
      "22 [1582, 1584, 1586, 1588, 1590, 1592, 1594, 1596, 1600, 1602, 1603, 1605, 1608, 1610, 1612, 1650, 1652, 1654, 1661, 1664, 1666, 1673, 1675, 1677, 1680, 1682, 1684, 1689, 1692, 1698, 1700, 1702, 1704]\n",
      "33 [1527, 1530, 1532, 1534, 1536, 1537, 1538, 1539, 1541, 1543, 1544, 1546, 1548, 1549, 1551, 1553, 1555, 1557, 1559, 1561, 1563, 1565, 1567, 1568, 1569]\n",
      "35 [1500, 1501, 1502, 1503, 1504, 1505, 1506, 1509, 1511, 1513, 1515, 1518, 1524, 1526, 1528, 1529, 1531, 1533, 1535, 1540, 1545, 1547, 1550, 1552, 1554, 1556, 1558, 1560, 1562, 1564, 1566, 1668, 1686, 1706, 1709, 1732, 1734, 1737, 1739, 1741, 1743, 1744, 1745, 1746, 1748, 1750, 1752, 1754, 1757, 1759, 1762, 1764, 1766, 1768, 1770, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1794, 1796, 1798, 1800, 1802, 1804, 1806, 1808, 1810, 1812, 1813, 1814, 1815, 1817, 1819, 1821, 1823, 1826, 1828, 1833, 1835, 1843, 1845, 1846, 1848, 1850, 1852, 1854, 1856, 1858, 1861, 1863, 1865, 1867, 1869, 1875, 1877, 1879, 1881, 1883]\n",
      "19 [1793, 1795, 1797, 1799, 1801, 1803, 1805, 1807, 1809, 1811, 1816, 1818, 1820, 1822, 1824, 1825, 1827, 1829, 1830, 1831, 1832, 1834, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1844, 1847, 1849, 1851, 1853, 1855, 1857, 1859, 1860, 1862, 1864, 1866, 1868, 1870, 1871, 1872, 1873, 1874, 1876, 1878, 1880, 1882, 1884]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 [1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1625, 1627, 1629, 1631, 1633, 1635, 1637, 1656, 1657, 1658, 1659, 1713, 1715, 1717, 1719, 1721, 1723, 1725, 1727, 1729]\n",
      "7 [1574, 1575, 1576, 1577, 1578, 1624, 1626, 1628, 1630, 1632, 1634, 1636, 1638, 1640, 1642, 1644, 1646, 1711, 1712, 1714, 1716, 1718, 1720, 1722, 1724, 1726, 1728, 1730, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779]\n",
      "9 [1507, 1508, 1510, 1512, 1514, 1516, 1517, 1519, 1520, 1521, 1522, 1523, 1525, 1542, 1570, 1571, 1572, 1573, 1579, 1580, 1581, 1583, 1585, 1587, 1589, 1591, 1593, 1595, 1597, 1598, 1599, 1601, 1604, 1606, 1607, 1609, 1611, 1613, 1647, 1648, 1649, 1651, 1653, 1655, 1660, 1662, 1663, 1665, 1667, 1669, 1670, 1671, 1672, 1674, 1676, 1678, 1679, 1681, 1683, 1685, 1687, 1688, 1690, 1691, 1693, 1694, 1695, 1696, 1697, 1699, 1701, 1703, 1705, 1707, 1708, 1710, 1731, 1733, 1735, 1736, 1738, 1740, 1742, 1747, 1749, 1751, 1753, 1755, 1756, 1758, 1760, 1761, 1763, 1765, 1767, 1769]\n",
      "10 [1639, 1641, 1643, 1645]\n",
      "22 [1582, 1584, 1586, 1588, 1590, 1592, 1594, 1596, 1600, 1602, 1603, 1605, 1608, 1610, 1612, 1650, 1652, 1654, 1661, 1664, 1666, 1673, 1675, 1677, 1680, 1682, 1684, 1689, 1692, 1698, 1700, 1702, 1704]\n",
      "33 [1527, 1530, 1532, 1534, 1536, 1537, 1538, 1539, 1541, 1543, 1544, 1546, 1548, 1549, 1551, 1553, 1555, 1557, 1559, 1561, 1563, 1565, 1567, 1568, 1569]\n",
      "35 [1500, 1501, 1502, 1503, 1504, 1505, 1506, 1509, 1511, 1513, 1515, 1518, 1524, 1526, 1528, 1529, 1531, 1533, 1535, 1540, 1545, 1547, 1550, 1552, 1554, 1556, 1558, 1560, 1562, 1564, 1566, 1668, 1686, 1706, 1709, 1732, 1734, 1737, 1739, 1741, 1743, 1744, 1745, 1746, 1748, 1750, 1752, 1754, 1757, 1759, 1762, 1764, 1766, 1768, 1770, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1794, 1796, 1798, 1800, 1802, 1804, 1806, 1808, 1810, 1812, 1813, 1814, 1815, 1817, 1819, 1821, 1823, 1826, 1828, 1833, 1835, 1843, 1845, 1846, 1848, 1850, 1852, 1854, 1856, 1858, 1861, 1863, 1865, 1867, 1869, 1875, 1877, 1879, 1881, 1883]\n",
      "19 [1793, 1795, 1797, 1799, 1801, 1803, 1805, 1807, 1809, 1811, 1816, 1818, 1820, 1822, 1824, 1825, 1827, 1829, 1830, 1831, 1832, 1834, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1844, 1847, 1849, 1851, 1853, 1855, 1857, 1859, 1860, 1862, 1864, 1866, 1868, 1870, 1871, 1872, 1873, 1874, 1876, 1878, 1880, 1882, 1884]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 [1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1625, 1627, 1629, 1631, 1633, 1635, 1637, 1656, 1657, 1658, 1659, 1713, 1715, 1717, 1719, 1721, 1723, 1725, 1727, 1729]\n",
      "7 [1574, 1575, 1576, 1577, 1578, 1624, 1626, 1628, 1630, 1632, 1634, 1636, 1638, 1640, 1642, 1644, 1646, 1711, 1712, 1714, 1716, 1718, 1720, 1722, 1724, 1726, 1728, 1730, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779]\n",
      "9 [1507, 1508, 1510, 1512, 1514, 1516, 1517, 1519, 1520, 1521, 1522, 1523, 1525, 1542, 1570, 1571, 1572, 1573, 1579, 1580, 1581, 1583, 1585, 1587, 1589, 1591, 1593, 1595, 1597, 1598, 1599, 1601, 1604, 1606, 1607, 1609, 1611, 1613, 1647, 1648, 1649, 1651, 1653, 1655, 1660, 1662, 1663, 1665, 1667, 1669, 1670, 1671, 1672, 1674, 1676, 1678, 1679, 1681, 1683, 1685, 1687, 1688, 1690, 1691, 1693, 1694, 1695, 1696, 1697, 1699, 1701, 1703, 1705, 1707, 1708, 1710, 1731, 1733, 1735, 1736, 1738, 1740, 1742, 1747, 1749, 1751, 1753, 1755, 1756, 1758, 1760, 1761, 1763, 1765, 1767, 1769]\n",
      "10 [1639, 1641, 1643, 1645]\n",
      "22 [1582, 1584, 1586, 1588, 1590, 1592, 1594, 1596, 1600, 1602, 1603, 1605, 1608, 1610, 1612, 1650, 1652, 1654, 1661, 1664, 1666, 1673, 1675, 1677, 1680, 1682, 1684, 1689, 1692, 1698, 1700, 1702, 1704]\n",
      "33 [1527, 1530, 1532, 1534, 1536, 1537, 1538, 1539, 1541, 1543, 1544, 1546, 1548, 1549, 1551, 1553, 1555, 1557, 1559, 1561, 1563, 1565, 1567, 1568, 1569]\n",
      "35 [1500, 1501, 1502, 1503, 1504, 1505, 1506, 1509, 1511, 1513, 1515, 1518, 1524, 1526, 1528, 1529, 1531, 1533, 1535, 1540, 1545, 1547, 1550, 1552, 1554, 1556, 1558, 1560, 1562, 1564, 1566, 1668, 1686, 1706, 1709, 1732, 1734, 1737, 1739, 1741, 1743, 1744, 1745, 1746, 1748, 1750, 1752, 1754, 1757, 1759, 1762, 1764, 1766, 1768, 1770, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1794, 1796, 1798, 1800, 1802, 1804, 1806, 1808, 1810, 1812, 1813, 1814, 1815, 1817, 1819, 1821, 1823, 1826, 1828, 1833, 1835, 1843, 1845, 1846, 1848, 1850, 1852, 1854, 1856, 1858, 1861, 1863, 1865, 1867, 1869, 1875, 1877, 1879, 1881, 1883]\n",
      "19 [1793, 1795, 1797, 1799, 1801, 1803, 1805, 1807, 1809, 1811, 1816, 1818, 1820, 1822, 1824, 1825, 1827, 1829, 1830, 1831, 1832, 1834, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1844, 1847, 1849, 1851, 1853, 1855, 1857, 1859, 1860, 1862, 1864, 1866, 1868, 1870, 1871, 1872, 1873, 1874, 1876, 1878, 1880, 1882, 1884]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 [1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1625, 1627, 1629, 1631, 1633, 1635, 1637, 1656, 1657, 1658, 1659, 1713, 1715, 1717, 1719, 1721, 1723, 1725, 1727, 1729]\n",
      "7 [1574, 1575, 1576, 1577, 1578, 1624, 1626, 1628, 1630, 1632, 1634, 1636, 1638, 1640, 1642, 1644, 1646, 1711, 1712, 1714, 1716, 1718, 1720, 1722, 1724, 1726, 1728, 1730, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779]\n",
      "9 [1507, 1508, 1510, 1512, 1514, 1516, 1517, 1519, 1520, 1521, 1522, 1523, 1525, 1542, 1570, 1571, 1572, 1573, 1579, 1580, 1581, 1583, 1585, 1587, 1589, 1591, 1593, 1595, 1597, 1598, 1599, 1601, 1604, 1606, 1607, 1609, 1611, 1613, 1647, 1648, 1649, 1651, 1653, 1655, 1660, 1662, 1663, 1665, 1667, 1669, 1670, 1671, 1672, 1674, 1676, 1678, 1679, 1681, 1683, 1685, 1687, 1688, 1690, 1691, 1693, 1694, 1695, 1696, 1697, 1699, 1701, 1703, 1705, 1707, 1708, 1710, 1731, 1733, 1735, 1736, 1738, 1740, 1742, 1747, 1749, 1751, 1753, 1755, 1756, 1758, 1760, 1761, 1763, 1765, 1767, 1769]\n",
      "10 [1639, 1641, 1643, 1645]\n",
      "22 [1582, 1584, 1586, 1588, 1590, 1592, 1594, 1596, 1600, 1602, 1603, 1605, 1608, 1610, 1612, 1650, 1652, 1654, 1661, 1664, 1666, 1673, 1675, 1677, 1680, 1682, 1684, 1689, 1692, 1698, 1700, 1702, 1704]\n",
      "33 [1527, 1530, 1532, 1534, 1536, 1537, 1538, 1539, 1541, 1543, 1544, 1546, 1548, 1549, 1551, 1553, 1555, 1557, 1559, 1561, 1563, 1565, 1567, 1568, 1569]\n",
      "35 [1500, 1501, 1502, 1503, 1504, 1505, 1506, 1509, 1511, 1513, 1515, 1518, 1524, 1526, 1528, 1529, 1531, 1533, 1535, 1540, 1545, 1547, 1550, 1552, 1554, 1556, 1558, 1560, 1562, 1564, 1566, 1668, 1686, 1706, 1709, 1732, 1734, 1737, 1739, 1741, 1743, 1744, 1745, 1746, 1748, 1750, 1752, 1754, 1757, 1759, 1762, 1764, 1766, 1768, 1770, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1794, 1796, 1798, 1800, 1802, 1804, 1806, 1808, 1810, 1812, 1813, 1814, 1815, 1817, 1819, 1821, 1823, 1826, 1828, 1833, 1835, 1843, 1845, 1846, 1848, 1850, 1852, 1854, 1856, 1858, 1861, 1863, 1865, 1867, 1869, 1875, 1877, 1879, 1881, 1883]\n",
      "19 [1793, 1795, 1797, 1799, 1801, 1803, 1805, 1807, 1809, 1811, 1816, 1818, 1820, 1822, 1824, 1825, 1827, 1829, 1830, 1831, 1832, 1834, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1844, 1847, 1849, 1851, 1853, 1855, 1857, 1859, 1860, 1862, 1864, 1866, 1868, 1870, 1871, 1872, 1873, 1874, 1876, 1878, 1880, 1882, 1884]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 [1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1625, 1627, 1629, 1631, 1633, 1635, 1637, 1656, 1657, 1658, 1659, 1713, 1715, 1717, 1719, 1721, 1723, 1725, 1727, 1729]\n",
      "7 [1574, 1575, 1576, 1577, 1578, 1624, 1626, 1628, 1630, 1632, 1634, 1636, 1638, 1640, 1642, 1644, 1646, 1711, 1712, 1714, 1716, 1718, 1720, 1722, 1724, 1726, 1728, 1730, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779]\n",
      "9 [1507, 1508, 1510, 1512, 1514, 1516, 1517, 1519, 1520, 1521, 1522, 1523, 1525, 1542, 1570, 1571, 1572, 1573, 1579, 1580, 1581, 1583, 1585, 1587, 1589, 1591, 1593, 1595, 1597, 1598, 1599, 1601, 1604, 1606, 1607, 1609, 1611, 1613, 1647, 1648, 1649, 1651, 1653, 1655, 1660, 1662, 1663, 1665, 1667, 1669, 1670, 1671, 1672, 1674, 1676, 1678, 1679, 1681, 1683, 1685, 1687, 1688, 1690, 1691, 1693, 1694, 1695, 1696, 1697, 1699, 1701, 1703, 1705, 1707, 1708, 1710, 1731, 1733, 1735, 1736, 1738, 1740, 1742, 1747, 1749, 1751, 1753, 1755, 1756, 1758, 1760, 1761, 1763, 1765, 1767, 1769]\n",
      "10 [1639, 1641, 1643, 1645]\n",
      "22 [1582, 1584, 1586, 1588, 1590, 1592, 1594, 1596, 1600, 1602, 1603, 1605, 1608, 1610, 1612, 1650, 1652, 1654, 1661, 1664, 1666, 1673, 1675, 1677, 1680, 1682, 1684, 1689, 1692, 1698, 1700, 1702, 1704]\n",
      "33 [1527, 1530, 1532, 1534, 1536, 1537, 1538, 1539, 1541, 1543, 1544, 1546, 1548, 1549, 1551, 1553, 1555, 1557, 1559, 1561, 1563, 1565, 1567, 1568, 1569]\n",
      "35 [1500, 1501, 1502, 1503, 1504, 1505, 1506, 1509, 1511, 1513, 1515, 1518, 1524, 1526, 1528, 1529, 1531, 1533, 1535, 1540, 1545, 1547, 1550, 1552, 1554, 1556, 1558, 1560, 1562, 1564, 1566, 1668, 1686, 1706, 1709, 1732, 1734, 1737, 1739, 1741, 1743, 1744, 1745, 1746, 1748, 1750, 1752, 1754, 1757, 1759, 1762, 1764, 1766, 1768, 1770, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1794, 1796, 1798, 1800, 1802, 1804, 1806, 1808, 1810, 1812, 1813, 1814, 1815, 1817, 1819, 1821, 1823, 1826, 1828, 1833, 1835, 1843, 1845, 1846, 1848, 1850, 1852, 1854, 1856, 1858, 1861, 1863, 1865, 1867, 1869, 1875, 1877, 1879, 1881, 1883]\n",
      "19 [1793, 1795, 1797, 1799, 1801, 1803, 1805, 1807, 1809, 1811, 1816, 1818, 1820, 1822, 1824, 1825, 1827, 1829, 1830, 1831, 1832, 1834, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1844, 1847, 1849, 1851, 1853, 1855, 1857, 1859, 1860, 1862, 1864, 1866, 1868, 1870, 1871, 1872, 1873, 1874, 1876, 1878, 1880, 1882, 1884]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 [1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1625, 1627, 1629, 1631, 1633, 1635, 1637, 1656, 1657, 1658, 1659, 1713, 1715, 1717, 1719, 1721, 1723, 1725, 1727, 1729]\n",
      "7 [1574, 1575, 1576, 1577, 1578, 1624, 1626, 1628, 1630, 1632, 1634, 1636, 1638, 1640, 1642, 1644, 1646, 1711, 1712, 1714, 1716, 1718, 1720, 1722, 1724, 1726, 1728, 1730, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779]\n",
      "9 [1507, 1508, 1510, 1512, 1514, 1516, 1517, 1519, 1520, 1521, 1522, 1523, 1525, 1542, 1570, 1571, 1572, 1573, 1579, 1580, 1581, 1583, 1585, 1587, 1589, 1591, 1593, 1595, 1597, 1598, 1599, 1601, 1604, 1606, 1607, 1609, 1611, 1613, 1647, 1648, 1649, 1651, 1653, 1655, 1660, 1662, 1663, 1665, 1667, 1669, 1670, 1671, 1672, 1674, 1676, 1678, 1679, 1681, 1683, 1685, 1687, 1688, 1690, 1691, 1693, 1694, 1695, 1696, 1697, 1699, 1701, 1703, 1705, 1707, 1708, 1710, 1731, 1733, 1735, 1736, 1738, 1740, 1742, 1747, 1749, 1751, 1753, 1755, 1756, 1758, 1760, 1761, 1763, 1765, 1767, 1769]\n",
      "10 [1639, 1641, 1643, 1645]\n",
      "22 [1582, 1584, 1586, 1588, 1590, 1592, 1594, 1596, 1600, 1602, 1603, 1605, 1608, 1610, 1612, 1650, 1652, 1654, 1661, 1664, 1666, 1673, 1675, 1677, 1680, 1682, 1684, 1689, 1692, 1698, 1700, 1702, 1704]\n",
      "33 [1527, 1530, 1532, 1534, 1536, 1537, 1538, 1539, 1541, 1543, 1544, 1546, 1548, 1549, 1551, 1553, 1555, 1557, 1559, 1561, 1563, 1565, 1567, 1568, 1569]\n",
      "35 [1500, 1501, 1502, 1503, 1504, 1505, 1506, 1509, 1511, 1513, 1515, 1518, 1524, 1526, 1528, 1529, 1531, 1533, 1535, 1540, 1545, 1547, 1550, 1552, 1554, 1556, 1558, 1560, 1562, 1564, 1566, 1668, 1686, 1706, 1709, 1732, 1734, 1737, 1739, 1741, 1743, 1744, 1745, 1746, 1748, 1750, 1752, 1754, 1757, 1759, 1762, 1764, 1766, 1768, 1770, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1794, 1796, 1798, 1800, 1802, 1804, 1806, 1808, 1810, 1812, 1813, 1814, 1815, 1817, 1819, 1821, 1823, 1826, 1828, 1833, 1835, 1843, 1845, 1846, 1848, 1850, 1852, 1854, 1856, 1858, 1861, 1863, 1865, 1867, 1869, 1875, 1877, 1879, 1881, 1883]\n",
      "19 [1793, 1795, 1797, 1799, 1801, 1803, 1805, 1807, 1809, 1811, 1816, 1818, 1820, 1822, 1824, 1825, 1827, 1829, 1830, 1831, 1832, 1834, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1844, 1847, 1849, 1851, 1853, 1855, 1857, 1859, 1860, 1862, 1864, 1866, 1868, 1870, 1871, 1872, 1873, 1874, 1876, 1878, 1880, 1882, 1884]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 [1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1625, 1627, 1629, 1631, 1633, 1635, 1637, 1656, 1657, 1658, 1659, 1713, 1715, 1717, 1719, 1721, 1723, 1725, 1727, 1729]\n",
      "7 [1574, 1575, 1576, 1577, 1578, 1624, 1626, 1628, 1630, 1632, 1634, 1636, 1638, 1640, 1642, 1644, 1646, 1711, 1712, 1714, 1716, 1718, 1720, 1722, 1724, 1726, 1728, 1730, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779]\n",
      "9 [1507, 1508, 1510, 1512, 1514, 1516, 1517, 1519, 1520, 1521, 1522, 1523, 1525, 1542, 1570, 1571, 1572, 1573, 1579, 1580, 1581, 1583, 1585, 1587, 1589, 1591, 1593, 1595, 1597, 1598, 1599, 1601, 1604, 1606, 1607, 1609, 1611, 1613, 1647, 1648, 1649, 1651, 1653, 1655, 1660, 1662, 1663, 1665, 1667, 1669, 1670, 1671, 1672, 1674, 1676, 1678, 1679, 1681, 1683, 1685, 1687, 1688, 1690, 1691, 1693, 1694, 1695, 1696, 1697, 1699, 1701, 1703, 1705, 1707, 1708, 1710, 1731, 1733, 1735, 1736, 1738, 1740, 1742, 1747, 1749, 1751, 1753, 1755, 1756, 1758, 1760, 1761, 1763, 1765, 1767, 1769]\n",
      "10 [1639, 1641, 1643, 1645]\n",
      "22 [1582, 1584, 1586, 1588, 1590, 1592, 1594, 1596, 1600, 1602, 1603, 1605, 1608, 1610, 1612, 1650, 1652, 1654, 1661, 1664, 1666, 1673, 1675, 1677, 1680, 1682, 1684, 1689, 1692, 1698, 1700, 1702, 1704]\n",
      "33 [1527, 1530, 1532, 1534, 1536, 1537, 1538, 1539, 1541, 1543, 1544, 1546, 1548, 1549, 1551, 1553, 1555, 1557, 1559, 1561, 1563, 1565, 1567, 1568, 1569]\n",
      "35 [1500, 1501, 1502, 1503, 1504, 1505, 1506, 1509, 1511, 1513, 1515, 1518, 1524, 1526, 1528, 1529, 1531, 1533, 1535, 1540, 1545, 1547, 1550, 1552, 1554, 1556, 1558, 1560, 1562, 1564, 1566, 1668, 1686, 1706, 1709, 1732, 1734, 1737, 1739, 1741, 1743, 1744, 1745, 1746, 1748, 1750, 1752, 1754, 1757, 1759, 1762, 1764, 1766, 1768, 1770, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1794, 1796, 1798, 1800, 1802, 1804, 1806, 1808, 1810, 1812, 1813, 1814, 1815, 1817, 1819, 1821, 1823, 1826, 1828, 1833, 1835, 1843, 1845, 1846, 1848, 1850, 1852, 1854, 1856, 1858, 1861, 1863, 1865, 1867, 1869, 1875, 1877, 1879, 1881, 1883]\n",
      "19 [1793, 1795, 1797, 1799, 1801, 1803, 1805, 1807, 1809, 1811, 1816, 1818, 1820, 1822, 1824, 1825, 1827, 1829, 1830, 1831, 1832, 1834, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1844, 1847, 1849, 1851, 1853, 1855, 1857, 1859, 1860, 1862, 1864, 1866, 1868, 1870, 1871, 1872, 1873, 1874, 1876, 1878, 1880, 1882, 1884]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 [1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1625, 1627, 1629, 1631, 1633, 1635, 1637, 1656, 1657, 1658, 1659, 1713, 1715, 1717, 1719, 1721, 1723, 1725, 1727, 1729]\n",
      "7 [1574, 1575, 1576, 1577, 1578, 1624, 1626, 1628, 1630, 1632, 1634, 1636, 1638, 1640, 1642, 1644, 1646, 1711, 1712, 1714, 1716, 1718, 1720, 1722, 1724, 1726, 1728, 1730, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779]\n",
      "9 [1507, 1508, 1510, 1512, 1514, 1516, 1517, 1519, 1520, 1521, 1522, 1523, 1525, 1542, 1570, 1571, 1572, 1573, 1579, 1580, 1581, 1583, 1585, 1587, 1589, 1591, 1593, 1595, 1597, 1598, 1599, 1601, 1604, 1606, 1607, 1609, 1611, 1613, 1647, 1648, 1649, 1651, 1653, 1655, 1660, 1662, 1663, 1665, 1667, 1669, 1670, 1671, 1672, 1674, 1676, 1678, 1679, 1681, 1683, 1685, 1687, 1688, 1690, 1691, 1693, 1694, 1695, 1696, 1697, 1699, 1701, 1703, 1705, 1707, 1708, 1710, 1731, 1733, 1735, 1736, 1738, 1740, 1742, 1747, 1749, 1751, 1753, 1755, 1756, 1758, 1760, 1761, 1763, 1765, 1767, 1769]\n",
      "10 [1639, 1641, 1643, 1645]\n",
      "22 [1582, 1584, 1586, 1588, 1590, 1592, 1594, 1596, 1600, 1602, 1603, 1605, 1608, 1610, 1612, 1650, 1652, 1654, 1661, 1664, 1666, 1673, 1675, 1677, 1680, 1682, 1684, 1689, 1692, 1698, 1700, 1702, 1704]\n",
      "33 [1527, 1530, 1532, 1534, 1536, 1537, 1538, 1539, 1541, 1543, 1544, 1546, 1548, 1549, 1551, 1553, 1555, 1557, 1559, 1561, 1563, 1565, 1567, 1568, 1569]\n",
      "35 [1500, 1501, 1502, 1503, 1504, 1505, 1506, 1509, 1511, 1513, 1515, 1518, 1524, 1526, 1528, 1529, 1531, 1533, 1535, 1540, 1545, 1547, 1550, 1552, 1554, 1556, 1558, 1560, 1562, 1564, 1566, 1668, 1686, 1706, 1709, 1732, 1734, 1737, 1739, 1741, 1743, 1744, 1745, 1746, 1748, 1750, 1752, 1754, 1757, 1759, 1762, 1764, 1766, 1768, 1770, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1794, 1796, 1798, 1800, 1802, 1804, 1806, 1808, 1810, 1812, 1813, 1814, 1815, 1817, 1819, 1821, 1823, 1826, 1828, 1833, 1835, 1843, 1845, 1846, 1848, 1850, 1852, 1854, 1856, 1858, 1861, 1863, 1865, 1867, 1869, 1875, 1877, 1879, 1881, 1883]\n",
      "19 [1793, 1795, 1797, 1799, 1801, 1803, 1805, 1807, 1809, 1811, 1816, 1818, 1820, 1822, 1824, 1825, 1827, 1829, 1830, 1831, 1832, 1834, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1844, 1847, 1849, 1851, 1853, 1855, 1857, 1859, 1860, 1862, 1864, 1866, 1868, 1870, 1871, 1872, 1873, 1874, 1876, 1878, 1880, 1882, 1884]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5 [1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1625, 1627, 1629, 1631, 1633, 1635, 1637, 1656, 1657, 1658, 1659, 1713, 1715, 1717, 1719, 1721, 1723, 1725, 1727, 1729]\n",
      "7 [1574, 1575, 1576, 1577, 1578, 1624, 1626, 1628, 1630, 1632, 1634, 1636, 1638, 1640, 1642, 1644, 1646, 1711, 1712, 1714, 1716, 1718, 1720, 1722, 1724, 1726, 1728, 1730, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779]\n",
      "9 [1507, 1508, 1510, 1512, 1514, 1516, 1517, 1519, 1520, 1521, 1522, 1523, 1525, 1542, 1570, 1571, 1572, 1573, 1579, 1580, 1581, 1583, 1585, 1587, 1589, 1591, 1593, 1595, 1597, 1598, 1599, 1601, 1604, 1606, 1607, 1609, 1611, 1613, 1647, 1648, 1649, 1651, 1653, 1655, 1660, 1662, 1663, 1665, 1667, 1669, 1670, 1671, 1672, 1674, 1676, 1678, 1679, 1681, 1683, 1685, 1687, 1688, 1690, 1691, 1693, 1694, 1695, 1696, 1697, 1699, 1701, 1703, 1705, 1707, 1708, 1710, 1731, 1733, 1735, 1736, 1738, 1740, 1742, 1747, 1749, 1751, 1753, 1755, 1756, 1758, 1760, 1761, 1763, 1765, 1767, 1769]\n",
      "10 [1639, 1641, 1643, 1645]\n",
      "22 [1582, 1584, 1586, 1588, 1590, 1592, 1594, 1596, 1600, 1602, 1603, 1605, 1608, 1610, 1612, 1650, 1652, 1654, 1661, 1664, 1666, 1673, 1675, 1677, 1680, 1682, 1684, 1689, 1692, 1698, 1700, 1702, 1704]\n",
      "33 [1527, 1530, 1532, 1534, 1536, 1537, 1538, 1539, 1541, 1543, 1544, 1546, 1548, 1549, 1551, 1553, 1555, 1557, 1559, 1561, 1563, 1565, 1567, 1568, 1569]\n",
      "35 [1500, 1501, 1502, 1503, 1504, 1505, 1506, 1509, 1511, 1513, 1515, 1518, 1524, 1526, 1528, 1529, 1531, 1533, 1535, 1540, 1545, 1547, 1550, 1552, 1554, 1556, 1558, 1560, 1562, 1564, 1566, 1668, 1686, 1706, 1709, 1732, 1734, 1737, 1739, 1741, 1743, 1744, 1745, 1746, 1748, 1750, 1752, 1754, 1757, 1759, 1762, 1764, 1766, 1768, 1770, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1794, 1796, 1798, 1800, 1802, 1804, 1806, 1808, 1810, 1812, 1813, 1814, 1815, 1817, 1819, 1821, 1823, 1826, 1828, 1833, 1835, 1843, 1845, 1846, 1848, 1850, 1852, 1854, 1856, 1858, 1861, 1863, 1865, 1867, 1869, 1875, 1877, 1879, 1881, 1883]\n",
      "19 [1793, 1795, 1797, 1799, 1801, 1803, 1805, 1807, 1809, 1811, 1816, 1818, 1820, 1822, 1824, 1825, 1827, 1829, 1830, 1831, 1832, 1834, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1844, 1847, 1849, 1851, 1853, 1855, 1857, 1859, 1860, 1862, 1864, 1866, 1868, 1870, 1871, 1872, 1873, 1874, 1876, 1878, 1880, 1882, 1884]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "negs = [pairs[i][2] for i in range(len(pairs))]\n",
    "anc = [pairs[i][1] for i in range(len(pairs))]\n",
    "\n",
    "sids = [pairs[i][0] for i in range(len(pairs))]\n",
    "\n",
    "for sid, neg in zip(sids, negs) : \n",
    "    for speaker_id, nn in neg.items() : \n",
    "        print(speaker_id, nn)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 7, 10, 35]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapterwise Author Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, sample pairs are create as follows:\n",
    "\n",
    "- A positive pair is (quotes from speaker A in chapter C ; quotes from speaker A in all other chapters)\n",
    "- A negative pair is (quotes from speaker A in chapter C ; quotes from speaker B in all other chapters)\n",
    "\n",
    "If models are able to discriminate against positive and negative examples, it means that quotes of characters within a chapter are more self-similar than the quotes of other characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOVEL: AHandfulOfDust, ID 0 Percent Active Speakers 0.588\n",
      "NOVEL: APassageToIndia, ID 1 Percent Active Speakers 1.000\n",
      "NOVEL: ARoomWithAView, ID 2 Percent Active Speakers 1.000\n",
      "NOVEL: AlicesAdventuresInWonderland, ID 3 Percent Active Speakers 0.909\n",
      "NOVEL: AnneOfGreenGables, ID 4 Percent Active Speakers 1.000\n",
      "NOVEL: DaisyMiller, ID 5 Percent Active Speakers 0.833\n",
      "NOVEL: Emma, ID 6 Percent Active Speakers 1.000\n",
      "NOVEL: HardTimes, ID 7 Percent Active Speakers 0.929\n",
      "NOVEL: HowardsEnd, ID 8 Percent Active Speakers 1.000\n",
      "NOVEL: MansfieldPark, ID 9 Percent Active Speakers 1.000\n",
      "NOVEL: NightAndDay, ID 10 Percent Active Speakers 0.846\n",
      "NOVEL: NorthangerAbbey, ID 11 Percent Active Speakers 1.000\n",
      "NOVEL: OliverTwist, ID 12 Percent Active Speakers 0.929\n",
      "NOVEL: Persuasion, ID 13 Percent Active Speakers 1.000\n",
      "NOVEL: PrideAndPrejudice, ID 14 Percent Active Speakers 1.000\n",
      "NOVEL: SenseAndSensibility, ID 15 Percent Active Speakers 1.000\n",
      "NOVEL: TheAgeOfInnocence, ID 16 Percent Active Speakers 0.917\n",
      "NOVEL: TheAwakening, ID 17 Percent Active Speakers 1.000\n",
      "NOVEL: TheGambler, ID 18 Percent Active Speakers 1.000\n",
      "NOVEL: TheInvisibleMan, ID 19 Percent Active Speakers 0.900\n",
      "NOVEL: TheManWhoWasThursday, ID 20 Percent Active Speakers 0.889\n",
      "NOVEL: TheMysteriousAffairAtStyles, ID 21 Percent Active Speakers 0.846\n",
      "NOVEL: ThePictureOfDorianGray, ID 22 Percent Active Speakers 0.857\n",
      "NOVEL: TheSignOfTheFour, ID 23 Percent Active Speakers 1.000\n",
      "NOVEL: TheSportOfTheGods, ID 24 Percent Active Speakers 1.000\n",
      "NOVEL: TheSunAlsoRises, ID 25 Percent Active Speakers 0.692\n",
      "NOVEL: WhereAngelsFearToTread, ID 26 Percent Active Speakers 0.857\n",
      "NOVEL: WinnieThePooh, ID 27 Percent Active Speakers 1.000\n",
      "# Novels 28, Speaker Activity 0.928\n"
     ]
    }
   ],
   "source": [
    "pairs = corpus.chapterwise_AV_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEMANTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "quote_embeddings = []\n",
    "\n",
    "for quotes in tqdm.tqdm(corpus[\"quotes\"]) : \n",
    "    quote_embeddings.append(model.encode(quotes, device=torch.device(\"cuda:0\"), normalize_embeddings=True, convert_to_numpy=False, convert_to_tensor=True).cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character - Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = score(quote_embeddings, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.bar([f\"novel_{i}\" for i in range(len(aucs))], aucs)\n",
    "x = plt.xticks(rotation = 40, ha=\"right\")\n",
    "plt.ylim(0.4)\n",
    "plt.title(\"Semantics AUCs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('AnnaWegmann/Style-Embedding')\n",
    "quote_embeddings = []\n",
    "\n",
    "for quotes in tqdm.tqdm(corpus[\"quotes\"]) : \n",
    "    quote_embeddings.append(model.encode(quotes, device=torch.device(\"cuda:0\"), normalize_embeddings=True, convert_to_numpy=False, convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_auc, aucs, recalls = score(quote_embeddings, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.bar([f\"novel_{i}\" for i in range(len(aucs))], aucs)\n",
    "x = plt.xticks(rotation = 40, ha=\"right\")\n",
    "plt.ylim(0.4)\n",
    "plt.title(\"STEL (Stylo) AUCs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:26,  3.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/chapter_quote_embeddings.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f676d696368656c5f7374796c6f6d657472696373227d@ssh-remote%2Bgpu7/workspace/chapter_quote_embeddings.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     tokens \u001b[39m=\u001b[39m tokenizer(quotes[idx:idx\u001b[39m+\u001b[39mbatch_size], return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f676d696368656c5f7374796c6f6d657472696373227d@ssh-remote%2Bgpu7/workspace/chapter_quote_embeddings.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     u \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokens\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m), return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, output_hidden_states \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f676d696368656c5f7374796c6f6d657472696373227d@ssh-remote%2Bgpu7/workspace/chapter_quote_embeddings.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     last_h\u001b[39m.\u001b[39mappend(u\u001b[39m.\u001b[39;49mhidden_states[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][:,\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu())\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f676d696368656c5f7374796c6f6d657472696373227d@ssh-remote%2Bgpu7/workspace/chapter_quote_embeddings.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m quote_embeddings\u001b[39m.\u001b[39mappend(F\u001b[39m.\u001b[39mnormalize(torch\u001b[39m.\u001b[39mcat(last_h), dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('SamLowe/roberta-base-go_emotions')\n",
    "config = AutoConfig.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\", config=config).to(\"cuda:0\")\n",
    "\n",
    "\n",
    "\n",
    "quote_embeddings = []\n",
    "\n",
    "batch_size = 16\n",
    "with torch.no_grad() : \n",
    "    for idx, quotes in tqdm.tqdm(enumerate(corpus[\"quotes\"])) : \n",
    "        last_h = []\n",
    "        \n",
    "        model.eval()\n",
    "        for idx in range(0,len(quotes), batch_size) : \n",
    "            tokens = tokenizer(quotes[idx:idx+batch_size], return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "            u = model(**tokens.to(\"cuda:0\"), return_dict=True, output_hidden_states = True)\n",
    "            last_h.append(u.hidden_states[-1][:,0].detach().cpu())\n",
    "        quote_embeddings.append(F.normalize(torch.cat(last_h), dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1885, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_embeddings[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1885"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[\"quotes\"][0].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_auc, aucs, recalls = score(quote_embeddings, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.bar([f\"novel_{i}\" for i in range(len(aucs))], aucs)\n",
    "x = plt.xticks(rotation = 40, ha=\"right\")\n",
    "plt.ylim(0.4)\n",
    "plt.title(\"Emotions AUCs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LUAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rrivera1849/LUAR-MUD\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"rrivera1849/LUAR-MUD\", trust_remote_code=True).to(\"cuda:0\")\n",
    "def tokenize(quotes) : \n",
    "    tokens = tokenizer(quotes, max_length = 32, return_tensors = \"pt\", truncation=True, padding=\"max_length\")\n",
    "    tokens[\"input_ids\"] = tokens[\"input_ids\"].reshape(1, -1, 32)\n",
    "    tokens[\"attention_mask\"] = tokens[\"attention_mask\"].reshape(1, -1, 32)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "preds = []\n",
    "aucs = []\n",
    "R1 = []\n",
    "R3 = []\n",
    "R8 = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad() : \n",
    "    for nidx, novel_pairs in enumerate(pairs) : \n",
    "        \n",
    "        labels_n = []\n",
    "        preds_n = []\n",
    "        r_1, r_3, r_8 = [], [], []\n",
    "        quotes = np.asarray( corpus[\"quotes\"][nidx])\n",
    "        for u, (speaker_id, anchor, neg_and_pos) in enumerate(novel_pairs) :\n",
    "             \n",
    "            q1 =quotes[anchor]\n",
    "            tokens = tokenize(q1.tolist())\n",
    "            ep1 = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 ).cpu()\n",
    "            \n",
    "            q2 = quotes[neg_and_pos[speaker_id]]\n",
    "            tokens = tokenize(q2.tolist())\n",
    "            ep2 = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 ).cpu()\n",
    "            \n",
    "            embeddings = [ep2]\n",
    "            for sid, quote_ids in neg_and_pos.items() :\n",
    "                if sid != speaker_id  : \n",
    "                    neg = quotes[quote_ids]\n",
    "                    tokens = tokenize(neg.tolist())\n",
    "                    eneg = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 )                \n",
    "                    embeddings.append(eneg.cpu())\n",
    "                    \n",
    "            sims = util.dot_score(ep1[0], torch.cat(embeddings, dim = 0))[0]\n",
    "            sorted_idx = torch.argsort(sims)\n",
    "            r_1.append(1 if 0 in sorted_idx[-1:] else 0 )\n",
    "            r_3.append(1 if 0 in sorted_idx[-3:] else 0 )\n",
    "            r_8.append(1 if 0 in sorted_idx[-8:] else 0 )\n",
    "\n",
    "            labels_n.extend([1] + [0] * (len(sims) - 1))\n",
    "            preds_n.extend(sims.tolist())    \n",
    "            \n",
    "        R8.append(sum(r_8) / len(novel_pairs))\n",
    "        R1.append(sum(r_1) / len(novel_pairs))\n",
    "        R3.append(sum(r_3) / len(novel_pairs))\n",
    "\n",
    "        aucs.append(roc_auc_score(labels_n, preds_n))\n",
    "        labels.extend(labels_n)\n",
    "        preds.extend(preds_n)\n",
    "        print(f\"NOVEL {nidx} ---- Recall@{1} {R1[-1]:0.3f} | Recall@{3} {R3[-1]:0.3f} | Recall@{8} {R8[-1]:0.3f} | AUC {aucs[-1]:0.3f}\")\n",
    "\n",
    "    overall_auc = roc_auc_score(labels, preds)\n",
    "    print(\"\")\n",
    "    print(f\"Recall@{1} {np.mean(R1):0.3f} | Recall@{3} {np.mean(R3):0.3f} | Recall@{8} {np.mean(R8):0.3f} | AUC (average novels) {np.mean(aucs):0.3f} | Overall AUC {overall_auc:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.bar([f\"novel_{i}\" for i in range(len(aucs))], aucs)\n",
    "x = plt.xticks(rotation = 40, ha=\"right\")\n",
    "plt.ylim(0.4)\n",
    "plt.title(\"LUAR (Reddit) AUCs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapterwise Author Verification (Quote by quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "quote_embeddings = []\n",
    "\n",
    "for quotes in tqdm.tqdm(corpus[\"quotes\"]) : \n",
    "    quote_embeddings.append(model.encode(quotes, device=torch.device(\"cuda:0\"), normalize_embeddings=True, convert_to_numpy=False, convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = score_quote_by_quote(quote_embeddings, pairs, simulate_randomness=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('AnnaWegmann/Style-Embedding')\n",
    "quote_embeddings = []\n",
    "\n",
    "for quotes in tqdm.tqdm(corpus[\"quotes\"]) : \n",
    "    quote_embeddings.append(model.encode(quotes, device=torch.device(\"cuda:0\"), normalize_embeddings=True, convert_to_numpy=False, convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = score_quote_by_quote(quote_embeddings, pairs, simulate_randomness=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('SamLowe/roberta-base-go_emotions')\n",
    "config = AutoConfig.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\", config=config).to(\"cuda:0\")\n",
    "\n",
    "\n",
    "quote_embeddings = []\n",
    "\n",
    "batch_size = 16\n",
    "with torch.no_grad() : \n",
    "    for idx, quotes in tqdm.tqdm(enumerate(corpus[\"quotes\"])) : \n",
    "        last_h = []\n",
    "        \n",
    "        model.eval()\n",
    "        for idx in range(0,len(quotes), batch_size) : \n",
    "            tokens = tokenizer(quotes[idx:idx+batch_size], return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "            u = model(**tokens.to(\"cuda:0\"), return_dict=True, output_hidden_states = True)\n",
    "            last_h.append(u.hidden_states[-1][:,0].detach().cpu())\n",
    "        quote_embeddings.append(F.normalize(torch.cat(last_h), dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = score_quote_by_quote(quote_embeddings, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LUAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rrivera1849/LUAR-MUD\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"rrivera1849/LUAR-MUD\", trust_remote_code=True).to(\"cuda:0\")\n",
    "\n",
    "\n",
    "def tokenize(quotes, batch_first = False) : \n",
    "    tokens = tokenizer(quotes, max_length = 32, return_tensors = \"pt\", truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    if not batch_first:  \n",
    "        tokens[\"input_ids\"] = tokens[\"input_ids\"].reshape(1, -1, 32)\n",
    "        tokens[\"attention_mask\"] = tokens[\"attention_mask\"].reshape(1, -1, 32)\n",
    "    else : \n",
    "        tokens[\"input_ids\"] = tokens[\"input_ids\"].reshape(-1, 1, 32)\n",
    "        tokens[\"attention_mask\"] = tokens[\"attention_mask\"].reshape(-1, 1, 32)\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "aucs = []\n",
    "labels, preds = [], []\n",
    "model.eval()\n",
    "with torch.no_grad() : \n",
    "    for nidx, novel_pairs in enumerate(pairs) : \n",
    "        \n",
    "        scores_n = []\n",
    "        labels_n, preds_n = [], []\n",
    "\n",
    "        quotes = np.asarray( corpus[\"quotes\"][nidx])\n",
    "        for u, (speaker_id, anchor, neg_and_pos) in enumerate(novel_pairs) :\n",
    "             \n",
    "            q1 =quotes[anchor]\n",
    "            tokens = tokenize(q1.tolist())\n",
    "            ep1 = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 ).cpu()\n",
    "            \n",
    "            q2 = quotes[neg_and_pos[speaker_id]]\n",
    "            tokens = tokenize(q2.tolist(), batch_first = True)\n",
    "            ep2 = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 ).cpu()\n",
    "            \n",
    "            embeddings = [ep2]\n",
    "            true_relevance = [1] * ep2.size(0)\n",
    "            for sid, quote_ids in neg_and_pos.items() :\n",
    "                if sid != speaker_id  : \n",
    "                    neg = quotes[quote_ids]\n",
    "                    tokens = tokenize(neg.tolist(), batch_first = True)\n",
    "                    eneg = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 )                \n",
    "                    embeddings.append(eneg.cpu())\n",
    "                    true_relevance.extend([0] * eneg.size(0))\n",
    "                    \n",
    "            sims = util.dot_score(ep1[0], torch.cat(embeddings, dim = 0))[0]\n",
    "            score = ndcg_score(np.asarray(true_relevance)[np.newaxis, :], sims.cpu().numpy()[np.newaxis, :])\n",
    "            scores_n.append(score)\n",
    "            labels_n.extend(true_relevance)\n",
    "            preds_n.extend(sims.cpu().numpy() )\n",
    "            \n",
    "        scores.append(scores_n)\n",
    "        aucs.append(roc_auc_score(labels_n, preds_n))\n",
    "        labels.extend(labels_n)\n",
    "        preds.extend(preds_n)\n",
    "        \n",
    "        print(f\"NOVEL {nidx} ---- nDCG {np.mean(scores_n):0.3f} | AUC {aucs[-1]:0.3f}\")\n",
    "        \n",
    "    overall_auc = roc_auc_score(labels, preds)\n",
    "    print(\"\")\n",
    "    print(f\"Overall nDCG {np.mean(sum(scores,[])):0.3f} | (average novel) nDCG {np.mean([np.mean(i) for i in scores]):0.3f} | AUC (average novels) {np.mean(aucs):0.3f} | Overall AUC {overall_auc:0.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit Quotes Author Verification (Representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ExplicitQuoteCorpus(\"/data/datasets/project-dialogism-novel-corpus/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = corpus.explicit_AV_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "quote_embeddings = []\n",
    "\n",
    "for quotes in tqdm.tqdm(corpus[\"quotes\"]) : \n",
    "    quote_embeddings.append(model.encode(quotes, device=torch.device(\"cuda:0\"), normalize_embeddings=True, convert_to_numpy=False, convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_auc, aucs, recalls = score(quote_embeddings, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('AnnaWegmann/Style-Embedding')\n",
    "quote_embeddings = []\n",
    "\n",
    "for quotes in tqdm.tqdm(corpus[\"quotes\"]) : \n",
    "    quote_embeddings.append(model.encode(quotes, device=torch.device(\"cuda:0\"), normalize_embeddings=True, convert_to_numpy=False, convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_auc, aucs, recalls = score(quote_embeddings, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmotionS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('SamLowe/roberta-base-go_emotions')\n",
    "config = AutoConfig.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\", config=config).to(\"cuda:0\")\n",
    "\n",
    "\n",
    "quote_embeddings = []\n",
    "\n",
    "batch_size = 16\n",
    "with torch.no_grad() : \n",
    "    for idx, quotes in tqdm.tqdm(enumerate(corpus[\"quotes\"])) : \n",
    "        last_h = []\n",
    "        \n",
    "        model.eval()\n",
    "        for idx in range(0,len(quotes), batch_size) : \n",
    "            tokens = tokenizer(quotes[idx:idx+batch_size], return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "            u = model(**tokens.to(\"cuda:0\"), return_dict=True, output_hidden_states = True)\n",
    "            last_h.append(u.hidden_states[-1][:,0].detach().cpu())\n",
    "        quote_embeddings.append(F.normalize(torch.cat(last_h), dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_auc, aucs, recalls = score(quote_embeddings, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LUAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rrivera1849/LUAR-MUD\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"rrivera1849/LUAR-MUD\", trust_remote_code=True).to(\"cuda:0\")\n",
    "def tokenize(quotes) : \n",
    "    tokens = tokenizer(quotes, max_length = 32, return_tensors = \"pt\", truncation=True, padding=\"max_length\")\n",
    "    tokens[\"input_ids\"] = tokens[\"input_ids\"].reshape(1, -1, 32)\n",
    "    tokens[\"attention_mask\"] = tokens[\"attention_mask\"].reshape(1, -1, 32)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "preds = []\n",
    "aucs = []\n",
    "R1 = []\n",
    "R3 = []\n",
    "R8 = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad() : \n",
    "    for nidx, novel_pairs in enumerate(pairs) : \n",
    "        \n",
    "        labels_n = []\n",
    "        preds_n = []\n",
    "        r_1, r_3, r_8 = [], [], []\n",
    "        quotes = np.asarray( corpus[\"quotes\"][nidx])\n",
    "        for u, (speaker_id, anchor, neg_and_pos) in enumerate(novel_pairs) :\n",
    "             \n",
    "            q1 =quotes[anchor]\n",
    "            tokens = tokenize(q1.tolist())\n",
    "            ep1 = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 ).cpu()\n",
    "            \n",
    "            q2 = quotes[neg_and_pos[speaker_id]]\n",
    "            tokens = tokenize(q2.tolist())\n",
    "            ep2 = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 ).cpu()\n",
    "            \n",
    "            embeddings = [ep2]\n",
    "            for sid, quote_ids in neg_and_pos.items() :\n",
    "                if sid != speaker_id  : \n",
    "                    neg = quotes[quote_ids]\n",
    "                    tokens = tokenize(neg.tolist())\n",
    "                    eneg = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 )                \n",
    "                    embeddings.append(eneg.cpu())\n",
    "                    \n",
    "            sims = util.dot_score(ep1[0], torch.cat(embeddings, dim = 0))[0]\n",
    "            sorted_idx = torch.argsort(sims)\n",
    "            r_1.append(1 if 0 in sorted_idx[-1:] else 0 )\n",
    "            r_3.append(1 if 0 in sorted_idx[-3:] else 0 )\n",
    "            r_8.append(1 if 0 in sorted_idx[-8:] else 0 )\n",
    "\n",
    "            labels_n.extend([1] + [0] * (len(sims) - 1))\n",
    "            preds_n.extend(sims.tolist())    \n",
    "            \n",
    "        R8.append(sum(r_8) / len(novel_pairs))\n",
    "        R1.append(sum(r_1) / len(novel_pairs))\n",
    "        R3.append(sum(r_3) / len(novel_pairs))\n",
    "\n",
    "        aucs.append(roc_auc_score(labels_n, preds_n))\n",
    "        labels.extend(labels_n)\n",
    "        preds.extend(preds_n)\n",
    "        print(f\"NOVEL {nidx} ---- Recall@{1} {R1[-1]:0.3f} | Recall@{3} {R3[-1]:0.3f} | Recall@{8} {R8[-1]:0.3f} | AUC {aucs[-1]:0.3f}\")\n",
    "\n",
    "    overall_auc = roc_auc_score(labels, preds)\n",
    "    print(\"\")\n",
    "    print(f\"Recall@{1} {np.mean(R1):0.3f} | Recall@{3} {np.mean(R3):0.3f} | Recall@{8} {np.mean(R8):0.3f} | AUC (average novels) {np.mean(aucs):0.3f} | Overall AUC {overall_auc:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit Quotes Author Verification (Quote per quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ExplicitQuoteCorpus(\"/data/datasets/project-dialogism-novel-corpus/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = corpus.explicit_AV_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "quote_embeddings = []\n",
    "\n",
    "for quotes in tqdm.tqdm(corpus[\"quotes\"]) : \n",
    "    quote_embeddings.append(model.encode(quotes, device=torch.device(\"cuda:0\"), normalize_embeddings=True, convert_to_numpy=False, convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = score_quote_by_quote(quote_embeddings, pairs, simulate_randomness=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('AnnaWegmann/Style-Embedding')\n",
    "quote_embeddings = []\n",
    "\n",
    "for quotes in tqdm.tqdm(corpus[\"quotes\"]) : \n",
    "    quote_embeddings.append(model.encode(quotes, device=torch.device(\"cuda:0\"), normalize_embeddings=True, convert_to_numpy=False, convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = score_quote_by_quote(quote_embeddings, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('SamLowe/roberta-base-go_emotions')\n",
    "config = AutoConfig.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\", config=config).to(\"cuda:0\")\n",
    "\n",
    "\n",
    "quote_embeddings = []\n",
    "\n",
    "batch_size = 16\n",
    "with torch.no_grad() : \n",
    "    for idx, quotes in tqdm.tqdm(enumerate(corpus[\"quotes\"])) : \n",
    "        last_h = []\n",
    "        \n",
    "        model.eval()\n",
    "        for idx in range(0,len(quotes), batch_size) : \n",
    "            tokens = tokenizer(quotes[idx:idx+batch_size], return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "            u = model(**tokens.to(\"cuda:0\"), return_dict=True, output_hidden_states = True)\n",
    "            last_h.append(u.hidden_states[-1][:,0].detach().cpu())\n",
    "        quote_embeddings.append(F.normalize(torch.cat(last_h), dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = score_quote_by_quote(quote_embeddings, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LUAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rrivera1849/LUAR-MUD\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"rrivera1849/LUAR-MUD\", trust_remote_code=True).to(\"cuda:0\")\n",
    "\n",
    "\n",
    "def tokenize(quotes, batch_first = False) : \n",
    "    tokens = tokenizer(quotes, max_length = 32, return_tensors = \"pt\", truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    if not batch_first:  \n",
    "        tokens[\"input_ids\"] = tokens[\"input_ids\"].reshape(1, -1, 32)\n",
    "        tokens[\"attention_mask\"] = tokens[\"attention_mask\"].reshape(1, -1, 32)\n",
    "    else : \n",
    "        tokens[\"input_ids\"] = tokens[\"input_ids\"].reshape(-1, 1, 32)\n",
    "        tokens[\"attention_mask\"] = tokens[\"attention_mask\"].reshape(-1, 1, 32)\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "aucs = []\n",
    "labels, preds = [], []\n",
    "model.eval()\n",
    "with torch.no_grad() : \n",
    "    for nidx, novel_pairs in enumerate(pairs) : \n",
    "        \n",
    "        scores_n = []\n",
    "        labels_n, preds_n = [], []\n",
    "\n",
    "        quotes = np.asarray( corpus[\"quotes\"][nidx])\n",
    "        for u, (speaker_id, anchor, neg_and_pos) in enumerate(novel_pairs) :\n",
    "             \n",
    "            q1 =quotes[anchor]\n",
    "            tokens = tokenize(q1.tolist())\n",
    "            ep1 = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 ).cpu()\n",
    "            \n",
    "            q2 = quotes[neg_and_pos[speaker_id]]\n",
    "            tokens = tokenize(q2.tolist(), batch_first = True)\n",
    "            ep2 = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 ).cpu()\n",
    "            \n",
    "            embeddings = [ep2]\n",
    "            true_relevance = [1] * ep2.size(0)\n",
    "            for sid, quote_ids in neg_and_pos.items() :\n",
    "                if sid != speaker_id  : \n",
    "                    neg = quotes[quote_ids]\n",
    "                    tokens = tokenize(neg.tolist(), batch_first = True)\n",
    "                    eneg = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 )                \n",
    "                    embeddings.append(eneg.cpu())\n",
    "                    true_relevance.extend([0] * eneg.size(0))\n",
    "                    \n",
    "            sims = util.dot_score(ep1[0], torch.cat(embeddings, dim = 0))[0]\n",
    "            score = ndcg_score(np.asarray(true_relevance)[np.newaxis, :], sims.cpu().numpy()[np.newaxis, :])\n",
    "            scores_n.append(score)\n",
    "            labels_n.extend(true_relevance)\n",
    "            preds_n.extend(sims.cpu().numpy() )\n",
    "            \n",
    "        scores.append(scores_n)\n",
    "        aucs.append(roc_auc_score(labels_n, preds_n))\n",
    "        labels.extend(labels_n)\n",
    "        preds.extend(preds_n)\n",
    "        \n",
    "        print(f\"NOVEL {nidx} ---- nDCG {np.mean(scores_n):0.3f} | AUC {aucs[-1]:0.3f}\")\n",
    "        \n",
    "    overall_auc = roc_auc_score(labels, preds)\n",
    "    print(\"\")\n",
    "    print(f\"Overall nDCG {np.mean(sum(scores,[])):0.3f} | (average novel) nDCG {np.mean([np.mean(i) for i in scores]):0.3f} | AUC (average novels) {np.mean(aucs):0.3f} | Overall AUC {overall_auc:0.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying Input Size - Reading Order (Representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0].speaker_id_by_chapter[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** PROCESSING CHAPTER PERCENT 0.1**********\n",
      "7 1 4.0\n",
      "5\n",
      "NOVEL: AHandfulOfDust, ID 0 Percent Active Speakers 0.059\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/chapter_quote_embeddings.ipynb Cell 72\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f676d696368656c5f7374796c6f6d657472696373227d@ssh-remote%2Bgpu7/workspace/chapter_quote_embeddings.ipynb#Y131sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m ns : \n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f676d696368656c5f7374796c6f6d657472696373227d@ssh-remote%2Bgpu7/workspace/chapter_quote_embeddings.ipynb#Y131sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m PROCESSING CHAPTER PERCENT \u001b[39m\u001b[39m{\u001b[39;00mn\u001b[39m \u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f676d696368656c5f7374796c6f6d657472696373227d@ssh-remote%2Bgpu7/workspace/chapter_quote_embeddings.ipynb#Y131sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     pairs \u001b[39m=\u001b[39m corpus\u001b[39m.\u001b[39;49mchapterwise_AV_samples(percent_train_chapters\u001b[39m=\u001b[39;49mn, percent_test_chapters\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f676d696368656c5f7374796c6f6d657472696373227d@ssh-remote%2Bgpu7/workspace/chapter_quote_embeddings.ipynb#Y131sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     all_pairs\u001b[39m.\u001b[39mappend(pairs)\n",
      "File \u001b[0;32m/workspace/quote_prediction/data.py:927\u001b[0m, in \u001b[0;36mExplicitQuoteCorpus.chapterwise_AV_samples\u001b[0;34m(self, percent_train_chapters, percent_test_chapters)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[39mfor\u001b[39;00m idx, novel \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnovels):\n\u001b[1;32m    926\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(novel, \u001b[39m\"\u001b[39m\u001b[39mchapter_boundaries\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 927\u001b[0m         novel\u001b[39m.\u001b[39;49msplit_by_chapter()\n\u001b[1;32m    928\u001b[0m     p \u001b[39m=\u001b[39m novel\u001b[39m.\u001b[39mchapterwise_AV_samples(\n\u001b[1;32m    929\u001b[0m         percent_train_chapters\u001b[39m=\u001b[39mpercent_train_chapters,\n\u001b[1;32m    930\u001b[0m         percent_test_chapters\u001b[39m=\u001b[39mpercent_test_chapters,\n\u001b[1;32m    931\u001b[0m     )\n\u001b[1;32m    932\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    933\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOVEL: \u001b[39m\u001b[39m{\u001b[39;00mnovel\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m, ID \u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m Percent Active Speakers \u001b[39m\u001b[39m{\u001b[39;00mnovel\u001b[39m.\u001b[39mpercent_active_speakers\u001b[39m:\u001b[39;00m\u001b[39m0.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/quote_prediction/data.py:487\u001b[0m, in \u001b[0;36mNovel.split_by_chapter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m     BEST_MATCH \u001b[39m=\u001b[39m CHAPTER_STRINGS[np\u001b[39m.\u001b[39margmax(occurences)]\n\u001b[1;32m    483\u001b[0m     chapter_condition \u001b[39m=\u001b[39m \u001b[39mall\u001b[39m(\n\u001b[1;32m    484\u001b[0m         [\n\u001b[1;32m    485\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m doc[n \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtext,\n\u001b[1;32m    486\u001b[0m             tok\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m BEST_MATCH,\n\u001b[0;32m--> 487\u001b[0m             VERIF_NUMBER_STATEMENT(doc[n \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mtext\u001b[39m.\u001b[39;49mlower()),\n\u001b[1;32m    488\u001b[0m         ]\n\u001b[1;32m    489\u001b[0m     )\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m chapter_condition:\n\u001b[1;32m    492\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchapter_boundaries) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/workspace/quote_prediction/data.py:21\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     17\u001b[0m LOWERCASED_LATIN_NUMBERS \u001b[39m=\u001b[39m [roman\u001b[39m.\u001b[39mtoRoman(i)\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m)]\n\u001b[1;32m     18\u001b[0m STRING_NUMBERS \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m)]\n\u001b[1;32m     20\u001b[0m VERIF_NUMBER_STATEMENT \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: \u001b[39many\u001b[39m(\n\u001b[0;32m---> 21\u001b[0m     [re\u001b[39m.\u001b[39;49msub(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m[^\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39ms]\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, x) \u001b[39min\u001b[39;00m LOWERCASED_LATIN_NUMBERS, x \u001b[39min\u001b[39;00m STRING_NUMBERS]\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m CHAPTER_STRINGS \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mchapter\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpart\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mStratifiedKFold3\u001b[39;00m(StratifiedKFold):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/re.py:210\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msub\u001b[39m(pattern, repl, string, count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    204\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39;49msub(repl, string, count)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ns = [0.1, 0.15, 0.2, 0.3, 0.5]\n",
    "all_pairs = []\n",
    "for n in ns : \n",
    "    print(\"*\"*10 + f\" PROCESSING CHAPTER PERCENT {n }\" + \"*\"*10)\n",
    "    pairs = corpus.chapterwise_AV_samples(percent_train_chapters=n, percent_test_chapters=0.5)\n",
    "    all_pairs.append(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5,\n",
       "  array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "         17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]),\n",
       "  defaultdict(list,\n",
       "              {5: [1614,\n",
       "                1615,\n",
       "                1616,\n",
       "                1617,\n",
       "                1618,\n",
       "                1619,\n",
       "                1620,\n",
       "                1621,\n",
       "                1622,\n",
       "                1623,\n",
       "                1625,\n",
       "                1627,\n",
       "                1629,\n",
       "                1631,\n",
       "                1633,\n",
       "                1635,\n",
       "                1637,\n",
       "                1656,\n",
       "                1657,\n",
       "                1658,\n",
       "                1659,\n",
       "                1713,\n",
       "                1715,\n",
       "                1717,\n",
       "                1719,\n",
       "                1721,\n",
       "                1723,\n",
       "                1725,\n",
       "                1727,\n",
       "                1729],\n",
       "               7: [1574,\n",
       "                1575,\n",
       "                1576,\n",
       "                1577,\n",
       "                1578,\n",
       "                1624,\n",
       "                1626,\n",
       "                1628,\n",
       "                1630,\n",
       "                1632,\n",
       "                1634,\n",
       "                1636,\n",
       "                1638,\n",
       "                1640,\n",
       "                1642,\n",
       "                1644,\n",
       "                1646,\n",
       "                1711,\n",
       "                1712,\n",
       "                1714,\n",
       "                1716,\n",
       "                1718,\n",
       "                1720,\n",
       "                1722,\n",
       "                1724,\n",
       "                1726,\n",
       "                1728,\n",
       "                1730,\n",
       "                1771,\n",
       "                1772,\n",
       "                1773,\n",
       "                1774,\n",
       "                1775,\n",
       "                1776,\n",
       "                1777,\n",
       "                1778,\n",
       "                1779],\n",
       "               9: [1507,\n",
       "                1508,\n",
       "                1510,\n",
       "                1512,\n",
       "                1514,\n",
       "                1516,\n",
       "                1517,\n",
       "                1519,\n",
       "                1520,\n",
       "                1521,\n",
       "                1522,\n",
       "                1523,\n",
       "                1525,\n",
       "                1542,\n",
       "                1570,\n",
       "                1571,\n",
       "                1572,\n",
       "                1573,\n",
       "                1579,\n",
       "                1580,\n",
       "                1581,\n",
       "                1583,\n",
       "                1585,\n",
       "                1587,\n",
       "                1589,\n",
       "                1591,\n",
       "                1593,\n",
       "                1595,\n",
       "                1597,\n",
       "                1598,\n",
       "                1599,\n",
       "                1601,\n",
       "                1604,\n",
       "                1606,\n",
       "                1607,\n",
       "                1609,\n",
       "                1611,\n",
       "                1613,\n",
       "                1647,\n",
       "                1648,\n",
       "                1649,\n",
       "                1651,\n",
       "                1653,\n",
       "                1655,\n",
       "                1660,\n",
       "                1662,\n",
       "                1663,\n",
       "                1665,\n",
       "                1667,\n",
       "                1669,\n",
       "                1670,\n",
       "                1671,\n",
       "                1672,\n",
       "                1674,\n",
       "                1676,\n",
       "                1678,\n",
       "                1679,\n",
       "                1681,\n",
       "                1683,\n",
       "                1685,\n",
       "                1687,\n",
       "                1688,\n",
       "                1690,\n",
       "                1691,\n",
       "                1693,\n",
       "                1694,\n",
       "                1695,\n",
       "                1696,\n",
       "                1697,\n",
       "                1699,\n",
       "                1701,\n",
       "                1703,\n",
       "                1705,\n",
       "                1707,\n",
       "                1708,\n",
       "                1710,\n",
       "                1731,\n",
       "                1733,\n",
       "                1735,\n",
       "                1736,\n",
       "                1738,\n",
       "                1740,\n",
       "                1742,\n",
       "                1747,\n",
       "                1749,\n",
       "                1751,\n",
       "                1753,\n",
       "                1755,\n",
       "                1756,\n",
       "                1758,\n",
       "                1760,\n",
       "                1761,\n",
       "                1763,\n",
       "                1765,\n",
       "                1767,\n",
       "                1769],\n",
       "               10: [1639, 1641, 1643, 1645],\n",
       "               22: [1582,\n",
       "                1584,\n",
       "                1586,\n",
       "                1588,\n",
       "                1590,\n",
       "                1592,\n",
       "                1594,\n",
       "                1596,\n",
       "                1600,\n",
       "                1602,\n",
       "                1603,\n",
       "                1605,\n",
       "                1608,\n",
       "                1610,\n",
       "                1612,\n",
       "                1650,\n",
       "                1652,\n",
       "                1654,\n",
       "                1661,\n",
       "                1664,\n",
       "                1666,\n",
       "                1673,\n",
       "                1675,\n",
       "                1677,\n",
       "                1680,\n",
       "                1682,\n",
       "                1684,\n",
       "                1689,\n",
       "                1692,\n",
       "                1698,\n",
       "                1700,\n",
       "                1702,\n",
       "                1704],\n",
       "               33: [1527,\n",
       "                1530,\n",
       "                1532,\n",
       "                1534,\n",
       "                1536,\n",
       "                1537,\n",
       "                1538,\n",
       "                1539,\n",
       "                1541,\n",
       "                1543,\n",
       "                1544,\n",
       "                1546,\n",
       "                1548,\n",
       "                1549,\n",
       "                1551,\n",
       "                1553,\n",
       "                1555,\n",
       "                1557,\n",
       "                1559,\n",
       "                1561,\n",
       "                1563,\n",
       "                1565,\n",
       "                1567,\n",
       "                1568,\n",
       "                1569],\n",
       "               35: [1500,\n",
       "                1501,\n",
       "                1502,\n",
       "                1503,\n",
       "                1504,\n",
       "                1505,\n",
       "                1506,\n",
       "                1509,\n",
       "                1511,\n",
       "                1513,\n",
       "                1515,\n",
       "                1518,\n",
       "                1524,\n",
       "                1526,\n",
       "                1528,\n",
       "                1529,\n",
       "                1531,\n",
       "                1533,\n",
       "                1535,\n",
       "                1540,\n",
       "                1545,\n",
       "                1547,\n",
       "                1550,\n",
       "                1552,\n",
       "                1554,\n",
       "                1556,\n",
       "                1558,\n",
       "                1560,\n",
       "                1562,\n",
       "                1564,\n",
       "                1566,\n",
       "                1668,\n",
       "                1686,\n",
       "                1706,\n",
       "                1709,\n",
       "                1732,\n",
       "                1734,\n",
       "                1737,\n",
       "                1739,\n",
       "                1741,\n",
       "                1743,\n",
       "                1744,\n",
       "                1745,\n",
       "                1746,\n",
       "                1748,\n",
       "                1750,\n",
       "                1752,\n",
       "                1754,\n",
       "                1757,\n",
       "                1759,\n",
       "                1762,\n",
       "                1764,\n",
       "                1766,\n",
       "                1768,\n",
       "                1770,\n",
       "                1780,\n",
       "                1781,\n",
       "                1782,\n",
       "                1783,\n",
       "                1784,\n",
       "                1785,\n",
       "                1786,\n",
       "                1787,\n",
       "                1788,\n",
       "                1789,\n",
       "                1790,\n",
       "                1791]}))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pairs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_id = 1 \n",
    "p10 = []\n",
    "p50 = []\n",
    "\n",
    "\n",
    "p10 = [all_pairs[0][0][i] for i in range(len(all_pairs[0][0])) if all_pairs[0][0][i][0] == 5]\n",
    "p50 = [all_pairs[0][-1][i] for i in range(len(all_pairs[0][-1])) if all_pairs[0][-1][i][0] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p10[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " [528,\n",
       "  530,\n",
       "  532,\n",
       "  534,\n",
       "  536,\n",
       "  539,\n",
       "  544,\n",
       "  547,\n",
       "  549,\n",
       "  551,\n",
       "  560,\n",
       "  562,\n",
       "  564,\n",
       "  569,\n",
       "  570,\n",
       "  572,\n",
       "  574,\n",
       "  576,\n",
       "  578,\n",
       "  580,\n",
       "  582,\n",
       "  584,\n",
       "  586,\n",
       "  587,\n",
       "  590,\n",
       "  593],\n",
       " defaultdict(list,\n",
       "             {5: [618, 658, 699, 701, 819, 825],\n",
       "              1: [460, 462, 464, 466, 844, 846, 848, 850, 851, 853],\n",
       "              2: [144,\n",
       "               147,\n",
       "               149,\n",
       "               152,\n",
       "               154,\n",
       "               156,\n",
       "               159,\n",
       "               161,\n",
       "               164,\n",
       "               165,\n",
       "               193,\n",
       "               196,\n",
       "               256,\n",
       "               258,\n",
       "               262,\n",
       "               315,\n",
       "               317,\n",
       "               319,\n",
       "               321,\n",
       "               323,\n",
       "               325,\n",
       "               468,\n",
       "               589,\n",
       "               592,\n",
       "               594,\n",
       "               603,\n",
       "               605,\n",
       "               607,\n",
       "               609,\n",
       "               611,\n",
       "               614,\n",
       "               616,\n",
       "               642,\n",
       "               655,\n",
       "               661,\n",
       "               671,\n",
       "               672,\n",
       "               676,\n",
       "               684,\n",
       "               686,\n",
       "               688,\n",
       "               690,\n",
       "               692,\n",
       "               694,\n",
       "               703,\n",
       "               708,\n",
       "               712,\n",
       "               714,\n",
       "               725,\n",
       "               727,\n",
       "               737,\n",
       "               743,\n",
       "               745,\n",
       "               747,\n",
       "               749,\n",
       "               751,\n",
       "               753,\n",
       "               755,\n",
       "               757,\n",
       "               760,\n",
       "               762,\n",
       "               764,\n",
       "               765,\n",
       "               767,\n",
       "               770,\n",
       "               772,\n",
       "               773,\n",
       "               775,\n",
       "               777,\n",
       "               779,\n",
       "               781,\n",
       "               783,\n",
       "               785,\n",
       "               787,\n",
       "               788,\n",
       "               790,\n",
       "               792,\n",
       "               794,\n",
       "               796,\n",
       "               820,\n",
       "               821,\n",
       "               833,\n",
       "               835],\n",
       "              9: [102,\n",
       "               103,\n",
       "               104,\n",
       "               105,\n",
       "               107,\n",
       "               109,\n",
       "               110,\n",
       "               112,\n",
       "               114,\n",
       "               115,\n",
       "               117,\n",
       "               119,\n",
       "               121,\n",
       "               124,\n",
       "               127,\n",
       "               128,\n",
       "               130,\n",
       "               132,\n",
       "               134,\n",
       "               136,\n",
       "               138,\n",
       "               140,\n",
       "               142,\n",
       "               145,\n",
       "               150,\n",
       "               153,\n",
       "               155,\n",
       "               158,\n",
       "               160,\n",
       "               162,\n",
       "               163,\n",
       "               168,\n",
       "               170,\n",
       "               172,\n",
       "               174,\n",
       "               176,\n",
       "               178,\n",
       "               179,\n",
       "               180,\n",
       "               182,\n",
       "               184,\n",
       "               185,\n",
       "               187,\n",
       "               189,\n",
       "               191,\n",
       "               194,\n",
       "               195,\n",
       "               197,\n",
       "               198,\n",
       "               202,\n",
       "               204,\n",
       "               205,\n",
       "               207,\n",
       "               209,\n",
       "               211,\n",
       "               214,\n",
       "               216,\n",
       "               219,\n",
       "               223,\n",
       "               224,\n",
       "               225,\n",
       "               227,\n",
       "               229,\n",
       "               231,\n",
       "               233,\n",
       "               235,\n",
       "               237,\n",
       "               239,\n",
       "               241,\n",
       "               242,\n",
       "               244,\n",
       "               246,\n",
       "               248,\n",
       "               250,\n",
       "               252,\n",
       "               254,\n",
       "               255,\n",
       "               266,\n",
       "               268,\n",
       "               269,\n",
       "               271,\n",
       "               272,\n",
       "               275,\n",
       "               277,\n",
       "               279,\n",
       "               281,\n",
       "               282,\n",
       "               284,\n",
       "               285,\n",
       "               287,\n",
       "               291,\n",
       "               294,\n",
       "               296,\n",
       "               298,\n",
       "               300,\n",
       "               302,\n",
       "               306,\n",
       "               307,\n",
       "               308,\n",
       "               312,\n",
       "               326,\n",
       "               328,\n",
       "               331,\n",
       "               333,\n",
       "               335,\n",
       "               336,\n",
       "               339,\n",
       "               340,\n",
       "               341,\n",
       "               343,\n",
       "               345,\n",
       "               347,\n",
       "               349,\n",
       "               352,\n",
       "               354,\n",
       "               357,\n",
       "               359,\n",
       "               361,\n",
       "               363,\n",
       "               365,\n",
       "               367,\n",
       "               368,\n",
       "               370,\n",
       "               372,\n",
       "               373,\n",
       "               375,\n",
       "               376,\n",
       "               377,\n",
       "               379,\n",
       "               381,\n",
       "               382,\n",
       "               383,\n",
       "               384,\n",
       "               385,\n",
       "               386,\n",
       "               388,\n",
       "               390,\n",
       "               392,\n",
       "               394,\n",
       "               397,\n",
       "               399,\n",
       "               401,\n",
       "               404,\n",
       "               406,\n",
       "               449,\n",
       "               451,\n",
       "               453,\n",
       "               455,\n",
       "               458,\n",
       "               467,\n",
       "               469,\n",
       "               470,\n",
       "               472,\n",
       "               474,\n",
       "               476,\n",
       "               480,\n",
       "               482,\n",
       "               484,\n",
       "               487,\n",
       "               490,\n",
       "               492,\n",
       "               493,\n",
       "               495,\n",
       "               497,\n",
       "               502,\n",
       "               506,\n",
       "               521,\n",
       "               522,\n",
       "               524,\n",
       "               526,\n",
       "               529,\n",
       "               531,\n",
       "               533,\n",
       "               535,\n",
       "               538,\n",
       "               546,\n",
       "               548,\n",
       "               550,\n",
       "               553,\n",
       "               557,\n",
       "               561,\n",
       "               563,\n",
       "               565,\n",
       "               595,\n",
       "               596,\n",
       "               597,\n",
       "               598,\n",
       "               599,\n",
       "               600,\n",
       "               601,\n",
       "               602,\n",
       "               604,\n",
       "               606,\n",
       "               608,\n",
       "               610,\n",
       "               612,\n",
       "               613,\n",
       "               615,\n",
       "               617,\n",
       "               619,\n",
       "               623,\n",
       "               625,\n",
       "               627,\n",
       "               629,\n",
       "               631,\n",
       "               632,\n",
       "               634,\n",
       "               636,\n",
       "               646,\n",
       "               648,\n",
       "               650,\n",
       "               652,\n",
       "               654,\n",
       "               656,\n",
       "               662,\n",
       "               666,\n",
       "               670,\n",
       "               673,\n",
       "               675,\n",
       "               680,\n",
       "               696,\n",
       "               705,\n",
       "               706,\n",
       "               707,\n",
       "               709,\n",
       "               716,\n",
       "               718,\n",
       "               720,\n",
       "               722,\n",
       "               723,\n",
       "               729,\n",
       "               738,\n",
       "               739,\n",
       "               740,\n",
       "               741,\n",
       "               763,\n",
       "               766,\n",
       "               768,\n",
       "               769,\n",
       "               774,\n",
       "               776,\n",
       "               778,\n",
       "               780,\n",
       "               782,\n",
       "               784,\n",
       "               786,\n",
       "               798,\n",
       "               799,\n",
       "               800,\n",
       "               801,\n",
       "               816,\n",
       "               827,\n",
       "               829,\n",
       "               831,\n",
       "               837,\n",
       "               838,\n",
       "               841,\n",
       "               843],\n",
       "              11: [461, 463, 465, 845, 847, 849, 852, 854],\n",
       "              8: [471,\n",
       "               473,\n",
       "               475,\n",
       "               477,\n",
       "               479,\n",
       "               481,\n",
       "               483,\n",
       "               485,\n",
       "               486,\n",
       "               489,\n",
       "               491,\n",
       "               494,\n",
       "               496,\n",
       "               499,\n",
       "               501,\n",
       "               504,\n",
       "               507,\n",
       "               509,\n",
       "               511,\n",
       "               513,\n",
       "               515,\n",
       "               518,\n",
       "               520,\n",
       "               523,\n",
       "               525,\n",
       "               527,\n",
       "               537,\n",
       "               540,\n",
       "               542,\n",
       "               545,\n",
       "               552,\n",
       "               554,\n",
       "               556,\n",
       "               558,\n",
       "               559,\n",
       "               620,\n",
       "               621,\n",
       "               622,\n",
       "               624,\n",
       "               626,\n",
       "               628,\n",
       "               630,\n",
       "               638,\n",
       "               644,\n",
       "               683,\n",
       "               685,\n",
       "               687,\n",
       "               689,\n",
       "               691,\n",
       "               693,\n",
       "               698,\n",
       "               704],\n",
       "              7: [257,\n",
       "               259,\n",
       "               260,\n",
       "               261,\n",
       "               263,\n",
       "               264,\n",
       "               265,\n",
       "               267,\n",
       "               270,\n",
       "               273,\n",
       "               274,\n",
       "               276,\n",
       "               278,\n",
       "               280,\n",
       "               283,\n",
       "               286,\n",
       "               288,\n",
       "               289,\n",
       "               290,\n",
       "               292,\n",
       "               293,\n",
       "               295,\n",
       "               297,\n",
       "               299,\n",
       "               301,\n",
       "               303,\n",
       "               304,\n",
       "               305,\n",
       "               309,\n",
       "               310,\n",
       "               311,\n",
       "               313,\n",
       "               314,\n",
       "               316,\n",
       "               318,\n",
       "               320,\n",
       "               322,\n",
       "               324,\n",
       "               369,\n",
       "               371,\n",
       "               374,\n",
       "               378,\n",
       "               380,\n",
       "               407,\n",
       "               408,\n",
       "               409,\n",
       "               412,\n",
       "               414,\n",
       "               417,\n",
       "               419,\n",
       "               421,\n",
       "               423,\n",
       "               425,\n",
       "               427,\n",
       "               429,\n",
       "               431,\n",
       "               433,\n",
       "               435,\n",
       "               438,\n",
       "               442,\n",
       "               445,\n",
       "               447,\n",
       "               456,\n",
       "               459,\n",
       "               478,\n",
       "               488,\n",
       "               498,\n",
       "               500,\n",
       "               503,\n",
       "               505,\n",
       "               508,\n",
       "               510,\n",
       "               512,\n",
       "               514,\n",
       "               516,\n",
       "               517,\n",
       "               519,\n",
       "               541,\n",
       "               543,\n",
       "               555,\n",
       "               566,\n",
       "               567,\n",
       "               568,\n",
       "               571,\n",
       "               573,\n",
       "               575,\n",
       "               577,\n",
       "               579,\n",
       "               581,\n",
       "               583,\n",
       "               585,\n",
       "               588,\n",
       "               591,\n",
       "               633,\n",
       "               635,\n",
       "               637,\n",
       "               647,\n",
       "               649,\n",
       "               651,\n",
       "               653,\n",
       "               657,\n",
       "               664,\n",
       "               667,\n",
       "               668,\n",
       "               674,\n",
       "               700,\n",
       "               732,\n",
       "               733,\n",
       "               734,\n",
       "               735,\n",
       "               736,\n",
       "               789,\n",
       "               826,\n",
       "               828,\n",
       "               830,\n",
       "               840,\n",
       "               842],\n",
       "              3: [327,\n",
       "               329,\n",
       "               330,\n",
       "               332,\n",
       "               334,\n",
       "               337,\n",
       "               338,\n",
       "               342,\n",
       "               344,\n",
       "               346,\n",
       "               348,\n",
       "               350,\n",
       "               351,\n",
       "               353,\n",
       "               355,\n",
       "               356,\n",
       "               358,\n",
       "               360,\n",
       "               362,\n",
       "               364,\n",
       "               366,\n",
       "               410,\n",
       "               411,\n",
       "               413,\n",
       "               415,\n",
       "               416,\n",
       "               418,\n",
       "               420,\n",
       "               422,\n",
       "               424,\n",
       "               426,\n",
       "               428,\n",
       "               430,\n",
       "               432,\n",
       "               434,\n",
       "               436,\n",
       "               437,\n",
       "               439,\n",
       "               440,\n",
       "               441,\n",
       "               443,\n",
       "               444,\n",
       "               446,\n",
       "               448,\n",
       "               450,\n",
       "               452,\n",
       "               454,\n",
       "               457,\n",
       "               639,\n",
       "               641,\n",
       "               645,\n",
       "               660,\n",
       "               677,\n",
       "               678,\n",
       "               679,\n",
       "               681,\n",
       "               682,\n",
       "               695,\n",
       "               697,\n",
       "               702,\n",
       "               710,\n",
       "               711,\n",
       "               713,\n",
       "               715,\n",
       "               717,\n",
       "               719,\n",
       "               721,\n",
       "               724,\n",
       "               726,\n",
       "               728,\n",
       "               730,\n",
       "               731,\n",
       "               803,\n",
       "               805,\n",
       "               807,\n",
       "               809,\n",
       "               811,\n",
       "               813,\n",
       "               815,\n",
       "               817,\n",
       "               822,\n",
       "               823,\n",
       "               824,\n",
       "               832,\n",
       "               834,\n",
       "               836,\n",
       "               839],\n",
       "              6: [387,\n",
       "               389,\n",
       "               391,\n",
       "               393,\n",
       "               395,\n",
       "               396,\n",
       "               398,\n",
       "               400,\n",
       "               402,\n",
       "               403,\n",
       "               405,\n",
       "               640,\n",
       "               643,\n",
       "               659,\n",
       "               663,\n",
       "               665,\n",
       "               669,\n",
       "               742,\n",
       "               744,\n",
       "               746,\n",
       "               748,\n",
       "               750,\n",
       "               752,\n",
       "               754,\n",
       "               756,\n",
       "               758,\n",
       "               759,\n",
       "               761,\n",
       "               771,\n",
       "               791,\n",
       "               793,\n",
       "               795,\n",
       "               797,\n",
       "               802,\n",
       "               804,\n",
       "               806,\n",
       "               808,\n",
       "               810,\n",
       "               812,\n",
       "               814,\n",
       "               818]}))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p50[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "quote_embeddings = []\n",
    "\n",
    "for quotes in tqdm.tqdm(corpus[\"quotes\"]) : \n",
    "    quote_embeddings.append(model.encode(quotes, device=torch.device(\"cuda:0\"), normalize_embeddings=True, convert_to_numpy=False, convert_to_tensor=True).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_scores = []\n",
    "\n",
    "for pairs in all_pairs : \n",
    "    \n",
    "    overall_auc, aucs, recalls = score(quote_embeddings, pairs)\n",
    "    semantic_scores.append((overall_auc, aucs, recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quoted_semantic_scores = []\n",
    "\n",
    "for pairs in all_pairs : \n",
    "    \n",
    "    overall_auc, aucs, recalls = score_quote_by_quote(quote_embeddings, pairs)\n",
    "    quoted_semantic_scores.append((overall_auc, aucs, recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('AnnaWegmann/Style-Embedding')\n",
    "quote_embeddings = []\n",
    "\n",
    "for quotes in tqdm.tqdm(corpus[\"quotes\"]) : \n",
    "    quote_embeddings.append(model.encode(quotes, device=torch.device(\"cuda:0\"), normalize_embeddings=True, convert_to_numpy=False, convert_to_tensor=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stel_scores = []\n",
    "\n",
    "for pairs in all_pairs : \n",
    "    \n",
    "    overall_auc, aucs, recalls = score(quote_embeddings, pairs)\n",
    "    stel_scores.append((overall_auc, aucs, recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quoted_stel_scores = []\n",
    "\n",
    "for pairs in all_pairs : \n",
    "    overall_auc, aucs, recalls = score_quote_by_quote(quote_embeddings, pairs)\n",
    "    quoted_stel_scores.append((overall_auc, aucs, recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('SamLowe/roberta-base-go_emotions')\n",
    "config = AutoConfig.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\", config=config).to(\"cuda:0\")\n",
    "\n",
    "\n",
    "\n",
    "quote_embeddings = []\n",
    "\n",
    "batch_size = 32\n",
    "with torch.no_grad() : \n",
    "    for idx, quotes in tqdm.tqdm(enumerate(corpus[\"quotes\"])) : \n",
    "        last_h = []\n",
    "        \n",
    "        model.eval()\n",
    "        for idx in range(0,len(quotes), batch_size) : \n",
    "            tokens = tokenizer(quotes[idx:idx+batch_size], return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "            u = model(**tokens.to(\"cuda:0\"), return_dict=True, output_hidden_states = True)\n",
    "            last_h.append(u.hidden_states[-1][:,0].detach().cpu())\n",
    "        quote_embeddings.append(F.normalize(torch.cat(last_h), dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_scores = []\n",
    "\n",
    "for pairs in all_pairs : \n",
    "    \n",
    "    overall_auc, aucs, recalls = score(quote_embeddings, pairs)\n",
    "    emotion_scores.append((overall_auc, aucs, recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quoted_emotion_scores = []\n",
    "\n",
    "for pairs in all_pairs : \n",
    "    \n",
    "    overall_auc, aucs, recalls = score_quote_by_quote(quote_embeddings, pairs)\n",
    "    quoted_emotion_scores.append((overall_auc, aucs, recalls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LUAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rrivera1849/LUAR-MUD\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"rrivera1849/LUAR-MUD\", trust_remote_code=True).to(\"cuda:0\")\n",
    "def tokenize(quotes, batch_first = False) : \n",
    "    tokens = tokenizer(quotes, max_length = 32, return_tensors = \"pt\", truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    if not batch_first:  \n",
    "        tokens[\"input_ids\"] = tokens[\"input_ids\"].reshape(1, -1, 32)\n",
    "        tokens[\"attention_mask\"] = tokens[\"attention_mask\"].reshape(1, -1, 32)\n",
    "    else : \n",
    "        tokens[\"input_ids\"] = tokens[\"input_ids\"].reshape(-1, 1, 32)\n",
    "        tokens[\"attention_mask\"] = tokens[\"attention_mask\"].reshape(-1, 1, 32)\n",
    "        \n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score(quote_embeddings, pairs):\n",
    "    scores = defaultdict(list)\n",
    "    labels = []\n",
    "    preds = []\n",
    "\n",
    "    for nidx, novel_pairs in enumerate(pairs):\n",
    "        if len(novel_pairs) == 0:\n",
    "            print(f\"NOVEL {nidx} ---- Found no pairs\")\n",
    "            scores[\"aucs_mean\"].append(None)\n",
    "            scores[\"aucs_std\"].append(None)\n",
    "\n",
    "            scores[\"R1\"].append(None)\n",
    "            scores[\"R3\"].append(None)\n",
    "            scores[\"R8\"].append(None)\n",
    "\n",
    "        else:\n",
    "            labels_n = []\n",
    "            preds_n = []\n",
    "            speaker_aucs = []\n",
    "            r_1, r_3, r_8 = [], [], []\n",
    "\n",
    "            q_emb = quote_embeddings[nidx]\n",
    "            for u, (speaker_id, anchor, neg_and_pos) in enumerate(novel_pairs):\n",
    "                ep1 = q_emb[anchor].mean(dim=0)\n",
    "                ep2 = q_emb[neg_and_pos[speaker_id]].mean(dim=0)\n",
    "\n",
    "                embeddings = [ep2.unsqueeze(0)]\n",
    "                for sid, quote_ids in neg_and_pos.items():\n",
    "                    if sid != speaker_id:\n",
    "                        eneg = q_emb[quote_ids].mean(dim=0)\n",
    "                        embeddings.append(eneg.unsqueeze(0))\n",
    "\n",
    "                sims = util.dot_score(ep1, torch.cat(embeddings, dim=0))[0]\n",
    "                sorted_idx = torch.argsort(sims)\n",
    "                r_1.append(1 if 0 in sorted_idx[-1:] else 0)\n",
    "                r_3.append(1 if 0 in sorted_idx[-3:] else 0)\n",
    "                r_8.append(1 if 0 in sorted_idx[-8:] else 0)\n",
    "\n",
    "                speaker_label = [1] + [0] * (len(sims) - 1)\n",
    "                speaker_pred = sims.tolist()\n",
    "                speaker_aucs.append(roc_auc_score(speaker_label, speaker_pred))\n",
    "\n",
    "                labels_n.extend()\n",
    "                preds_n.extend(sims.tolist())\n",
    "\n",
    "            scores[\"R8\"].append(sum(r_8) / len(novel_pairs))\n",
    "            scores[\"R1\"].append(sum(r_1) / len(novel_pairs))\n",
    "            scores[\"R3\"].append(sum(r_3) / len(novel_pairs))\n",
    "\n",
    "            scores[\"aucs_mean\"].append(np.mean(speaker_aucs))\n",
    "            scores[\"aucs_std\"].append(np.std(speaker_aucs))\n",
    "\n",
    "            labels.extend(labels_n)\n",
    "            preds.extend(preds_n)\n",
    "            print(\n",
    "                f\"NOVEL {nidx} ---- Recall@{1} {scores['R1'][-1]:0.3f} | Recall@{3} {scores['R3'][-1]:0.3f} | Recall@{8} {scores['R8'][-1]:0.3f} | AUC {scores['auc_mean'][-1]:0.3f}\"\n",
    "            )\n",
    "\n",
    "    overall_auc = roc_auc_score(labels, preds)\n",
    "\n",
    "    mean_aucs = np.mean([i for i in scores[\"auc_mean\"] if i != None])\n",
    "    mean_R1 = np.mean([i for i in scores[\"R1\"] if i != None])\n",
    "    mean_R3 = np.mean([i for i in scores[\"R3\"] if i != None])\n",
    "    mean_R8 = np.mean([i for i in scores[\"R8\"] if i != None])\n",
    "\n",
    "    print(\"\")\n",
    "    print(\n",
    "        f\"Recall@{1} {mean_R1:0.3f} | Recall@{3} {mean_R3:0.3f} | Recall@{8} {mean_R8:0.3f} | AUC (average novels) {mean_aucs:0.3f} | Overall AUC {overall_auc:0.3f}\"\n",
    "    )\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def luar_score(model, quotes, pairs) : \n",
    "    scores = defaultdict(list)\n",
    "    model.eval()\n",
    "    with torch.no_grad() : \n",
    "        for nidx, novel_pairs in enumerate(pairs) : \n",
    "            if len(novel_pairs) == 0:\n",
    "                print(f\"NOVEL {nidx} ---- Found no pairs\")\n",
    "                scores[\"aucs_mean\"].append(None)\n",
    "                scores[\"aucs_std\"].append(None)\n",
    "\n",
    "                scores[\"R1\"].append(None)\n",
    "                scores[\"R3\"].append(None)\n",
    "                scores[\"R8\"].append(None)\n",
    "            else : \n",
    "                r_1, r_3, r_8 = [], [], []\n",
    "                speaker_aucs = []\n",
    "                quotes = np.asarray( corpus[\"quotes\"][nidx])\n",
    "                for u, (speaker_id, anchor, neg_and_pos) in enumerate(novel_pairs) :\n",
    "                    \n",
    "                    q1 =quotes[anchor]\n",
    "                    tokens = tokenize(q1.tolist())\n",
    "                    ep1 = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 ).cpu()\n",
    "                    \n",
    "                    q2 = quotes[neg_and_pos[speaker_id]]\n",
    "                    tokens = tokenize(q2.tolist())\n",
    "                    ep2 = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 ).cpu()\n",
    "                    \n",
    "                    embeddings = [ep2]\n",
    "                    for sid, quote_ids in neg_and_pos.items() :\n",
    "                        if sid != speaker_id  : \n",
    "                            neg = quotes[quote_ids]\n",
    "                            tokens = tokenize(neg.tolist())\n",
    "                            eneg = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 )                \n",
    "                            embeddings.append(eneg.cpu())\n",
    "                            \n",
    "                    sims = util.dot_score(ep1[0], torch.cat(embeddings, dim = 0))[0]\n",
    "                    sorted_idx = torch.argsort(sims)\n",
    "                    r_1.append(1 if 0 in sorted_idx[-1:] else 0 )\n",
    "                    r_3.append(1 if 0 in sorted_idx[-3:] else 0 )\n",
    "                    r_8.append(1 if 0 in sorted_idx[-8:] else 0 )\n",
    "\n",
    "                    speaker_label = [1] + [0] * (len(sims) - 1)\n",
    "                    speaker_pred = sims.tolist()\n",
    "                    speaker_aucs.append(roc_auc_score(speaker_label, speaker_pred))\n",
    "\n",
    "                scores[\"R8\"].append(sum(r_8) / len(novel_pairs))\n",
    "                scores[\"R1\"].append(sum(r_1) / len(novel_pairs))\n",
    "                scores[\"R3\"].append(sum(r_3) / len(novel_pairs))\n",
    "\n",
    "                scores[\"aucs_mean\"].append(np.mean(speaker_aucs))\n",
    "                scores[\"aucs_std\"].append(np.std(speaker_aucs))\n",
    "            \n",
    "                print(\n",
    "                    f\"NOVEL {nidx} ---- Recall@{1} {scores['R1'][-1]:0.3f} | Recall@{3} {scores['R3'][-1]:0.3f} | Recall@{8} {scores['R8'][-1]:0.3f} | AUC {scores['auc_mean'][-1]:0.3f}\"\n",
    "                )\n",
    "        mean_aucs = np.mean([i for i in aucs if i != None])\n",
    "        mean_R1 = np.mean([i for i in R1 if i != None])\n",
    "        mean_R3 = np.mean([i for i in R3 if i != None])\n",
    "        mean_R8 =  np.mean([i for i in R8 if i != None])\n",
    "\n",
    "        print(\"\")\n",
    "        print(f\"Recall@{1} {mean_R1:0.3f} | Recall@{3} {mean_R3:0.3f} | Recall@{8} {mean_R8:0.3f} | AUC (average novels) {mean_aucs:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def luar_score_quote_by_quote(model, tokenizer, corpus, pairs) : \n",
    "    scores = defaultdict(list)\n",
    "    model.eval()\n",
    "    with torch.no_grad() : \n",
    "        for nidx, novel_pairs in enumerate(pairs) : \n",
    "            if len(novel_pairs) == 0 : \n",
    "                print(f\"NOVEL {nidx} ---- Found no pairs\")\n",
    "                scores[\"aucs_mean\"].append(None)\n",
    "                scores[\"aucs_std\"].append(None)\n",
    "                scores[\"ndcg_mean\"].append(None)\n",
    "                scores[\"ndcg_std\"].append(None)                \n",
    "            else : \n",
    "                scores_n = []\n",
    "                speaker_aucs = []\n",
    "                quotes = np.asarray( corpus[\"quotes\"][nidx])\n",
    "                for u, (speaker_id, anchor, neg_and_pos) in enumerate(novel_pairs) :\n",
    "                    \n",
    "                    q1 =quotes[anchor]\n",
    "                    tokens = tokenize(tokenizer, q1.tolist())\n",
    "                    ep1 = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 ).cpu()\n",
    "                    \n",
    "                    q2 = quotes[neg_and_pos[speaker_id]]\n",
    "                    tokens = tokenize(tokenizer, q2.tolist(), batch_first = True)\n",
    "                    ep2 = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 ).cpu()\n",
    "                    \n",
    "                    embeddings = [ep2]\n",
    "                    true_relevance = [1] * ep2.size(0)\n",
    "                    for sid, quote_ids in neg_and_pos.items() :\n",
    "                        if sid != speaker_id  : \n",
    "                            neg = quotes[quote_ids]\n",
    "                            tokens = tokenize(tokenizer, neg.tolist(), batch_first = True)\n",
    "                            eneg = F.normalize( model(**tokens.to(\"cuda:0\")), dim =1 )                \n",
    "                            embeddings.append(eneg.cpu())\n",
    "                            true_relevance.extend([0] * eneg.size(0))\n",
    "                            \n",
    "                    sims = util.dot_score(ep1[0], torch.cat(embeddings, dim = 0))[0]\n",
    "                    \n",
    "                    score = ndcg_score(np.asarray(true_relevance)[np.newaxis, :], sims.cpu().numpy()[np.newaxis, :])\n",
    "                    scores_n.append(score)\n",
    "                    speaker_aucs.append(roc_auc_score(true_relevance, sims.cpu().numpy()))\n",
    "\n",
    "                scores[\"ndcg_mean\"].append(np.mean(scores_n))\n",
    "                scores[\"ndcg_std\"].append(np.std(scores_n))\n",
    "                scores[\"auc_mean\"].append(np.mean(speaker_aucs))\n",
    "                scores[\"auc_std\"].append(np.std(speaker_aucs))\n",
    "                \n",
    "                print(\n",
    "                    f\"NOVEL {nidx} ---- nDCG {scores['ndcg_mean'][-1]:0.3f} | AUC {scores['auc_mean'][-1]:0.3f}\"\n",
    "                )\n",
    "            \n",
    "        mean_aucs = np.mean([i for i in scores[\"auc_mean\"] if i != None])\n",
    "        mean_scores = np.mean([i for i in scores[\"ndcg_mean\"] if i is not None])\n",
    "\n",
    "        print(\"\")\n",
    "        print(\n",
    "                    f\"(average novel) nDCG {mean_scores:0.3f} | AUC (average novels) {mean_aucs:0.3f}\"\n",
    "                )\n",
    "        return scores        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character - Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(l) :\n",
    "    return np.mean([i for i in l if i is not None])\n",
    "\n",
    "for model_name, scores in zip([\"Semantics\", \"STEL\", \"Emotion\", \"LUAR\"], [semantic_scores, stel_scores, emotion_scores, luar_scores] ): \n",
    "    aucs = []\n",
    "    for n, score_ in zip(ns, scores) : \n",
    "        aucs.append(mean(score_[1]))\n",
    "    plt.plot(ns, aucs, marker = \".\", label= model_name, alpha=0.8, lw=2)\n",
    "    \n",
    "#plt.plot(ns, R1s, marker = \".\", label = \"Recall@1\", color=\"green\", alpha=0.7, lw=2)\n",
    "plt.xlim(0.05, 0.55)\n",
    "plt.title(\"Character - Character\")\n",
    "plt.xlabel(ns)\n",
    "#plt.ylim(min(aucs)-0.05, max(aucs)+0.05)\n",
    "plt.legend(frameon=True)\n",
    "plt.grid()\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"% of chapters used for encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, scores in zip([\"Semantics\", \"STEL\", \"Emotion\", \"LUAR\"], [semantic_scores, stel_scores, emotion_scores, luar_scores] ): \n",
    "    r1s= []\n",
    "    for n, score_ in zip(ns, scores) : \n",
    "        r1s.append(mean(score_[2][0]))\n",
    "    plt.plot(ns, r1s, marker = \".\", label= model_name, alpha=0.8, lw=2)\n",
    "    \n",
    "#plt.plot(ns, R1s, marker = \".\", label = \"Recall@1\", color=\"green\", alpha=0.7, lw=2)\n",
    "plt.xlim(0.05, 0.55)\n",
    "plt.title(\"Character - Character\")\n",
    "plt.xlabel(ns)\n",
    "#plt.ylim(min(aucs)-0.05, max(aucs)+0.05)\n",
    "plt.legend(frameon=True)\n",
    "plt.grid()\n",
    "plt.ylabel(\"Recall@1\")\n",
    "plt.xlabel(\"% of chapters used for encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character - Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, scores in zip([\"Semantics\", \"STEL\", \"Emotion\", \"LUAR\"], [quoted_semantic_scores, quoted_stel_scores, quoted_emotion_scores, quoted_luar_scores] ): \n",
    "    aucs = []\n",
    "    for n, score_ in zip(ns, scores) : \n",
    "        aucs.append(mean(score_[1]))\n",
    "    plt.plot(ns, aucs, marker = \".\", label= model_name, alpha=0.8, lw=2)\n",
    "    \n",
    "#plt.plot(ns, R1s, marker = \".\", label = \"Recall@1\", color=\"green\", alpha=0.7, lw=2)\n",
    "plt.xlim(0.05, 0.55)\n",
    "plt.title(\"Character - Quote\")\n",
    "plt.xlabel(ns)\n",
    "#plt.ylim(min(aucs)-0.05, max(aucs)+0.05)\n",
    "plt.legend(frameon=True)\n",
    "plt.grid()\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlabel(\"Chapter %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
